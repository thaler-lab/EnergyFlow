{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to EnergyFlow EnergyFlow is a Python package containing a suite of particle physics tools. Originally designed to compute Energy Flow Polynomials (EFPs), as of version 0.10.0 the package expanded to include implementations of Energy Flow Networks (EFNs) and Particle Flow Networks (PFNs). As of version 0.11.0 , functions for facilitating the computation of the Energy Mover's Distance (EMD) on particle physics events are included. To summarize the main features: Energy Flow Polynomials : EFPs are a collection of jet substructure observables which form a complete linear basis of IRC-safe observables. EnergyFlow provides tools to compute EFPs on events for several energy and angular measures as well as custom measures. Energy Flow Networks : EFNs are infrared- and collinear-safe models designed for learning from collider events as unordered, variable-length sets of particles. EnergyFlow contains customizable Keras implementations of EFNs. Particle Flow Networks : PFNs are general models designed for learning from collider events as unordered, variable-length sets of particles, based on the Deep Sets framework. EnergyFlow contains customizable Keras implementations of PFNs. Energy Mover's Distance : The EMD is a common metric between probability distributions that has been adapted for use as a metric between collider events. EnergyFlow contains code to facilitate the computation of the EMD between events based on an underlying implementation provided by the Python Optimal Transport (POT) library. Beyond the primary functions described above, the EnergyFlow package also provides useful supplementary features. These include a large quark/gluon jet dataset, implementations of additional machine learning architectures useful for collider physics, and many examples exhibiting the usage of the package. Jet Tagging Datasets : A dataset of 2 million simulated quark and gluon jets is provided. Additional Architectures : Implementations of other architectures useful for particle physics are also provided, such as convolutional neural networks (CNNs) for jet images. Detailed Examples : Examples showcasing EFPs, EFNs, PFNs, EMDs, and more. Also see the demos . The current version is 0.13.0 . Changes are summarized in the Release Notes . Using the most up-to-date version is recommended. As of version 0.7.0 , tests have been written covering the majority of the EFP and EMD code. The architectures code is currently tested by running the examples. The source code can be found on GitHub . Get started by installing EnergyFlow , exploring the demos , and running the examples ! References [1] P. T. Komiske, E. M. Metodiev, and J. Thaler, Energy Flow Polynomials: A complete linear basis for jet substructure , JHEP 04 (2018) 013 [ 1712.07124 ]. [2] P. T. Komiske, E. M. Metodiev, and J. Thaler, Energy Flow Networks: Deep Sets for Particle Jets , JHEP 01 (2019) 121 [ 1810.05165 ]. [3] P. T. Komiske, E. M. Metodiev, and J. Thaler, The Metric Space of Collider Events , Phys. Rev. Lett. 123 (2019) 041801 [ 1902.02346 ]. Copyright See the LICENSE for detailed copyright information. EnergyFlow uses a customized einsumfunc.py from the NumPy GitHub repository as well as a few functions relating to downloading files copied from the Keras GitHub repository. The copyrights for these parts of the code are attributed to their respective owners in the LICENSE file.","title":"Home"},{"location":"#welcome-to-energyflow","text":"EnergyFlow is a Python package containing a suite of particle physics tools. Originally designed to compute Energy Flow Polynomials (EFPs), as of version 0.10.0 the package expanded to include implementations of Energy Flow Networks (EFNs) and Particle Flow Networks (PFNs). As of version 0.11.0 , functions for facilitating the computation of the Energy Mover's Distance (EMD) on particle physics events are included. To summarize the main features: Energy Flow Polynomials : EFPs are a collection of jet substructure observables which form a complete linear basis of IRC-safe observables. EnergyFlow provides tools to compute EFPs on events for several energy and angular measures as well as custom measures. Energy Flow Networks : EFNs are infrared- and collinear-safe models designed for learning from collider events as unordered, variable-length sets of particles. EnergyFlow contains customizable Keras implementations of EFNs. Particle Flow Networks : PFNs are general models designed for learning from collider events as unordered, variable-length sets of particles, based on the Deep Sets framework. EnergyFlow contains customizable Keras implementations of PFNs. Energy Mover's Distance : The EMD is a common metric between probability distributions that has been adapted for use as a metric between collider events. EnergyFlow contains code to facilitate the computation of the EMD between events based on an underlying implementation provided by the Python Optimal Transport (POT) library. Beyond the primary functions described above, the EnergyFlow package also provides useful supplementary features. These include a large quark/gluon jet dataset, implementations of additional machine learning architectures useful for collider physics, and many examples exhibiting the usage of the package. Jet Tagging Datasets : A dataset of 2 million simulated quark and gluon jets is provided. Additional Architectures : Implementations of other architectures useful for particle physics are also provided, such as convolutional neural networks (CNNs) for jet images. Detailed Examples : Examples showcasing EFPs, EFNs, PFNs, EMDs, and more. Also see the demos . The current version is 0.13.0 . Changes are summarized in the Release Notes . Using the most up-to-date version is recommended. As of version 0.7.0 , tests have been written covering the majority of the EFP and EMD code. The architectures code is currently tested by running the examples. The source code can be found on GitHub . Get started by installing EnergyFlow , exploring the demos , and running the examples !","title":"Welcome to EnergyFlow"},{"location":"#references","text":"[1] P. T. Komiske, E. M. Metodiev, and J. Thaler, Energy Flow Polynomials: A complete linear basis for jet substructure , JHEP 04 (2018) 013 [ 1712.07124 ]. [2] P. T. Komiske, E. M. Metodiev, and J. Thaler, Energy Flow Networks: Deep Sets for Particle Jets , JHEP 01 (2019) 121 [ 1810.05165 ]. [3] P. T. Komiske, E. M. Metodiev, and J. Thaler, The Metric Space of Collider Events , Phys. Rev. Lett. 123 (2019) 041801 [ 1902.02346 ].","title":"References"},{"location":"#copyright","text":"See the LICENSE for detailed copyright information. EnergyFlow uses a customized einsumfunc.py from the NumPy GitHub repository as well as a few functions relating to downloading files copied from the Keras GitHub repository. The copyrights for these parts of the code are attributed to their respective owners in the LICENSE file.","title":"Copyright"},{"location":"demos/","text":"Interactive Demos Each of the following demos are provided as Jupyter notebooks that are available on GitHub. Binder provides an awesome platform for trying out these notebooks without installing anything whatsoever. EnergyFlow Demo The EnergyFlow Demo provides an introduction to using EnergyFlow to compute Energy Flow Polynomials. View or download the notebook from GitHub EMD Demo The EMD Demo provides an introduction to using EnergyFlow for computing the Energy Mover's Distance (EMD) between jets. View or download the notebook from GitHub MOD Jet Demo The MOD Jet Demo provides an introduction to using CMS Open Data in the MOD HDF5 format via EnergyFlow. Jets from the CMS 2011A Jet Primary Dataset have been processed into this easy-to-use format and are available on Zenodo along with corresponding simulated datasets. View or download the notebook from GitHub","title":"Demos"},{"location":"demos/#interactive-demos","text":"Each of the following demos are provided as Jupyter notebooks that are available on GitHub. Binder provides an awesome platform for trying out these notebooks without installing anything whatsoever.","title":"Interactive Demos"},{"location":"demos/#energyflow-demo","text":"The EnergyFlow Demo provides an introduction to using EnergyFlow to compute Energy Flow Polynomials. View or download the notebook from GitHub","title":"EnergyFlow Demo"},{"location":"demos/#emd-demo","text":"The EMD Demo provides an introduction to using EnergyFlow for computing the Energy Mover's Distance (EMD) between jets. View or download the notebook from GitHub","title":"EMD Demo"},{"location":"demos/#mod-jet-demo","text":"The MOD Jet Demo provides an introduction to using CMS Open Data in the MOD HDF5 format via EnergyFlow. Jets from the CMS 2011A Jet Primary Dataset have been processed into this easy-to-use format and are available on Zenodo along with corresponding simulated datasets. View or download the notebook from GitHub","title":"MOD Jet Demo"},{"location":"examples/","text":"There are currently 5 examples provided for the EnergyFlow package. They currently focus on demonstrating the various architectures included as part of EnergyFlow (see Architectures ). For examples involving the computation of EFPs or EMDs, see the demos . To install the examples to the default directory, ~/.energyflow/examples/ , simply run python -c \"import energyflow; energyflow.utils.get_examples()\" See the get_examples function for more detailed information. efp_example.py An example involving Energy Flow Polynomials (EFPs) and a linear classifier (Fisher's Linear Discriminant by default). First, the EFPSet class is used to compute the EFPs up to the specified dmax , the default being dmax=5 . Then linear classifiers are trained for different numbers of EFPs as input, determined by taking all EFPs up to degree d with d from 1 to dmax . The output of the example is a plot of the ROC curves for the classifiers with different numbers of EFP inputs. # standard library imports from __future__ import absolute_import, division, print_function # standard numerical library imports import numpy as np # energyflow imports import energyflow as ef from energyflow.archs import LinearClassifier from energyflow.datasets import qg_jets from energyflow.utils import data_split, standardize, to_categorical # attempt to import sklearn try: from sklearn.metrics import roc_auc_score, roc_curve except: print('please install scikit-learn in order to make ROC curves') roc_curve = False # attempt to import matplotlib try: import matplotlib.pyplot as plt except: print('please install matploltib in order to make plots') plt = False ################################### SETTINGS ################################### # data controls num_data = 20000 test_frac = 0.2 # efp parameters dmax = 5 measure = 'hadr' beta = 0.5 # plotting colors = ['tab:red', 'tab:orange', 'tab:olive', 'tab:green', 'tab:blue'] ################################################################################ # load data X, y = qg_jets.load(num_data) print('Loaded quark and gluon jets') # calculate EFPs print('Calculating d <= {} EFPs for {} jets... '.format(dmax, num_data), end='') efpset = ef.EFPSet(('d<=', dmax), measure='hadr', beta=beta) masked_X = [x[x[:,0] > 0] for x in X] X = efpset.batch_compute(masked_X) print('Done') # train models with different numbers of EFPs as input rocs = [] for d in range(1, dmax+1): # build architecture model = LinearClassifier(linclass_type='lda') # select EFPs with degree <= d X_d = X[:,efpset.sel(('d<=', d))] # do train/val/test split (X_train, X_test, y_train, y_test) = data_split(X_d, y, val=0, test=test_frac) print('Done train/val/test split') # train model model.fit(X_train, y_train) # get predictions on test data preds = model.predict(X_test) # get ROC curve if we have sklearn if roc_curve: rocs.append(roc_curve(y_test, preds[:,1])) # get area under the ROC curve auc = roc_auc_score(y_test, preds[:,1]) print() print('EFPs d <= {} AUC:'.format(d), auc) print() # make ROC curve plot if we have matplotlib if plt: # some nicer plot settings plt.rcParams['figure.figsize'] = (4,4) plt.rcParams['font.family'] = 'serif' plt.rcParams['figure.autolayout'] = True # iterate over the ROC curves and plot them for i,d in enumerate(range(1, dmax+1)): plt.plot(rocs[i][1], 1-rocs[i][0], '-', color=colors[i], label='LDA: d <= {} EFPs'.format(d)) # axes labels plt.xlabel('Quark Jet Efficiency') plt.ylabel('Gluon Jet Rejection') # axes limits plt.xlim(0, 1) plt.ylim(0, 1) # make legend and show plot plt.legend(loc='lower left', frameon=False) plt.show() efn_example.py An example involving Energy Flow Networks (EFNs), which were introduced in 1810.05165 . The EFN class is used to construct the network architecture. The output of the example is a plot of the ROC curves obtained by the EFN as well as the jet mass and constituent multiplicity observables. # standard library imports from __future__ import absolute_import, division, print_function # standard numerical library imports import numpy as np # energyflow imports import energyflow as ef from energyflow.archs import EFN from energyflow.datasets import qg_jets from energyflow.utils import data_split, to_categorical # attempt to import sklearn try: from sklearn.metrics import roc_auc_score, roc_curve except: print('please install scikit-learn in order to make ROC curves') roc_curve = False # attempt to import matplotlib try: import matplotlib.pyplot as plt except: print('please install matploltib in order to make plots') plt = False ################################### SETTINGS ################################## # the commented values correspond to those in 1810.05165 ############################################################################### # data controls, can go up to 2000000 total for full dataset train, val, test = 75000, 10000, 15000 # train, val, test = 1000000, 200000, 200000 # network architecture parameters Phi_sizes, F_sizes = (100, 100, 128), (100, 100, 100) # Phi_sizes, F_sizes = (100, 100, 256), (100, 100, 100) # network training parameters num_epoch = 5 batch_size = 500 ############################################################################### # load data X, y = qg_jets.load(train + val + test) # ignore pid information X = X[:,:,:3] # convert labels to categorical Y = to_categorical(y, num_classes=2) print('Loaded quark and gluon jets') # preprocess by centering jets and normalizing pts for x in X: mask = x[:,0] > 0 yphi_avg = np.average(x[mask,1:3], weights=x[mask,0], axis=0) x[mask,1:3] -= yphi_avg x[mask,0] /= x[:,0].sum() print('Finished preprocessing') # do train/val/test split (z_train, z_val, z_test, p_train, p_val, p_test, Y_train, Y_val, Y_test) = data_split(X[:,:,0], X[:,:,1:], Y, val=val, test=test) print('Done train/val/test split') print('Model summary:') # build architecture efn = EFN(input_dim=2, Phi_sizes=Phi_sizes, F_sizes=F_sizes) # train model efn.fit([z_train, p_train], Y_train, epochs=num_epoch, batch_size=batch_size, validation_data=([z_val, p_val], Y_val), verbose=1) # get predictions on test data preds = efn.predict([z_test, p_test], batch_size=1000) # get ROC curve if we have sklearn if roc_curve: efn_fp, efn_tp, threshs = roc_curve(Y_test[:,1], preds[:,1]) # get area under the ROC curve auc = roc_auc_score(Y_test[:,1], preds[:,1]) print() print('EFN AUC:', auc) print() # make ROC curve and filter plot if we have matplotlib if plt: # some nicer plot settings plt.rcParams['font.family'] = 'serif' plt.rcParams['figure.autolayout'] = True fig, axes = plt.subplots(1, 2, figsize=(8,4)) ######################### ROC Curve Plot ######################### # get multiplicity and mass for comparison masses = np.asarray([ef.ms_from_p4s(ef.p4s_from_ptyphims(x).sum(axis=0)) for x in X]) mults = np.asarray([np.count_nonzero(x[:,0]) for x in X]) mass_fp, mass_tp, threshs = roc_curve(Y[:,1], -masses) mult_fp, mult_tp, threshs = roc_curve(Y[:,1], -mults) # plot the ROC curves axes[0].plot(efn_tp, 1-efn_fp, '-', color='black', label='EFN') axes[0].plot(mass_tp, 1-mass_fp, '-', color='blue', label='Jet Mass') axes[0].plot(mult_tp, 1-mult_fp, '-', color='red', label='Multiplicity') # axes labels axes[0].set_xlabel('Quark Jet Efficiency') axes[0].set_ylabel('Gluon Jet Rejection') # axes limits axes[0].set_xlim(0, 1) axes[0].set_ylim(0, 1) # make legend and show plot axes[0].legend(loc='lower left', frameon=False) ######################### Filter Plot ######################### # plot settings R, n = 0.4, 100 colors = ['Reds', 'Oranges', 'Greens', 'Blues', 'Purples', 'Greys'] grads = np.linspace(0.45, 0.55, 4) # evaluate filters X, Y, Z = efn.eval_filters(R, n=n) # plot filters for i,z in enumerate(Z): axes[1].contourf(X, Y, z/np.max(z), grads, cmap=colors[i%len(colors)]) axes[1].set_xticks(np.linspace(-R, R, 5)) axes[1].set_yticks(np.linspace(-R, R, 5)) axes[1].set_xticklabels(['-R', '-R/2', '0', 'R/2', 'R']) axes[1].set_yticklabels(['-R', '-R/2', '0', 'R/2', 'R']) axes[1].set_xlabel('Translated Rapidity y') axes[1].set_ylabel('Translated Azimuthal Angle phi') axes[1].set_title('Energy Flow Network Latent Space', fontdict={'fontsize': 10}) plt.show() pfn_example.py An example involving Particle Flow Networks (PFNs), which were introduced in 1810.05165 . The PFN class is used to construct the network architecture. The output of the example is a plot of the ROC curves obtained by the PFN as well as the jet mass and constituent multiplicity observables. # standard library imports from __future__ import absolute_import, division, print_function # standard numerical library imports import numpy as np # energyflow imports import energyflow as ef from energyflow.archs import PFN from energyflow.datasets import qg_jets from energyflow.utils import data_split, remap_pids, to_categorical # attempt to import sklearn try: from sklearn.metrics import roc_auc_score, roc_curve except: print('please install scikit-learn in order to make ROC curves') roc_curve = False # attempt to import matplotlib try: import matplotlib.pyplot as plt except: print('please install matploltib in order to make plots') plt = False ################################### SETTINGS ################################### # the commented values correspond to those in 1810.05165 ############################################################################### # data controls, can go up to 2000000 for full dataset train, val, test = 75000, 10000, 15000 # train, val, test = 1000000, 200000, 200000 use_pids = True # network architecture parameters Phi_sizes, F_sizes = (100, 100, 128), (100, 100, 100) # Phi_sizes, F_sizes = (100, 100, 256), (100, 100, 100) # network training parameters num_epoch = 5 batch_size = 500 ################################################################################ # load data X, y = qg_jets.load(train + val + test) # convert labels to categorical Y = to_categorical(y, num_classes=2) print('Loaded quark and gluon jets') # preprocess by centering jets and normalizing pts for x in X: mask = x[:,0] > 0 yphi_avg = np.average(x[mask,1:3], weights=x[mask,0], axis=0) x[mask,1:3] -= yphi_avg x[mask,0] /= x[:,0].sum() # handle particle id channel if use_pids: remap_pids(X, pid_i=3) else: X = X[:,:,:3] print('Finished preprocessing') # do train/val/test split (X_train, X_val, X_test, Y_train, Y_val, Y_test) = data_split(X, Y, val=val, test=test) print('Done train/val/test split') print('Model summary:') # build architecture pfn = PFN(input_dim=X.shape[-1], Phi_sizes=Phi_sizes, F_sizes=F_sizes) # train model pfn.fit(X_train, Y_train, epochs=num_epoch, batch_size=batch_size, validation_data=(X_val, Y_val), verbose=1) # get predictions on test data preds = pfn.predict(X_test, batch_size=1000) # get ROC curve if we have sklearn if roc_curve: pfn_fp, pfn_tp, threshs = roc_curve(Y_test[:,1], preds[:,1]) # get area under the ROC curve auc = roc_auc_score(Y_test[:,1], preds[:,1]) print() print('PFN AUC:', auc) print() # make ROC curve plot if we have matplotlib if plt: # get multiplicity and mass for comparison masses = np.asarray([ef.ms_from_p4s(ef.p4s_from_ptyphims(x).sum(axis=0)) for x in X]) mults = np.asarray([np.count_nonzero(x[:,0]) for x in X]) mass_fp, mass_tp, threshs = roc_curve(Y[:,1], -masses) mult_fp, mult_tp, threshs = roc_curve(Y[:,1], -mults) # some nicer plot settings plt.rcParams['figure.figsize'] = (4,4) plt.rcParams['font.family'] = 'serif' plt.rcParams['figure.autolayout'] = True # plot the ROC curves plt.plot(pfn_tp, 1-pfn_fp, '-', color='black', label='PFN') plt.plot(mass_tp, 1-mass_fp, '-', color='blue', label='Jet Mass') plt.plot(mult_tp, 1-mult_fp, '-', color='red', label='Multiplicity') # axes labels plt.xlabel('Quark Jet Efficiency') plt.ylabel('Gluon Jet Rejection') # axes limits plt.xlim(0, 1) plt.ylim(0, 1) # make legend and show plot plt.legend(loc='lower left', frameon=False) plt.show() cnn_example.py An example involving jet images and convolutional neural networks (CNNs). The CNN class is used to provide a network architecture based on that described in 1612.01551 . Jet images are constructed using the pixelate function and can be either one-channel (grayscale), meaning that only $p_T$ information is used, or two-channel (color), meaning that $p_T$ information and local charged particle counts are used. The images are preprocessed by subtracting the average image in the training set and dividing by the per-pixel standard deviations, using the zero_center and standardize functions, respectively. The output of the example is a plot of the ROC curves of the CNN as well as the jet mass and constituent multiplicity observables. Note that the number of epochs is quite small because it is quite time consuming to train a CNN without a GPU (which will speed up this example immensely). # standard library imports from __future__ import absolute_import, division, print_function # standard numerical library imports import numpy as np # energyflow imports import energyflow as ef from energyflow.archs import CNN from energyflow.datasets import qg_jets from energyflow.utils import data_split, pixelate, standardize, to_categorical, zero_center # attempt to import sklearn try: from sklearn.metrics import roc_auc_score, roc_curve except: print('please install scikit-learn in order to make ROC curves') roc_curve = False # attempt to import matplotlib try: import matplotlib.pyplot as plt except: print('please install matploltib in order to make plots') plt = False ################################### SETTINGS ################################### # data controls num_data = 100000 val_frac, test_frac = 0.1, 0.15 # image parameters R = 0.4 img_width = 2*R npix = 33 nb_chan = 2 norm = True # required network architecture parameters input_shape = (nb_chan, npix, npix) filter_sizes = [8, 4, 4] num_filters = [8, 8, 8] # very small so can run on non-GPUs in reasonable time # optional network architecture parameters dense_sizes = [50] pool_sizes = 2 # network training parameters num_epoch = 2 batch_size = 100 ################################################################################ # load data X, y = qg_jets.load(num_data=num_data) # convert labels to categorical Y = to_categorical(y, num_classes=2) print('Loaded quark and gluon jets') # make jet images images = np.asarray([pixelate(x, npix=npix, img_width=img_width, nb_chan=nb_chan, charged_counts_only=True, norm=norm) for x in X]) print('Done making jet images') # do train/val/test split (X_train, X_val, X_test, Y_train, Y_val, Y_test) = data_split(images, Y, val=val_frac, test=test_frac) print('Done train/val/test split') # preprocess by zero centering images and standardizing each pixel X_train, X_val, X_test = standardize(*zero_center(X_train, X_val, X_test)) print('Finished preprocessing') print('Model summary:') # build architecture hps = {'input_shape': input_shape, 'filter_sizes': filter_sizes, 'num_filters': num_filters, 'dense_sizes': dense_sizes, 'pool_sizes': pool_sizes} cnn = CNN(hps) # train model cnn.fit(X_train, Y_train, epochs=num_epoch, batch_size=batch_size, validation_data=(X_val, Y_val), verbose=1) # get predictions on test data preds = cnn.predict(X_test, batch_size=1000) # get ROC curve if we have sklearn if roc_curve: cnn_fp, cnn_tp, threshs = roc_curve(Y_test[:,1], preds[:,1]) # get area under the ROC curve auc = roc_auc_score(Y_test[:,1], preds[:,1]) print() print('CNN AUC:', auc) print() # make ROC curve plot if we have matplotlib if plt: # get multiplicity and mass for comparison masses = np.asarray([ef.ms_from_p4s(ef.p4s_from_ptyphims(x).sum(axis=0)) for x in X]) mults = np.asarray([np.count_nonzero(x[:,0]) for x in X]) mass_fp, mass_tp, threshs = roc_curve(Y[:,1], -masses) mult_fp, mult_tp, threshs = roc_curve(Y[:,1], -mults) # some nicer plot settings plt.rcParams['figure.figsize'] = (4,4) plt.rcParams['font.family'] = 'serif' plt.rcParams['figure.autolayout'] = True # plot the ROC curves plt.plot(cnn_tp, 1-cnn_fp, '-', color='black', label='CNN') plt.plot(mass_tp, 1-mass_fp, '-', color='blue', label='Jet Mass') plt.plot(mult_tp, 1-mult_fp, '-', color='red', label='Multiplicity') # axes labels plt.xlabel('Quark Jet Efficiency') plt.ylabel('Gluon Jet Rejection') # axes limits plt.xlim(0, 1) plt.ylim(0, 1) # make legend and show plot plt.legend(loc='lower left', frameon=False) plt.show() dnn_example.py An example involving deep, fully-connected neural networks (DNNs). The DNN class is used to construct the network architecture. The inputs are taken to be the $N$-subjettiness observables as specified as part of the phase space basis from 1704.08249 , cut off at some total number of observables. The output of the example is a plot showing the ROC curves obtained from training the DNN on different numbers of $N$-subjettiness observables. # standard library imports from __future__ import absolute_import, division, print_function # standard numerical library imports import numpy as np # energyflow imports import energyflow as ef from energyflow.archs import DNN from energyflow.datasets import qg_nsubs from energyflow.utils import data_split, to_categorical # attempt to import sklearn try: from sklearn.metrics import roc_auc_score, roc_curve except: print('please install scikit-learn in order to make ROC curves') roc_curve = False # attempt to import matplotlib try: import matplotlib.pyplot as plt except: print('please install matploltib in order to make plots') plt = False ################################### SETTINGS ################################### # data controls num_data = 100000 val_frac, test_frac = 0.1, 0.15 # network architecture parameters dense_sizes = (100, 100) # network training parameters num_epoch = 10 batch_size = 100 # sweep parameters num_nsubs = [1, 2, 4, 8, 16, 32] colors = ['tab:red', 'tab:orange', 'tab:olive', 'tab:green', 'tab:blue', 'tab:purple'] ################################################################################ # load data X, y = qg_nsubs.load(num_data=num_data) # convert labels to categorical Y = to_categorical(y, num_classes=2) print('Loaded quark and gluon jets') print('Model summary:') # train models with different numbers of nsubs as input rocs = [] for i,num_nsub in enumerate(num_nsubs): # build architecture dnn = DNN(input_dim=num_nsub, dense_sizes=dense_sizes, summary=(i==0)) # do train/val/test split (X_train, X_val, X_test, Y_train, Y_val, Y_test) = data_split(X[:,:num_nsub], Y, val=val_frac, test=test_frac) print('Done train/val/test split') # train model dnn.fit(X_train, Y_train, epochs=num_epoch, batch_size=batch_size, validation_data=(X_val, Y_val), verbose=1) # get predictions on test data preds = dnn.predict(X_test, batch_size=1000) # get ROC curve if we have sklearn if roc_curve: rocs.append(roc_curve(Y_test[:,1], preds[:,1])) # get area under the ROC curve auc = roc_auc_score(Y_test[:,1], preds[:,1]) print() print('{} nsubs DNN AUC:'.format(num_nsub), auc) print() # make ROC curve plot if we have matplotlib if plt: # some nicer plot settings plt.rcParams['figure.figsize'] = (4,4) plt.rcParams['font.family'] = 'serif' plt.rcParams['figure.autolayout'] = True # iterate over the ROC curves and plot them for i in range(len(rocs)): plt.plot(rocs[i][1], 1-rocs[i][0], '-', color=colors[i], label='DNN: {} N-subs'.format(num_nsubs[i])) # axes labels plt.xlabel('Quark Jet Efficiency') plt.ylabel('Gluon Jet Rejection') # axes limits plt.xlim(0, 1) plt.ylim(0, 1) # make legend and show plot plt.legend(loc='lower left', frameon=False) plt.show()","title":"Examples"},{"location":"examples/#efp_examplepy","text":"An example involving Energy Flow Polynomials (EFPs) and a linear classifier (Fisher's Linear Discriminant by default). First, the EFPSet class is used to compute the EFPs up to the specified dmax , the default being dmax=5 . Then linear classifiers are trained for different numbers of EFPs as input, determined by taking all EFPs up to degree d with d from 1 to dmax . The output of the example is a plot of the ROC curves for the classifiers with different numbers of EFP inputs. # standard library imports from __future__ import absolute_import, division, print_function # standard numerical library imports import numpy as np # energyflow imports import energyflow as ef from energyflow.archs import LinearClassifier from energyflow.datasets import qg_jets from energyflow.utils import data_split, standardize, to_categorical # attempt to import sklearn try: from sklearn.metrics import roc_auc_score, roc_curve except: print('please install scikit-learn in order to make ROC curves') roc_curve = False # attempt to import matplotlib try: import matplotlib.pyplot as plt except: print('please install matploltib in order to make plots') plt = False ################################### SETTINGS ################################### # data controls num_data = 20000 test_frac = 0.2 # efp parameters dmax = 5 measure = 'hadr' beta = 0.5 # plotting colors = ['tab:red', 'tab:orange', 'tab:olive', 'tab:green', 'tab:blue'] ################################################################################ # load data X, y = qg_jets.load(num_data) print('Loaded quark and gluon jets') # calculate EFPs print('Calculating d <= {} EFPs for {} jets... '.format(dmax, num_data), end='') efpset = ef.EFPSet(('d<=', dmax), measure='hadr', beta=beta) masked_X = [x[x[:,0] > 0] for x in X] X = efpset.batch_compute(masked_X) print('Done') # train models with different numbers of EFPs as input rocs = [] for d in range(1, dmax+1): # build architecture model = LinearClassifier(linclass_type='lda') # select EFPs with degree <= d X_d = X[:,efpset.sel(('d<=', d))] # do train/val/test split (X_train, X_test, y_train, y_test) = data_split(X_d, y, val=0, test=test_frac) print('Done train/val/test split') # train model model.fit(X_train, y_train) # get predictions on test data preds = model.predict(X_test) # get ROC curve if we have sklearn if roc_curve: rocs.append(roc_curve(y_test, preds[:,1])) # get area under the ROC curve auc = roc_auc_score(y_test, preds[:,1]) print() print('EFPs d <= {} AUC:'.format(d), auc) print() # make ROC curve plot if we have matplotlib if plt: # some nicer plot settings plt.rcParams['figure.figsize'] = (4,4) plt.rcParams['font.family'] = 'serif' plt.rcParams['figure.autolayout'] = True # iterate over the ROC curves and plot them for i,d in enumerate(range(1, dmax+1)): plt.plot(rocs[i][1], 1-rocs[i][0], '-', color=colors[i], label='LDA: d <= {} EFPs'.format(d)) # axes labels plt.xlabel('Quark Jet Efficiency') plt.ylabel('Gluon Jet Rejection') # axes limits plt.xlim(0, 1) plt.ylim(0, 1) # make legend and show plot plt.legend(loc='lower left', frameon=False) plt.show()","title":"efp_example.py"},{"location":"examples/#efn_examplepy","text":"An example involving Energy Flow Networks (EFNs), which were introduced in 1810.05165 . The EFN class is used to construct the network architecture. The output of the example is a plot of the ROC curves obtained by the EFN as well as the jet mass and constituent multiplicity observables. # standard library imports from __future__ import absolute_import, division, print_function # standard numerical library imports import numpy as np # energyflow imports import energyflow as ef from energyflow.archs import EFN from energyflow.datasets import qg_jets from energyflow.utils import data_split, to_categorical # attempt to import sklearn try: from sklearn.metrics import roc_auc_score, roc_curve except: print('please install scikit-learn in order to make ROC curves') roc_curve = False # attempt to import matplotlib try: import matplotlib.pyplot as plt except: print('please install matploltib in order to make plots') plt = False ################################### SETTINGS ################################## # the commented values correspond to those in 1810.05165 ############################################################################### # data controls, can go up to 2000000 total for full dataset train, val, test = 75000, 10000, 15000 # train, val, test = 1000000, 200000, 200000 # network architecture parameters Phi_sizes, F_sizes = (100, 100, 128), (100, 100, 100) # Phi_sizes, F_sizes = (100, 100, 256), (100, 100, 100) # network training parameters num_epoch = 5 batch_size = 500 ############################################################################### # load data X, y = qg_jets.load(train + val + test) # ignore pid information X = X[:,:,:3] # convert labels to categorical Y = to_categorical(y, num_classes=2) print('Loaded quark and gluon jets') # preprocess by centering jets and normalizing pts for x in X: mask = x[:,0] > 0 yphi_avg = np.average(x[mask,1:3], weights=x[mask,0], axis=0) x[mask,1:3] -= yphi_avg x[mask,0] /= x[:,0].sum() print('Finished preprocessing') # do train/val/test split (z_train, z_val, z_test, p_train, p_val, p_test, Y_train, Y_val, Y_test) = data_split(X[:,:,0], X[:,:,1:], Y, val=val, test=test) print('Done train/val/test split') print('Model summary:') # build architecture efn = EFN(input_dim=2, Phi_sizes=Phi_sizes, F_sizes=F_sizes) # train model efn.fit([z_train, p_train], Y_train, epochs=num_epoch, batch_size=batch_size, validation_data=([z_val, p_val], Y_val), verbose=1) # get predictions on test data preds = efn.predict([z_test, p_test], batch_size=1000) # get ROC curve if we have sklearn if roc_curve: efn_fp, efn_tp, threshs = roc_curve(Y_test[:,1], preds[:,1]) # get area under the ROC curve auc = roc_auc_score(Y_test[:,1], preds[:,1]) print() print('EFN AUC:', auc) print() # make ROC curve and filter plot if we have matplotlib if plt: # some nicer plot settings plt.rcParams['font.family'] = 'serif' plt.rcParams['figure.autolayout'] = True fig, axes = plt.subplots(1, 2, figsize=(8,4)) ######################### ROC Curve Plot ######################### # get multiplicity and mass for comparison masses = np.asarray([ef.ms_from_p4s(ef.p4s_from_ptyphims(x).sum(axis=0)) for x in X]) mults = np.asarray([np.count_nonzero(x[:,0]) for x in X]) mass_fp, mass_tp, threshs = roc_curve(Y[:,1], -masses) mult_fp, mult_tp, threshs = roc_curve(Y[:,1], -mults) # plot the ROC curves axes[0].plot(efn_tp, 1-efn_fp, '-', color='black', label='EFN') axes[0].plot(mass_tp, 1-mass_fp, '-', color='blue', label='Jet Mass') axes[0].plot(mult_tp, 1-mult_fp, '-', color='red', label='Multiplicity') # axes labels axes[0].set_xlabel('Quark Jet Efficiency') axes[0].set_ylabel('Gluon Jet Rejection') # axes limits axes[0].set_xlim(0, 1) axes[0].set_ylim(0, 1) # make legend and show plot axes[0].legend(loc='lower left', frameon=False) ######################### Filter Plot ######################### # plot settings R, n = 0.4, 100 colors = ['Reds', 'Oranges', 'Greens', 'Blues', 'Purples', 'Greys'] grads = np.linspace(0.45, 0.55, 4) # evaluate filters X, Y, Z = efn.eval_filters(R, n=n) # plot filters for i,z in enumerate(Z): axes[1].contourf(X, Y, z/np.max(z), grads, cmap=colors[i%len(colors)]) axes[1].set_xticks(np.linspace(-R, R, 5)) axes[1].set_yticks(np.linspace(-R, R, 5)) axes[1].set_xticklabels(['-R', '-R/2', '0', 'R/2', 'R']) axes[1].set_yticklabels(['-R', '-R/2', '0', 'R/2', 'R']) axes[1].set_xlabel('Translated Rapidity y') axes[1].set_ylabel('Translated Azimuthal Angle phi') axes[1].set_title('Energy Flow Network Latent Space', fontdict={'fontsize': 10}) plt.show()","title":"efn_example.py"},{"location":"examples/#pfn_examplepy","text":"An example involving Particle Flow Networks (PFNs), which were introduced in 1810.05165 . The PFN class is used to construct the network architecture. The output of the example is a plot of the ROC curves obtained by the PFN as well as the jet mass and constituent multiplicity observables. # standard library imports from __future__ import absolute_import, division, print_function # standard numerical library imports import numpy as np # energyflow imports import energyflow as ef from energyflow.archs import PFN from energyflow.datasets import qg_jets from energyflow.utils import data_split, remap_pids, to_categorical # attempt to import sklearn try: from sklearn.metrics import roc_auc_score, roc_curve except: print('please install scikit-learn in order to make ROC curves') roc_curve = False # attempt to import matplotlib try: import matplotlib.pyplot as plt except: print('please install matploltib in order to make plots') plt = False ################################### SETTINGS ################################### # the commented values correspond to those in 1810.05165 ############################################################################### # data controls, can go up to 2000000 for full dataset train, val, test = 75000, 10000, 15000 # train, val, test = 1000000, 200000, 200000 use_pids = True # network architecture parameters Phi_sizes, F_sizes = (100, 100, 128), (100, 100, 100) # Phi_sizes, F_sizes = (100, 100, 256), (100, 100, 100) # network training parameters num_epoch = 5 batch_size = 500 ################################################################################ # load data X, y = qg_jets.load(train + val + test) # convert labels to categorical Y = to_categorical(y, num_classes=2) print('Loaded quark and gluon jets') # preprocess by centering jets and normalizing pts for x in X: mask = x[:,0] > 0 yphi_avg = np.average(x[mask,1:3], weights=x[mask,0], axis=0) x[mask,1:3] -= yphi_avg x[mask,0] /= x[:,0].sum() # handle particle id channel if use_pids: remap_pids(X, pid_i=3) else: X = X[:,:,:3] print('Finished preprocessing') # do train/val/test split (X_train, X_val, X_test, Y_train, Y_val, Y_test) = data_split(X, Y, val=val, test=test) print('Done train/val/test split') print('Model summary:') # build architecture pfn = PFN(input_dim=X.shape[-1], Phi_sizes=Phi_sizes, F_sizes=F_sizes) # train model pfn.fit(X_train, Y_train, epochs=num_epoch, batch_size=batch_size, validation_data=(X_val, Y_val), verbose=1) # get predictions on test data preds = pfn.predict(X_test, batch_size=1000) # get ROC curve if we have sklearn if roc_curve: pfn_fp, pfn_tp, threshs = roc_curve(Y_test[:,1], preds[:,1]) # get area under the ROC curve auc = roc_auc_score(Y_test[:,1], preds[:,1]) print() print('PFN AUC:', auc) print() # make ROC curve plot if we have matplotlib if plt: # get multiplicity and mass for comparison masses = np.asarray([ef.ms_from_p4s(ef.p4s_from_ptyphims(x).sum(axis=0)) for x in X]) mults = np.asarray([np.count_nonzero(x[:,0]) for x in X]) mass_fp, mass_tp, threshs = roc_curve(Y[:,1], -masses) mult_fp, mult_tp, threshs = roc_curve(Y[:,1], -mults) # some nicer plot settings plt.rcParams['figure.figsize'] = (4,4) plt.rcParams['font.family'] = 'serif' plt.rcParams['figure.autolayout'] = True # plot the ROC curves plt.plot(pfn_tp, 1-pfn_fp, '-', color='black', label='PFN') plt.plot(mass_tp, 1-mass_fp, '-', color='blue', label='Jet Mass') plt.plot(mult_tp, 1-mult_fp, '-', color='red', label='Multiplicity') # axes labels plt.xlabel('Quark Jet Efficiency') plt.ylabel('Gluon Jet Rejection') # axes limits plt.xlim(0, 1) plt.ylim(0, 1) # make legend and show plot plt.legend(loc='lower left', frameon=False) plt.show()","title":"pfn_example.py"},{"location":"examples/#cnn_examplepy","text":"An example involving jet images and convolutional neural networks (CNNs). The CNN class is used to provide a network architecture based on that described in 1612.01551 . Jet images are constructed using the pixelate function and can be either one-channel (grayscale), meaning that only $p_T$ information is used, or two-channel (color), meaning that $p_T$ information and local charged particle counts are used. The images are preprocessed by subtracting the average image in the training set and dividing by the per-pixel standard deviations, using the zero_center and standardize functions, respectively. The output of the example is a plot of the ROC curves of the CNN as well as the jet mass and constituent multiplicity observables. Note that the number of epochs is quite small because it is quite time consuming to train a CNN without a GPU (which will speed up this example immensely). # standard library imports from __future__ import absolute_import, division, print_function # standard numerical library imports import numpy as np # energyflow imports import energyflow as ef from energyflow.archs import CNN from energyflow.datasets import qg_jets from energyflow.utils import data_split, pixelate, standardize, to_categorical, zero_center # attempt to import sklearn try: from sklearn.metrics import roc_auc_score, roc_curve except: print('please install scikit-learn in order to make ROC curves') roc_curve = False # attempt to import matplotlib try: import matplotlib.pyplot as plt except: print('please install matploltib in order to make plots') plt = False ################################### SETTINGS ################################### # data controls num_data = 100000 val_frac, test_frac = 0.1, 0.15 # image parameters R = 0.4 img_width = 2*R npix = 33 nb_chan = 2 norm = True # required network architecture parameters input_shape = (nb_chan, npix, npix) filter_sizes = [8, 4, 4] num_filters = [8, 8, 8] # very small so can run on non-GPUs in reasonable time # optional network architecture parameters dense_sizes = [50] pool_sizes = 2 # network training parameters num_epoch = 2 batch_size = 100 ################################################################################ # load data X, y = qg_jets.load(num_data=num_data) # convert labels to categorical Y = to_categorical(y, num_classes=2) print('Loaded quark and gluon jets') # make jet images images = np.asarray([pixelate(x, npix=npix, img_width=img_width, nb_chan=nb_chan, charged_counts_only=True, norm=norm) for x in X]) print('Done making jet images') # do train/val/test split (X_train, X_val, X_test, Y_train, Y_val, Y_test) = data_split(images, Y, val=val_frac, test=test_frac) print('Done train/val/test split') # preprocess by zero centering images and standardizing each pixel X_train, X_val, X_test = standardize(*zero_center(X_train, X_val, X_test)) print('Finished preprocessing') print('Model summary:') # build architecture hps = {'input_shape': input_shape, 'filter_sizes': filter_sizes, 'num_filters': num_filters, 'dense_sizes': dense_sizes, 'pool_sizes': pool_sizes} cnn = CNN(hps) # train model cnn.fit(X_train, Y_train, epochs=num_epoch, batch_size=batch_size, validation_data=(X_val, Y_val), verbose=1) # get predictions on test data preds = cnn.predict(X_test, batch_size=1000) # get ROC curve if we have sklearn if roc_curve: cnn_fp, cnn_tp, threshs = roc_curve(Y_test[:,1], preds[:,1]) # get area under the ROC curve auc = roc_auc_score(Y_test[:,1], preds[:,1]) print() print('CNN AUC:', auc) print() # make ROC curve plot if we have matplotlib if plt: # get multiplicity and mass for comparison masses = np.asarray([ef.ms_from_p4s(ef.p4s_from_ptyphims(x).sum(axis=0)) for x in X]) mults = np.asarray([np.count_nonzero(x[:,0]) for x in X]) mass_fp, mass_tp, threshs = roc_curve(Y[:,1], -masses) mult_fp, mult_tp, threshs = roc_curve(Y[:,1], -mults) # some nicer plot settings plt.rcParams['figure.figsize'] = (4,4) plt.rcParams['font.family'] = 'serif' plt.rcParams['figure.autolayout'] = True # plot the ROC curves plt.plot(cnn_tp, 1-cnn_fp, '-', color='black', label='CNN') plt.plot(mass_tp, 1-mass_fp, '-', color='blue', label='Jet Mass') plt.plot(mult_tp, 1-mult_fp, '-', color='red', label='Multiplicity') # axes labels plt.xlabel('Quark Jet Efficiency') plt.ylabel('Gluon Jet Rejection') # axes limits plt.xlim(0, 1) plt.ylim(0, 1) # make legend and show plot plt.legend(loc='lower left', frameon=False) plt.show()","title":"cnn_example.py"},{"location":"examples/#dnn_examplepy","text":"An example involving deep, fully-connected neural networks (DNNs). The DNN class is used to construct the network architecture. The inputs are taken to be the $N$-subjettiness observables as specified as part of the phase space basis from 1704.08249 , cut off at some total number of observables. The output of the example is a plot showing the ROC curves obtained from training the DNN on different numbers of $N$-subjettiness observables. # standard library imports from __future__ import absolute_import, division, print_function # standard numerical library imports import numpy as np # energyflow imports import energyflow as ef from energyflow.archs import DNN from energyflow.datasets import qg_nsubs from energyflow.utils import data_split, to_categorical # attempt to import sklearn try: from sklearn.metrics import roc_auc_score, roc_curve except: print('please install scikit-learn in order to make ROC curves') roc_curve = False # attempt to import matplotlib try: import matplotlib.pyplot as plt except: print('please install matploltib in order to make plots') plt = False ################################### SETTINGS ################################### # data controls num_data = 100000 val_frac, test_frac = 0.1, 0.15 # network architecture parameters dense_sizes = (100, 100) # network training parameters num_epoch = 10 batch_size = 100 # sweep parameters num_nsubs = [1, 2, 4, 8, 16, 32] colors = ['tab:red', 'tab:orange', 'tab:olive', 'tab:green', 'tab:blue', 'tab:purple'] ################################################################################ # load data X, y = qg_nsubs.load(num_data=num_data) # convert labels to categorical Y = to_categorical(y, num_classes=2) print('Loaded quark and gluon jets') print('Model summary:') # train models with different numbers of nsubs as input rocs = [] for i,num_nsub in enumerate(num_nsubs): # build architecture dnn = DNN(input_dim=num_nsub, dense_sizes=dense_sizes, summary=(i==0)) # do train/val/test split (X_train, X_val, X_test, Y_train, Y_val, Y_test) = data_split(X[:,:num_nsub], Y, val=val_frac, test=test_frac) print('Done train/val/test split') # train model dnn.fit(X_train, Y_train, epochs=num_epoch, batch_size=batch_size, validation_data=(X_val, Y_val), verbose=1) # get predictions on test data preds = dnn.predict(X_test, batch_size=1000) # get ROC curve if we have sklearn if roc_curve: rocs.append(roc_curve(Y_test[:,1], preds[:,1])) # get area under the ROC curve auc = roc_auc_score(Y_test[:,1], preds[:,1]) print() print('{} nsubs DNN AUC:'.format(num_nsub), auc) print() # make ROC curve plot if we have matplotlib if plt: # some nicer plot settings plt.rcParams['figure.figsize'] = (4,4) plt.rcParams['font.family'] = 'serif' plt.rcParams['figure.autolayout'] = True # iterate over the ROC curves and plot them for i in range(len(rocs)): plt.plot(rocs[i][1], 1-rocs[i][0], '-', color=colors[i], label='DNN: {} N-subs'.format(num_nsubs[i])) # axes labels plt.xlabel('Quark Jet Efficiency') plt.ylabel('Gluon Jet Rejection') # axes limits plt.xlim(0, 1) plt.ylim(0, 1) # make legend and show plot plt.legend(loc='lower left', frameon=False) plt.show()","title":"dnn_example.py"},{"location":"faqs/","text":"Frequently Asked EnergyFlow Questions How do I cite the EnergyFlow package? Why Python instead of C++? Can I contribute to the code? How do I report an issue or a bug? Where can I get graph image files? How do I cite the EnergyFlow package? Please cite the relevant papers if they or this package help your research. Here are the BibTeX entries to use: @article{Komiske:2017aww, author = \"Komiske, Patrick T. and Metodiev, Eric M. and Thaler, Jesse\", title = \"{Energy Flow Polynomials: A complete linear basis for jet substructure}\", journal = \"JHEP\", volume = \"04\", year = \"2018\", pages = \"013\", doi = \"10.1007/JHEP04(2018)013\", eprint = \"1712.07124\", archivePrefix = \"arXiv\", primaryClass = \"hep-ph\", reportNumber = \"MIT-CTP-4965\" } @article{Komiske:2018cqr, author = \"Komiske, Patrick T. and Metodiev, Eric M. and Thaler, Jesse\", title = \"{Energy Flow Networks: Deep Sets for Particle Jets}\", journal = \"JHEP\", volume = \"01\", year = \"2019\", pages = \"121\", doi = \"10.1007/JHEP01(2019)121\", eprint = \"1810.05165\", archivePrefix = \"arXiv\", primaryClass = \"hep-ph\", reportNumber = \"MIT-CTP 5064\" } @article{Komiske:2019fks, author = \"Komiske, Patrick T. and Metodiev, Eric M. and Thaler, Jesse\", title = \"{The Metric Space of Collider Events}\", year = \"2019\", eprint = \"1902.02346\", archivePrefix = \"arXiv\", primaryClass = \"hep-ph\", reportNumber = \"MIT-CTP 5102\" } In addition to the above papers, we ask that you cite the quark and gluon datasets directly. Here are the BibTex entries to use: @misc{komiske_patrick_2019_3164691, author = {Komiske, Patrick and Metodiev, Eric and Thaler, Jesse}, title = {Pythia8 Quark and Gluon Jets for Energy Flow}, month = may, year = 2019, doi = {10.5281/zenodo.3164691}, url = {https://doi.org/10.5281/zenodo.3164691} } @misc{pathak_aditya_2019_3066475, author = {Pathak, Aditya and Komiske, Patrick and Metodiev, Eric and Schwartz, Matthew}, title = {Herwig7.1 Quark and Gluon Jets}, month = may, year = 2019, doi = {10.5281/zenodo.3066475}, url = {https://doi.org/10.5281/zenodo.3066475} } - Pythia samples - Herwig samples Why Python instead of C++? Computing the EFPs requires a function such as NumPy's einsum that can efficiently evaluate arbitrary tensor contractions. To write such a function from scratch in C++ is difficult, and there is no obvious library in C++ to use (though if one were to attempt this the tensor algebra compiler seems like a promising tool). NumPy is a highly-optimized Python library written in C that provides all of the tools required to efficiently compute the EFPs. Libraries like NumPy take advantage of optimizations that the physicist-programmer typically does not, such as architecture-optimized libraries like BLAS or LAPACK and low-level features such as SSE/AVX instructions. Beyond just computing EFPs, EnergyFlow makes use of other libraries written in python including Keras and scikit-learn for the architecture implementations, POT and SciPy for EMD computations, and matplotlib for the examples. Can I contribute to the code? All of our code is open source and hosted on GitHub . We welcome additional contributors, and if you are interested in getting involved please contact us directly. Contact information is included in the relevant Energy Flow papers and our GitHub repository. How do I report an issue? Please let us know of any issues you encounter as soon as possible by creating a GitHub Issue . Where can I get graph image files? Image files for all connected multigraphs with up to 7 edges in the EFP style are available as PDF files here . You are free to use them with the proper attribution.","title":"FAQs"},{"location":"faqs/#frequently-asked-energyflow-questions","text":"How do I cite the EnergyFlow package? Why Python instead of C++? Can I contribute to the code? How do I report an issue or a bug? Where can I get graph image files?","title":"Frequently Asked EnergyFlow Questions"},{"location":"faqs/#how-do-i-cite-the-energyflow-package","text":"Please cite the relevant papers if they or this package help your research. Here are the BibTeX entries to use: @article{Komiske:2017aww, author = \"Komiske, Patrick T. and Metodiev, Eric M. and Thaler, Jesse\", title = \"{Energy Flow Polynomials: A complete linear basis for jet substructure}\", journal = \"JHEP\", volume = \"04\", year = \"2018\", pages = \"013\", doi = \"10.1007/JHEP04(2018)013\", eprint = \"1712.07124\", archivePrefix = \"arXiv\", primaryClass = \"hep-ph\", reportNumber = \"MIT-CTP-4965\" } @article{Komiske:2018cqr, author = \"Komiske, Patrick T. and Metodiev, Eric M. and Thaler, Jesse\", title = \"{Energy Flow Networks: Deep Sets for Particle Jets}\", journal = \"JHEP\", volume = \"01\", year = \"2019\", pages = \"121\", doi = \"10.1007/JHEP01(2019)121\", eprint = \"1810.05165\", archivePrefix = \"arXiv\", primaryClass = \"hep-ph\", reportNumber = \"MIT-CTP 5064\" } @article{Komiske:2019fks, author = \"Komiske, Patrick T. and Metodiev, Eric M. and Thaler, Jesse\", title = \"{The Metric Space of Collider Events}\", year = \"2019\", eprint = \"1902.02346\", archivePrefix = \"arXiv\", primaryClass = \"hep-ph\", reportNumber = \"MIT-CTP 5102\" } In addition to the above papers, we ask that you cite the quark and gluon datasets directly. Here are the BibTex entries to use: @misc{komiske_patrick_2019_3164691, author = {Komiske, Patrick and Metodiev, Eric and Thaler, Jesse}, title = {Pythia8 Quark and Gluon Jets for Energy Flow}, month = may, year = 2019, doi = {10.5281/zenodo.3164691}, url = {https://doi.org/10.5281/zenodo.3164691} } @misc{pathak_aditya_2019_3066475, author = {Pathak, Aditya and Komiske, Patrick and Metodiev, Eric and Schwartz, Matthew}, title = {Herwig7.1 Quark and Gluon Jets}, month = may, year = 2019, doi = {10.5281/zenodo.3066475}, url = {https://doi.org/10.5281/zenodo.3066475} } - Pythia samples - Herwig samples","title":"How do I cite the EnergyFlow package?"},{"location":"faqs/#why-python-instead-of-c","text":"Computing the EFPs requires a function such as NumPy's einsum that can efficiently evaluate arbitrary tensor contractions. To write such a function from scratch in C++ is difficult, and there is no obvious library in C++ to use (though if one were to attempt this the tensor algebra compiler seems like a promising tool). NumPy is a highly-optimized Python library written in C that provides all of the tools required to efficiently compute the EFPs. Libraries like NumPy take advantage of optimizations that the physicist-programmer typically does not, such as architecture-optimized libraries like BLAS or LAPACK and low-level features such as SSE/AVX instructions. Beyond just computing EFPs, EnergyFlow makes use of other libraries written in python including Keras and scikit-learn for the architecture implementations, POT and SciPy for EMD computations, and matplotlib for the examples.","title":"Why Python instead of C++?"},{"location":"faqs/#can-i-contribute-to-the-code","text":"All of our code is open source and hosted on GitHub . We welcome additional contributors, and if you are interested in getting involved please contact us directly. Contact information is included in the relevant Energy Flow papers and our GitHub repository.","title":"Can I contribute to the code?"},{"location":"faqs/#how-do-i-report-an-issue","text":"Please let us know of any issues you encounter as soon as possible by creating a GitHub Issue .","title":"How do I report an issue?"},{"location":"faqs/#where-can-i-get-graph-image-files","text":"Image files for all connected multigraphs with up to 7 edges in the EFP style are available as PDF files here . You are free to use them with the proper attribution.","title":"Where can I get graph image files?"},{"location":"installation/","text":"The EnergyFlow package is written in pure Python and the core depends only on NumPy, the fundamental package for scientific computing with Python, and six, which is a lightweight module to patch some inconvenient differences between Python 2 and Python 3. The extra features require additional packages as specified below: Architectures: Keras , scikit-learn . EMD: Python Optimal Transport (POT) . Multigraph Generation: iGraph The EnergyFlow package is designed to work with Python 2.7, 3.5, 3.6, and 3.7. These can be installed from here . A recent version of Python 3 is highly recommended, ideally 3.6 or higher. Install via pip To install from PyPI using pip , make sure you have one of the supported versions of Python installed and that pip is available in the system path. Simply execute pip install energyflow and EnergyFlow will be installed in your default location for Python packages. NumPy As of version 0.8.2 , EnergyFlow has used a modified version of numpy.einsum to do the heavy lifting for the computation of EFPs. NumPy 1.14.0 changed einsum to use tensordot when possible compared to 1.13.3 , which only used c_einsum . It was found that the multi-process approach used by batch_compute is typically much faster when using the inherently single-threaded c_einsum versus tensordot , which can call BLAS. Hence the custom version of einsum shipped with EnergyFlow disables all calls to tensordot and uses only c_einsum .","title":"Installation"},{"location":"installation/#install-via-pip","text":"To install from PyPI using pip , make sure you have one of the supported versions of Python installed and that pip is available in the system path. Simply execute pip install energyflow and EnergyFlow will be installed in your default location for Python packages.","title":"Install via pip"},{"location":"installation/#numpy","text":"As of version 0.8.2 , EnergyFlow has used a modified version of numpy.einsum to do the heavy lifting for the computation of EFPs. NumPy 1.14.0 changed einsum to use tensordot when possible compared to 1.13.3 , which only used c_einsum . It was found that the multi-process approach used by batch_compute is typically much faster when using the inherently single-threaded c_einsum versus tensordot , which can call BLAS. Hence the custom version of einsum shipped with EnergyFlow disables all calls to tensordot and uses only c_einsum .","title":"NumPy"},{"location":"releases/","text":"Notes regarding the released versions of EnergyFlow will be published here. 0.13.x 0.13.0 Added support for downloading and reading MOD datasets containing CMS Open Data and Simulation from Zenodo. Added particle utility functions to map PDG IDs to electric charges. A ~ is now expanded to the user's home directory properly in the filepath option to architectures. Added sum_ptyphims and sum_ptyphipids functions to sum four-vectors in hadronic coordinates. EMD module now has support for spherical measure. Added scheme choices for summing four-vectors of particles. Added preprocessing functions to particle utilities, including center_ptyphims , rotate_ptyphims and reflect_ptyphims . Added h5py install dependency for MOD Datasets. Improved binder environment with latex and default matplotlib settings. Added observables submodule which currently includes image_activity and zg. EMD module now imported when importing toplevel energyflow. 0.12.x 0.12.3 Set allow_pickle to True explicitly in Generator (recently changed default in NumPy). Added Herwig7.1 dataset to qg_jets . A big thanks to Aditya Pathak for generating these Herwig samples! Quark and gluon dataset files can now be obtained from Zenodo in addition to Dropbox. Changed internals of EFN to use standalone functions for easier use of subnetwork components. Added some particle utility functions to deal with PDG IDs. Added particle utilities to deal with pseudorapidities. Changed gen_random_events_mcom to have positive energies (momenta still sum to zero). Added l2 regularization to layers in EFN and PFN architectures. Thanks to Anders Andreassen for submiting this pull request! Added tests for particle utils. Particle utilities now accept arrays of events. 0.12.2 Added another periodic phi test for event EMD. Changed gdim default to None (to reduce potentially unexpected behavior). Increased numerical stability of EMD computation by including an internal change of units. Added verbosity functionality to EFP Generator. 0.12.1 Named lambda functions inside EFNs and PFNs (necessary for saving models). Fixed typo in archbase code. Added tests for architecture code. 0.12.0 Fixed potential issue involving the Keras Masking layer not functioning as documented. This is not expected to affect any EFN models that were padded with zeros, nor any PFN models for which the padding was consistent across training and testing sets. Thanks to Anders Andreassen for pointing this out! Added arbitrary attribute lookup in the underlying model for all EnergyFlow architectures. Deprecated old EFN/PFN parameter names. Built-in support for ModelCheckpoint and EarlyStopping callbacks for neural network models. Made naming of neural network layers optional, allowing pieces to be reused more easily. Support for periodic phi values in EMD module. Added support for passing arbitrary compilation options to Keras models. Added EMD Demo notebook 0.11.x 0.11.2 Added advanced activations support for neural network architectures. Thanks to Kevin Bauer for this suggestion! 0.11.1 Fixed issue when using Python 2 caused by not importing division in dataset loading code. Thanks to Matt LeBlanc for pointing this out! Added n_iter_max option to EMD functions. 0.11.0 Added emd module to EnergyFlow. This new module is not imported by default and relies on the Python Optimal Transport library and SciPy . Included binder support for the jupyter notebook demos. Thanks to Matthew Feickert for contributing this feature! 0.10.x 0.10.5 Minor improvement and fixes. Thanks to Preksha Naik for pointing out a typo! 0.10.4 Updates to the documentation and enhanced examples provided. 0.10.3 Finalized initial documentation pages. Minor improvement and fixes. 0.10.2 Minor improvement and fixes. 0.10.1 Minor improvement and fixes. 0.10.0 Added archs module containing EFN, PFN, DNN, CNN, and Linear models. <0.9.x Rapid development of EFP code.","title":"Release Notes"},{"location":"releases/#013x","text":"0.13.0 Added support for downloading and reading MOD datasets containing CMS Open Data and Simulation from Zenodo. Added particle utility functions to map PDG IDs to electric charges. A ~ is now expanded to the user's home directory properly in the filepath option to architectures. Added sum_ptyphims and sum_ptyphipids functions to sum four-vectors in hadronic coordinates. EMD module now has support for spherical measure. Added scheme choices for summing four-vectors of particles. Added preprocessing functions to particle utilities, including center_ptyphims , rotate_ptyphims and reflect_ptyphims . Added h5py install dependency for MOD Datasets. Improved binder environment with latex and default matplotlib settings. Added observables submodule which currently includes image_activity and zg. EMD module now imported when importing toplevel energyflow.","title":"0.13.x"},{"location":"releases/#012x","text":"0.12.3 Set allow_pickle to True explicitly in Generator (recently changed default in NumPy). Added Herwig7.1 dataset to qg_jets . A big thanks to Aditya Pathak for generating these Herwig samples! Quark and gluon dataset files can now be obtained from Zenodo in addition to Dropbox. Changed internals of EFN to use standalone functions for easier use of subnetwork components. Added some particle utility functions to deal with PDG IDs. Added particle utilities to deal with pseudorapidities. Changed gen_random_events_mcom to have positive energies (momenta still sum to zero). Added l2 regularization to layers in EFN and PFN architectures. Thanks to Anders Andreassen for submiting this pull request! Added tests for particle utils. Particle utilities now accept arrays of events. 0.12.2 Added another periodic phi test for event EMD. Changed gdim default to None (to reduce potentially unexpected behavior). Increased numerical stability of EMD computation by including an internal change of units. Added verbosity functionality to EFP Generator. 0.12.1 Named lambda functions inside EFNs and PFNs (necessary for saving models). Fixed typo in archbase code. Added tests for architecture code. 0.12.0 Fixed potential issue involving the Keras Masking layer not functioning as documented. This is not expected to affect any EFN models that were padded with zeros, nor any PFN models for which the padding was consistent across training and testing sets. Thanks to Anders Andreassen for pointing this out! Added arbitrary attribute lookup in the underlying model for all EnergyFlow architectures. Deprecated old EFN/PFN parameter names. Built-in support for ModelCheckpoint and EarlyStopping callbacks for neural network models. Made naming of neural network layers optional, allowing pieces to be reused more easily. Support for periodic phi values in EMD module. Added support for passing arbitrary compilation options to Keras models. Added EMD Demo notebook","title":"0.12.x"},{"location":"releases/#011x","text":"0.11.2 Added advanced activations support for neural network architectures. Thanks to Kevin Bauer for this suggestion! 0.11.1 Fixed issue when using Python 2 caused by not importing division in dataset loading code. Thanks to Matt LeBlanc for pointing this out! Added n_iter_max option to EMD functions. 0.11.0 Added emd module to EnergyFlow. This new module is not imported by default and relies on the Python Optimal Transport library and SciPy . Included binder support for the jupyter notebook demos. Thanks to Matthew Feickert for contributing this feature!","title":"0.11.x"},{"location":"releases/#010x","text":"0.10.5 Minor improvement and fixes. Thanks to Preksha Naik for pointing out a typo! 0.10.4 Updates to the documentation and enhanced examples provided. 0.10.3 Finalized initial documentation pages. Minor improvement and fixes. 0.10.2 Minor improvement and fixes. 0.10.1 Minor improvement and fixes. 0.10.0 Added archs module containing EFN, PFN, DNN, CNN, and Linear models.","title":"0.10.x"},{"location":"releases/#09x","text":"Rapid development of EFP code.","title":"&lt;0.9.x"},{"location":"docs/archs/","text":"Energy Flow Networks (EFNs) and Particle Flow Networks (PFNs) are model architectures designed for learning from collider events as unordered, variable-length sets of particles. Both EFNs and PFNs are parameterized by a learnable per-particle function $\\Phi$ and latent space function $F$. An EFN takes the following form: \\text{EFN}=F\\left(\\sum_{i=1}^M z_i \\Phi(\\hat p_i)\\right) where $z_i$ is a measure of the energy of particle $i$, such as $z_i=p_{T,i}$, and $\\hat p_i$ is a measure of the angular information of particle $i$, such as $\\hat p_i = (y_i,\\phi_i)$. Any infrared- and collinear-safe observable can be parameterized in this form. A PFN takes the following form: \\text{PFN}=F\\left(\\sum_{i=1}^M \\Phi(p_i)\\right) where $p_i$ is the information of particle $i$, such as its four-momentum, charge, or flavor. Any observable can be parameterized in this form. See the Deep Sets framework for additional discussion. Since these architectures are not used by the core EnergyFlow code, and require the external Keras and scikit-learn libraries, they are not imported by default but must be explicitly imported, e.g. from energyflow.archs import * . EnergyFlow also contains several additional model architectures for ease of using common models that frequently appear in the intersection of particle physics and machine learning. EFN Energy Flow Network (EFN) architecture. energyflow.archs.EFN(*args, **kwargs) See ArchBase for how to pass in hyperparameters as well as defaults common to all EnergyFlow neural network models. Required EFN Hyperparameters input_dim : int The number of features for each particle. Phi_sizes (formerly ppm_sizes ) : { tuple , list } of int The sizes of the dense layers in the per-particle frontend module $\\Phi$. The last element will be the number of latent observables that the model defines. F_sizes (formerly dense_sizes ) : { tuple , list } of int The sizes of the dense layers in the backend module $F$. Default EFN Hyperparameters Phi_acts = 'relu' (formerly ppm_acts ) : { tuple , list } of str or Keras activation Activation functions(s) for the dense layers in the per-particle frontend module $\\Phi$. A single string or activation layer will apply the same activation to all layers. Keras advanced activation layers are also accepted, either as strings (which use the default arguments) or as Keras Layer instances. If passing a single Layer instance, be aware that this layer will be used for all activations and may introduce weight sharing (such as with PReLU ); it is recommended in this case to pass as many activations as there are layers in the model. See the Keras activations docs for more detail. F_acts = 'relu' (formerly dense_acts ) : { tuple , list } of str or Keras activation Activation functions(s) for the dense layers in the backend module $F$. A single string or activation layer will apply the same activation to all layers. Phi_k_inits = 'he_uniform' (formerly ppm_k_inits ) : { tuple , list } of str or Keras initializer Kernel initializers for the dense layers in the per-particle frontend module $\\Phi$. A single string will apply the same initializer to all layers. See the Keras initializer docs for more detail. F_k_inits = 'he_uniform' (formerly dense_k_inits ) : { tuple , list } of str or Keras initializer Kernel initializers for the dense layers in the backend module $F$. A single string will apply the same initializer to all layers. latent_dropout = 0 : float Dropout rates for the summation layer that defines the value of the latent observables on the inputs. See the Keras Dropout layer for more detail. F_dropouts = 0 (formerly dense_dropouts ) : { tuple , list } of float Dropout rates for the dense layers in the backend module $F$. A single float will apply the same dropout rate to all dense layers. Phi_l2_regs = 0 : { tuple , list } of float $L_2$-regulatization strength for both the weights and biases of the layers in the $\\Phi$ network. A single float will apply the same $L_2$-regulatization to all layers. F_l2_regs = 0 : { tuple , list } of float $L_2$-regulatization strength for both the weights and biases of the layers in the $F$ network. A single float will apply the same $L_2$-regulatization to all layers. mask_val = 0 : float The value for which particles with all features set equal to this value will be ignored. The Keras Masking layer appears to have issues masking the biases of a network, so this has been implemented in a custom (and correct) manner since version 0.12.0 . eval_filters eval_filters(patch, n=100, prune=True) Evaluates the latent space filters of this model on a patch of the two-dimensional geometric input space. Arguments patch : { tuple , list } of float Specifies the patch of the geometric input space to be evaluated. A list of length 4 is interpretted as [xmin, ymin, xmax, ymax] . Passing a single float R is equivalent to [-R,-R,R,R] . n : { tuple , list } of int The number of grid points on which to evaluate the filters. A list of length 2 is interpretted as [nx, ny] where nx is the number of points along the x (or first) dimension and ny is the number of points along the y (or second) dimension. prune : bool Whether to remove filters that are all zero (which happens sometimes due to dying ReLUs). Returns ( numpy.ndarray , numpy.ndarray , numpy.ndarray ) Returns three arrays, (X, Y, Z) , where X and Y have shape (nx, ny) and are arrays of the values of the geometric inputs in the specified patch. Z has shape (num_filters, nx, ny) and is the value of the different filters at each point. inputs inputs List of input tensors to the model. EFNs have two input tensors: inputs[0] corresponds to the zs input and inputs[1] corresponds to the phats input. weights weights Weight tensor for the model. This is the zs input where entries equal to mask_val have been set to zero. Phi Phi List of tensors corresponding to the layers in the $\\Phi$ network. latent latent List of tensors corresponding to the summation layer in the network, including any dropout layer if present. F F List of tensors corresponding to the layers in the $F$ network. output output Output tensor for the model. PFN Particle Flow Network (PFN) architecture. Accepts the same hyperparameters as the EFN . energyflow.archs.PFN(*args, **kwargs) inputs inputs List of input tensors to the model. PFNs have one input tensor corresponding to the ps input. weights weights Weight tensor for the model. A weight of 0 is assigned to any particle which has all features equal to mask_val , and 1 is assigned otherwise. Phi Phi List of tensors corresponding to the layers in the $\\Phi$ network. latent latent List of tensors corresponding to the summation layer in the network, including any dropout layer if present. F F List of tensors corresponding to the layers in the $F$ network. output output Output tensor for the model. CNN Convolutional Neural Network architecture. energyflow.archs.CNN(*args, **kwargs) See ArchBase for how to pass in hyperparameters as well as defaults common to all EnergyFlow neural network models. Required CNN Hyperparameters input_shape : { tuple , list } of int The shape of a single jet image. Assuming that data_format is set to channels_first , this is (nb_chan,npix,npix) . filter_sizes : { tuple , list } of int The size of the filters, which are taken to be square, in each convolutional layer of the network. The length of the list will be the number of convolutional layers in the network. num_filters : { tuple , list } of int The number of filters in each convolutional layer. The length of num_filters must match that of filter_sizes . Default CNN Hyperparameters dense_sizes = None : { tuple , list } of int The sizes of the dense layer backend. A value of None is equivalent to an empty list. pool_sizes = 0 : { tuple , list } of int Size of maxpooling filter, taken to be a square. A value of 0 will not use maxpooling. conv_acts = 'relu' : { tuple , list } of str or Keras activation Activation function(s) for the conv layers. A single string or activation layer will apply the same activation to all conv layers. Keras advanced activation layers are also accepted, either as strings (which use the default arguments) or as Keras Layer instances. If passing a single Layer instance, be aware that this layer will be used for all activations and may introduce weight sharing (such as with PReLU ); it is recommended in this case to pass as many activations as there are layers in the model.See the Keras activations docs for more detail. dense_acts = 'relu' : { tuple , list } of str or Keras activation Activation functions(s) for the dense layers. A single string or activation layer will apply the same activation to all dense layers. conv_k_inits = 'he_uniform' : { tuple , list } of str or Keras initializer Kernel initializers for the convolutional layers. A single string will apply the same initializer to all layers. See the Keras initializer docs for more detail. dense_k_inits = 'he_uniform' : { tuple , list } of str or Keras initializer Kernel initializers for the dense layers. A single string will apply the same initializer to all layers. conv_dropouts = 0 : { tuple , list } of float Dropout rates for the convolutional layers. A single float will apply the same dropout rate to all conv layers. See the Keras Dropout layer for more detail. num_spatial2d_dropout = 0 : int The number of convolutional layers, starting from the beginning of the model, for which to apply SpatialDropout2D instead of Dropout. dense_dropouts = 0 : { tuple , list } of float Dropout rates for the dense layers. A single float will apply the same dropout rate to all dense layers. paddings = 'valid' : { tuple , list } of str Controls how the filters are convoled with the inputs. See the Keras Conv2D layer for more detail. data_format = 'channels_first' : { 'channels_first' , 'channels_last' } Sets which axis is expected to contain the different channels. DNN Dense Neural Network architecture. energyflow.archs.DNN(*args, **kwargs) See ArchBase for how to pass in hyperparameters as well as defaults common to all EnergyFlow neural network models. Required DNN Hyperparameters input_dim : int The number of inputs to the model. dense_sizes : { tuple , list } of int The number of nodes in the dense layers of the model. Default DNN Hyperparameters acts = 'relu' : { tuple , list } of str or Keras activation Activation functions(s) for the dense layers. A single string or activation layer will apply the same activation to all dense layers. Keras advanced activation layers are also accepted, either as strings (which use the default arguments) or as Keras Layer instances. If passing a single Layer instance, be aware that this layer will be used for all activations and may introduce weight sharing (such as with PReLU ); it is recommended in this case to pass as many activations as there are layers in the model.See the Keras activations docs for more detail. k_inits = 'he_uniform' : { tuple , list } of str or Keras initializer Kernel initializers for the dense layers. A single string will apply the same initializer to all layers. See the Keras initializer docs for more detail. dropouts = 0 : { tuple , list } of float Dropout rates for the dense layers. A single float will apply the same dropout rate to all layers. See the Keras Dropout layer for more detail. l2_regs = 0 : { tuple , list } of float $L_2$-regulatization strength for both the weights and biases of the dense layers. A single float will apply the same $L_2$-regulatization to all layers. LinearClassifier Linear classifier that can be either Fisher's linear discriminant or logistic regression. Relies on the scikit-learn implementations of these classifiers. energyflow.archs.LinearClassifier(*args, **kwargs) See ArchBase for how to pass in hyperparameters. Default Hyperparameters linclass_type = 'lda' : { 'lda' , 'lr' } Controls which type of linear classifier is used. 'lda' corresponds to LinearDisciminantAnalysis and 'lr' to Logistic Regression . If using 'lr' all arguments are passed on directly to the scikit-learn class. Linear Discriminant Analysis Hyperparameters solver = 'svd' : { 'svd' , 'lsqr' , 'eigen' } Which LDA solver to use. tol = 1e-12 : float Threshold used for rank estimation. Notably not a convergence parameter. Logistic Regression Hyperparameters LR_hps = {} : dict Dictionary of keyword arguments to pass on to the underlying LogisticRegression model. ArchBase Base class for all architectures contained in EnergyFlow. The mechanism of specifying hyperparameters for all architectures is described here. Methods common to all architectures are documented here. Note that this class cannot be instantiated directly as it is an abstract base class. energyflow.archs.archbase.ArchBase(*args, **kwargs) Accepts arbitrary arguments. Positional arguments (if present) are dictionaries of hyperparameters, keyword arguments (if present) are hyperparameters directly. Keyword hyperparameters take precedence over positional hyperparameter dictionaries. Arguments *args : arbitrary positional arguments Each argument is a dictionary containing hyperparameter (name, value) pairs. *kwargs : arbitrary keyword arguments Hyperparameters as keyword arguments. Takes precedence over the positional arguments. Default NN Hyperparameters Common hyperparameters that apply to all architectures except for LinearClassifier . Compilation Options loss = 'categorical_crossentropy' : str The loss function to use for the model. See the Keras loss function docs for available loss functions. optimizer = 'adam' : Keras optimizer or str A Keras optimizer instance or a string referring to one (in which case the default arguments are used). metrics = ['accuracy'] : list of str The Keras metrics to apply to the model. compile_opts = {} : dict Dictionary of keyword arguments to be passed on to the compile method of the model. loss , optimizer , and metrics (see above) are included in this dictionary. All other values are the Keras defaults. Output Options output_dim = 2 : int The output dimension of the model. output_act = 'softmax' : str or Keras activation Activation function to apply to the output. Callback Options filepath = None : str The file path for where to save the model. If None then the model will not be saved. save_while_training = True : bool Whether the model is saved during training (using the ModelCheckpoint callback) or only once training terminates. Only relevant if filepath is set. save_weights_only = False : bool Whether only the weights of the model or the full model are saved. Only relevant if filepath is set. modelcheck_opts = {'save_best_only':True, 'verbose':1} : dict Dictionary of keyword arguments to be passed on to the ModelCheckpoint callback, if it is present. save_weights_only (see above) is included in this dictionary. All other arguments are the Keras defaults. patience = None : int The number of epochs with no improvement after which the training is stopped (using the EarlyStopping callback). If None then no early stopping is used. earlystop_opts = {'restore_best_weights':True, 'verbose':1} : dict Dictionary of keyword arguments to be passed on to the EarlyStopping callback, if it is present. patience (see above) is included in this dictionary. All other arguments are the Keras defaults. Flags name_layers = True : bool Whether to give the layers of the model explicit names or let them be named automatically. One reason to set this to False would be in order to use parts of this model in another model (all Keras layers in a model are required to have unique names). compile = True : bool Whether the model should be compiled or not. summary = True : bool Whether a summary should be printed or not. fit fit(X_train, Y_train, **kwargs) Train the model by fitting the provided training dataset and labels. Transparently calls the fit() method of the underlying model. Arguments X_train : numpy.ndarray The training dataset as an array of features for each sample. Y_train : numpy.ndarray The labels for the training dataset. May need to be one-hot encoded depending on the requirements of the underlying model (typically Keras models will use one-hot encoding whereas the linear model does not.) kwargs : dict Keyword arguments passed on to the fit method of the underlying model. Most relevant for neural network models, where the Keras model docs contain detailed information on the possible arguments. Returns Whatever the underlying model's fit() returns. predict predict(X_test, **kwargs) Evaluate the model on a dataset. Note that for the LinearClassifier this corresponds to the predict_proba method of the underlying scikit-learn model. Arguments X_test : numpy.ndarray The dataset to evaluate the model on. kwargs : dict Keyword arguments passed on to the underlying model when predicting on a dataset. Returns numpy.ndarray The value of the model on the input dataset. model model The underlying model held by this architecture. Note that accessing an attribute that the architecture does not have will resulting in attempting to retrieve the attribute from this model. This allows for interrogation of the EnergyFlow architecture in the same manner as the underlying model. Examples For neural network models: model.layers will return a list of the layers, where model is any EnergFlow neural network. For linear models: model.coef_ will return the coefficients, where model is any EnergyFlow LinearClassifier instance.","title":"Architectures"},{"location":"docs/archs/#efn","text":"Energy Flow Network (EFN) architecture. energyflow.archs.EFN(*args, **kwargs) See ArchBase for how to pass in hyperparameters as well as defaults common to all EnergyFlow neural network models. Required EFN Hyperparameters input_dim : int The number of features for each particle. Phi_sizes (formerly ppm_sizes ) : { tuple , list } of int The sizes of the dense layers in the per-particle frontend module $\\Phi$. The last element will be the number of latent observables that the model defines. F_sizes (formerly dense_sizes ) : { tuple , list } of int The sizes of the dense layers in the backend module $F$. Default EFN Hyperparameters Phi_acts = 'relu' (formerly ppm_acts ) : { tuple , list } of str or Keras activation Activation functions(s) for the dense layers in the per-particle frontend module $\\Phi$. A single string or activation layer will apply the same activation to all layers. Keras advanced activation layers are also accepted, either as strings (which use the default arguments) or as Keras Layer instances. If passing a single Layer instance, be aware that this layer will be used for all activations and may introduce weight sharing (such as with PReLU ); it is recommended in this case to pass as many activations as there are layers in the model. See the Keras activations docs for more detail. F_acts = 'relu' (formerly dense_acts ) : { tuple , list } of str or Keras activation Activation functions(s) for the dense layers in the backend module $F$. A single string or activation layer will apply the same activation to all layers. Phi_k_inits = 'he_uniform' (formerly ppm_k_inits ) : { tuple , list } of str or Keras initializer Kernel initializers for the dense layers in the per-particle frontend module $\\Phi$. A single string will apply the same initializer to all layers. See the Keras initializer docs for more detail. F_k_inits = 'he_uniform' (formerly dense_k_inits ) : { tuple , list } of str or Keras initializer Kernel initializers for the dense layers in the backend module $F$. A single string will apply the same initializer to all layers. latent_dropout = 0 : float Dropout rates for the summation layer that defines the value of the latent observables on the inputs. See the Keras Dropout layer for more detail. F_dropouts = 0 (formerly dense_dropouts ) : { tuple , list } of float Dropout rates for the dense layers in the backend module $F$. A single float will apply the same dropout rate to all dense layers. Phi_l2_regs = 0 : { tuple , list } of float $L_2$-regulatization strength for both the weights and biases of the layers in the $\\Phi$ network. A single float will apply the same $L_2$-regulatization to all layers. F_l2_regs = 0 : { tuple , list } of float $L_2$-regulatization strength for both the weights and biases of the layers in the $F$ network. A single float will apply the same $L_2$-regulatization to all layers. mask_val = 0 : float The value for which particles with all features set equal to this value will be ignored. The Keras Masking layer appears to have issues masking the biases of a network, so this has been implemented in a custom (and correct) manner since version 0.12.0 .","title":"EFN"},{"location":"docs/archs/#eval_filters","text":"eval_filters(patch, n=100, prune=True) Evaluates the latent space filters of this model on a patch of the two-dimensional geometric input space. Arguments patch : { tuple , list } of float Specifies the patch of the geometric input space to be evaluated. A list of length 4 is interpretted as [xmin, ymin, xmax, ymax] . Passing a single float R is equivalent to [-R,-R,R,R] . n : { tuple , list } of int The number of grid points on which to evaluate the filters. A list of length 2 is interpretted as [nx, ny] where nx is the number of points along the x (or first) dimension and ny is the number of points along the y (or second) dimension. prune : bool Whether to remove filters that are all zero (which happens sometimes due to dying ReLUs). Returns ( numpy.ndarray , numpy.ndarray , numpy.ndarray ) Returns three arrays, (X, Y, Z) , where X and Y have shape (nx, ny) and are arrays of the values of the geometric inputs in the specified patch. Z has shape (num_filters, nx, ny) and is the value of the different filters at each point.","title":"eval_filters"},{"location":"docs/archs/#inputs","text":"inputs List of input tensors to the model. EFNs have two input tensors: inputs[0] corresponds to the zs input and inputs[1] corresponds to the phats input.","title":"inputs"},{"location":"docs/archs/#weights","text":"weights Weight tensor for the model. This is the zs input where entries equal to mask_val have been set to zero.","title":"weights"},{"location":"docs/archs/#phi","text":"Phi List of tensors corresponding to the layers in the $\\Phi$ network.","title":"Phi"},{"location":"docs/archs/#latent","text":"latent List of tensors corresponding to the summation layer in the network, including any dropout layer if present.","title":"latent"},{"location":"docs/archs/#f","text":"F List of tensors corresponding to the layers in the $F$ network.","title":"F"},{"location":"docs/archs/#output","text":"output Output tensor for the model.","title":"output"},{"location":"docs/archs/#pfn","text":"Particle Flow Network (PFN) architecture. Accepts the same hyperparameters as the EFN . energyflow.archs.PFN(*args, **kwargs)","title":"PFN"},{"location":"docs/archs/#inputs_1","text":"inputs List of input tensors to the model. PFNs have one input tensor corresponding to the ps input.","title":"inputs"},{"location":"docs/archs/#weights_1","text":"weights Weight tensor for the model. A weight of 0 is assigned to any particle which has all features equal to mask_val , and 1 is assigned otherwise.","title":"weights"},{"location":"docs/archs/#phi_1","text":"Phi List of tensors corresponding to the layers in the $\\Phi$ network.","title":"Phi"},{"location":"docs/archs/#latent_1","text":"latent List of tensors corresponding to the summation layer in the network, including any dropout layer if present.","title":"latent"},{"location":"docs/archs/#f_1","text":"F List of tensors corresponding to the layers in the $F$ network.","title":"F"},{"location":"docs/archs/#output_1","text":"output Output tensor for the model.","title":"output"},{"location":"docs/archs/#cnn","text":"Convolutional Neural Network architecture. energyflow.archs.CNN(*args, **kwargs) See ArchBase for how to pass in hyperparameters as well as defaults common to all EnergyFlow neural network models. Required CNN Hyperparameters input_shape : { tuple , list } of int The shape of a single jet image. Assuming that data_format is set to channels_first , this is (nb_chan,npix,npix) . filter_sizes : { tuple , list } of int The size of the filters, which are taken to be square, in each convolutional layer of the network. The length of the list will be the number of convolutional layers in the network. num_filters : { tuple , list } of int The number of filters in each convolutional layer. The length of num_filters must match that of filter_sizes . Default CNN Hyperparameters dense_sizes = None : { tuple , list } of int The sizes of the dense layer backend. A value of None is equivalent to an empty list. pool_sizes = 0 : { tuple , list } of int Size of maxpooling filter, taken to be a square. A value of 0 will not use maxpooling. conv_acts = 'relu' : { tuple , list } of str or Keras activation Activation function(s) for the conv layers. A single string or activation layer will apply the same activation to all conv layers. Keras advanced activation layers are also accepted, either as strings (which use the default arguments) or as Keras Layer instances. If passing a single Layer instance, be aware that this layer will be used for all activations and may introduce weight sharing (such as with PReLU ); it is recommended in this case to pass as many activations as there are layers in the model.See the Keras activations docs for more detail. dense_acts = 'relu' : { tuple , list } of str or Keras activation Activation functions(s) for the dense layers. A single string or activation layer will apply the same activation to all dense layers. conv_k_inits = 'he_uniform' : { tuple , list } of str or Keras initializer Kernel initializers for the convolutional layers. A single string will apply the same initializer to all layers. See the Keras initializer docs for more detail. dense_k_inits = 'he_uniform' : { tuple , list } of str or Keras initializer Kernel initializers for the dense layers. A single string will apply the same initializer to all layers. conv_dropouts = 0 : { tuple , list } of float Dropout rates for the convolutional layers. A single float will apply the same dropout rate to all conv layers. See the Keras Dropout layer for more detail. num_spatial2d_dropout = 0 : int The number of convolutional layers, starting from the beginning of the model, for which to apply SpatialDropout2D instead of Dropout. dense_dropouts = 0 : { tuple , list } of float Dropout rates for the dense layers. A single float will apply the same dropout rate to all dense layers. paddings = 'valid' : { tuple , list } of str Controls how the filters are convoled with the inputs. See the Keras Conv2D layer for more detail. data_format = 'channels_first' : { 'channels_first' , 'channels_last' } Sets which axis is expected to contain the different channels.","title":"CNN"},{"location":"docs/archs/#dnn","text":"Dense Neural Network architecture. energyflow.archs.DNN(*args, **kwargs) See ArchBase for how to pass in hyperparameters as well as defaults common to all EnergyFlow neural network models. Required DNN Hyperparameters input_dim : int The number of inputs to the model. dense_sizes : { tuple , list } of int The number of nodes in the dense layers of the model. Default DNN Hyperparameters acts = 'relu' : { tuple , list } of str or Keras activation Activation functions(s) for the dense layers. A single string or activation layer will apply the same activation to all dense layers. Keras advanced activation layers are also accepted, either as strings (which use the default arguments) or as Keras Layer instances. If passing a single Layer instance, be aware that this layer will be used for all activations and may introduce weight sharing (such as with PReLU ); it is recommended in this case to pass as many activations as there are layers in the model.See the Keras activations docs for more detail. k_inits = 'he_uniform' : { tuple , list } of str or Keras initializer Kernel initializers for the dense layers. A single string will apply the same initializer to all layers. See the Keras initializer docs for more detail. dropouts = 0 : { tuple , list } of float Dropout rates for the dense layers. A single float will apply the same dropout rate to all layers. See the Keras Dropout layer for more detail. l2_regs = 0 : { tuple , list } of float $L_2$-regulatization strength for both the weights and biases of the dense layers. A single float will apply the same $L_2$-regulatization to all layers.","title":"DNN"},{"location":"docs/archs/#linearclassifier","text":"Linear classifier that can be either Fisher's linear discriminant or logistic regression. Relies on the scikit-learn implementations of these classifiers. energyflow.archs.LinearClassifier(*args, **kwargs) See ArchBase for how to pass in hyperparameters. Default Hyperparameters linclass_type = 'lda' : { 'lda' , 'lr' } Controls which type of linear classifier is used. 'lda' corresponds to LinearDisciminantAnalysis and 'lr' to Logistic Regression . If using 'lr' all arguments are passed on directly to the scikit-learn class. Linear Discriminant Analysis Hyperparameters solver = 'svd' : { 'svd' , 'lsqr' , 'eigen' } Which LDA solver to use. tol = 1e-12 : float Threshold used for rank estimation. Notably not a convergence parameter. Logistic Regression Hyperparameters LR_hps = {} : dict Dictionary of keyword arguments to pass on to the underlying LogisticRegression model.","title":"LinearClassifier"},{"location":"docs/archs/#archbase","text":"Base class for all architectures contained in EnergyFlow. The mechanism of specifying hyperparameters for all architectures is described here. Methods common to all architectures are documented here. Note that this class cannot be instantiated directly as it is an abstract base class. energyflow.archs.archbase.ArchBase(*args, **kwargs) Accepts arbitrary arguments. Positional arguments (if present) are dictionaries of hyperparameters, keyword arguments (if present) are hyperparameters directly. Keyword hyperparameters take precedence over positional hyperparameter dictionaries. Arguments *args : arbitrary positional arguments Each argument is a dictionary containing hyperparameter (name, value) pairs. *kwargs : arbitrary keyword arguments Hyperparameters as keyword arguments. Takes precedence over the positional arguments. Default NN Hyperparameters Common hyperparameters that apply to all architectures except for LinearClassifier . Compilation Options loss = 'categorical_crossentropy' : str The loss function to use for the model. See the Keras loss function docs for available loss functions. optimizer = 'adam' : Keras optimizer or str A Keras optimizer instance or a string referring to one (in which case the default arguments are used). metrics = ['accuracy'] : list of str The Keras metrics to apply to the model. compile_opts = {} : dict Dictionary of keyword arguments to be passed on to the compile method of the model. loss , optimizer , and metrics (see above) are included in this dictionary. All other values are the Keras defaults. Output Options output_dim = 2 : int The output dimension of the model. output_act = 'softmax' : str or Keras activation Activation function to apply to the output. Callback Options filepath = None : str The file path for where to save the model. If None then the model will not be saved. save_while_training = True : bool Whether the model is saved during training (using the ModelCheckpoint callback) or only once training terminates. Only relevant if filepath is set. save_weights_only = False : bool Whether only the weights of the model or the full model are saved. Only relevant if filepath is set. modelcheck_opts = {'save_best_only':True, 'verbose':1} : dict Dictionary of keyword arguments to be passed on to the ModelCheckpoint callback, if it is present. save_weights_only (see above) is included in this dictionary. All other arguments are the Keras defaults. patience = None : int The number of epochs with no improvement after which the training is stopped (using the EarlyStopping callback). If None then no early stopping is used. earlystop_opts = {'restore_best_weights':True, 'verbose':1} : dict Dictionary of keyword arguments to be passed on to the EarlyStopping callback, if it is present. patience (see above) is included in this dictionary. All other arguments are the Keras defaults. Flags name_layers = True : bool Whether to give the layers of the model explicit names or let them be named automatically. One reason to set this to False would be in order to use parts of this model in another model (all Keras layers in a model are required to have unique names). compile = True : bool Whether the model should be compiled or not. summary = True : bool Whether a summary should be printed or not.","title":"ArchBase"},{"location":"docs/archs/#fit","text":"fit(X_train, Y_train, **kwargs) Train the model by fitting the provided training dataset and labels. Transparently calls the fit() method of the underlying model. Arguments X_train : numpy.ndarray The training dataset as an array of features for each sample. Y_train : numpy.ndarray The labels for the training dataset. May need to be one-hot encoded depending on the requirements of the underlying model (typically Keras models will use one-hot encoding whereas the linear model does not.) kwargs : dict Keyword arguments passed on to the fit method of the underlying model. Most relevant for neural network models, where the Keras model docs contain detailed information on the possible arguments. Returns Whatever the underlying model's fit() returns.","title":"fit"},{"location":"docs/archs/#predict","text":"predict(X_test, **kwargs) Evaluate the model on a dataset. Note that for the LinearClassifier this corresponds to the predict_proba method of the underlying scikit-learn model. Arguments X_test : numpy.ndarray The dataset to evaluate the model on. kwargs : dict Keyword arguments passed on to the underlying model when predicting on a dataset. Returns numpy.ndarray The value of the model on the input dataset.","title":"predict"},{"location":"docs/archs/#model","text":"model The underlying model held by this architecture. Note that accessing an attribute that the architecture does not have will resulting in attempting to retrieve the attribute from this model. This allows for interrogation of the EnergyFlow architecture in the same manner as the underlying model. Examples For neural network models: model.layers will return a list of the layers, where model is any EnergFlow neural network. For linear models: model.coef_ will return the coefficients, where model is any EnergyFlow LinearClassifier instance.","title":"model"},{"location":"docs/datasets/","text":"CMS Open Data and the MOD HDF5 Format Starting in 2014, the CMS Collaboration began to release research-grade recorded and simulated datasets on the CERN Open Data Portal . These fantastic resources provide a unique opportunity for researchers with diverse connections to the hep-ex world to engage with cutting edge particle physics by developing tools and testing novel strategies on actual LHC data. Our goal in making portions of the CMS Open Data available in a reprocessed format is to ease as best as possible the technical complications that have thus far been present when attempting to use Open Data (see also recent efforts by the CMS Collaboration to make the data more accessible). To facilitate access to Open Data, we have developed a format utilizing the widespread HDF5 file format that stores essential information for some particle physics analyses, such as the ones in Exploring the Space of Jets with CMS Open Data . This \"MOD HDF5 Format\" is currently optimized for studies based on jets, but may be updated in the future to support other types of analyses. To further the goals of Open Data, we have made our reprocessed samples available on the Zenodo platform . Currently, the only \"collection\" (in the parlance used below by the functions and classes that load and read MOD HDF5 files) of datasets that is available is the CMS2011AJets collection, which was used in Exploring the Space of Jets with CMS Open Data for EMD -based studies. More collections may be added in the future as our research group completes more studies with the Open Data. For now, this page focuses on the CMS2011AJets collection. This collection of datasets includes jets recorded by the CMS detector, as well as CMS-generated Pythia samples that are further passed through full CMS detector simulation. Hence, the datasets are refered to as CMS, SIM, and GEN (or in code as 'cms' , 'sim' , 'gen' ) for these three different ways of obtaining jets. The datasets include all available jets above 375 GeV, which is where the Jet300 trigger was found to be fully efficient in both data and simulation. Note that the pT values references in the name of the SIM/GEN datasets are those of the generator-level hard parton. The DOIs of CMS2011AJets MOD HDF5 datasets are: - CMS 2011A Jets, pT > 375 GeV - SIM/GEN QCD Jets 170-300 GeV - SIM/GEN QCD Jets 300-470 GeV - SIM/GEN QCD Jets 470-600 GeV - SIM/GEN QCD Jets 600-800 GeV - SIM/GEN QCD Jets 800-1000 GeV - SIM/GEN QCD Jets 1000-1400 GeV - SIM/GEN QCD Jets 1400-1800 GeV - SIM/GEN QCD Jets 1800-$\\infty$ GeV For more details regarding the creation of these samples, as well as for the DOIs of the original CMS Open Datasets, see Exploring the Space of Jets with CMS Open Data . MODDataset Loads and provides access to datasets in MOD HDF5 format. Jets can be selected while loading from file according to a number of kinematic attributes. MOD HDF5 datasets are created via the save method. Currently, the MOD HDF5 format consists of an HDF5 file with the following arrays, each of which are stored as properties of the MODDataset : /jets_i - int64 An array of integer jet attributes, which are currently: fn : The file number of the jet, used to index the filenames array. rn : The run number of the jet. lbn : The lumiblock number (or lumisection) of the jet. evn : The event number of the jet. npv (CMS/SIM only) : The number of primary vertices of the event containing the jet. quality (CMS/SIM only) : The quality of the jet, where 0 means no quality, 1 is \"loose\", 2 is \"medium\", and 3 is \"strict\". hard_pid (SIM/GEN only) : The particle ID of the hard parton associated to the jet ( 0 if not associated). /jets_f - float64 An array of floating point jet attributes, which are currently: jet_pt : Transverse momentum of the jet. jet_y : Rapidity of the jet. jet_phi : Azimuthal angle of the jet. jet_m : Mass of the jet. jet_eta : Pseudorapidity of the jet. jec (CMS/SIM only) : Jet energy correction. jet_area (CMS/SIM only) : Area of the jet. jet_max_nef (CMS/SIM only) : Maximum of the hadronic and electromagnetic energy fractions of the jet. gen_jet_pt (SIM only) : Transverse momentum of an associated GEN jet. -1 if not associated. gen_jet_y (SIM only) : Rapidity of an associated GEN jet. -1 if not associated. gen_jet_phi (SIM only) : Azimuthal angle of an associated GEN jet. -1 if not associated. gen_jet_m (SIM only) : Mass of an associated GEN jet. -1 if not associated. gen_jet_eta (SIM only) : Pseudorapidity of an associated GEN jet. -1 if not associated. hard_pt (SIM/GEN only) : Transverse momentum of an associated hard parton. -1 if not associated. hard_y (SIM/GEN only) : Rapidity of an associated hard parton. -1 if not associated. hard_phi (SIM/GEN only) : Azimuthal angle of an associated hard parton. -1 if not associated. weight : Contribution of this jet to the cross-section, in nanobarns. /pfcs - float64 (CMS/SIM only) An array of all particle flow candidates, with attributes listed below. There is a separate /pfcs_index array in the file which contains information for MODDataset to separate these particles into separate jets. The columns of the array are currently: pt : Transverse momentum of the PFC. y : Rapidity of the PFC. phi : Azimuthal angle of the PFC. m : Mass of the PFC. pid : PDG ID of the PFC. vertex : Vertex ID of the PFC. 0 is leading vertex, >0 is a pileup vertex, and -1 is unknown. Neutral particles are assigned to the leading vertex. /gens - float64 (SIM/GEN only) An array of all generator-level particles, currently with the same columns as the pfcs array (the vertex column contains all 0 s). For the SIM dataset, these are the particles of jets associated to the SIM jets which are described in the jets_i and jets_f arrays. As with pfcs , there is a separate gens_index array which tells MODDataset how to separate these gen particles into distinct jets. /filenames - str An array of strings indexed by the fn attribute of each jet. For CMS, this array is one dimensional and contains the CMS-provided filenames. For SIM/GEN, this array is two dimensional where the first column is the pT value that appears in the name of the dataset and the second column is the CMS-provided filename. In all cases, indexing this array with the fn attribute of a jet gives the file information in which the event containing that jet is to be found. Note that the column names of the jets_i , jets_f , pfcs , and gens arrays are stored as lists of strings in the attributes jets_i_cols , jets_f_cols , pfcs_cols , and gens_cols . For each of the above arrays, MODDataset stores the index of the column as an attribute with the same name as the column. For example, for an instance called modds , modds.fn has a value of 0 since it is the first column in the jets_i array, modds.jet_phi has a value of 2 , modds.m has a value of 3 , etc. Even more helpfully, a view of each column of the jets arrays is stored as an attribute as well, so that modds.jet_pts is the same as modds.jets_f[:,modds.jet_pt] , modds.evns is the same as modds.jets_i[:,modds.evn] , etc. Additionally, one special view is stored, corr_jet_pts , which is equal to the product of the jet pTs and the JECs, i.e. modds.jet_pts*modds.jecs . MODDataset supports the builtin len() method, which returns the number of jets currently stored in the dataset, as well as the print() method, which prints a summary of the dataset. energyflow.datasets.mod.MODDataset(*args, datasets=None, path=None, num=-1, shuffle=True, store_pfcs=True, store_gens=True) MODDataset can be initialized from a MOD HDF5 file or from a list of existing MODDataset s. In the first case, the filename should be given as the first positional argument. In the second case, the datasets keyword argument should be set to a list of MODDataset objects. Arguments *args : arbitrary positional arguments Each argument specifies a requirement for an event to be selected and kept in the MODDataset . All requirements are ANDed together. Each specification can be a string or a tuple/list of objects that will be converted to strings and concatenated together. Each string specifies the name of one of the columns of one of the jets arrays ( 'corr_jet_pts' is also accepted, see above, as well as 'abs_jet_eta' , 'abs_gen_jet_y' , etc, which use the absolute values of the [pseudo]rapidities of the jets) as well as one or more comparisons to be performed using the values of that column and the given values in the string. For example, ('corr_jet_pts >', 400) , which is the same as 'corr_jet_pts>400' , will select jets with a corrected pT above 400 GeV. Ampersands may be used within one string to indicated multiple requirements, e.g. 'corr_jet_pts > 400 & abs_jet_eta' , which has the same effect as using multiple arguements each with a single requirement. datasets : { tuple , list } of MODDataset instances or None MODDataset s from which to initialize this dataset. Effectively what this does is to concatenate the arrays held by the datasets. Should always be None when initializing from an existing file. path : str or None If not None , then path is prepended to the filename when initializing from file. Has no effect when initializing from existing datasets. num : int The number of events or jets to keep after subselections are applied. A value of -1 keeps the entire dataset. The weights are properly rescaled to preserve the total cross section of the selection. shuffle : bool When subselecting a fraction of the dataset (i.e. num!=-1 ), if False the first num events passing cuts will be kept, if True then a random subset of num events will be kept. Note that this has no effect when num is -1 , and also that this flag only affects which events are selected and does not randomize the order of the events that are ultimately stored by the MODDataset object. store_pfcs : bool Whether or not to store PFCs if they are present in the dataset. store_gens : bool Whether or not to store gen-level particles (referred to as \"gens\") if they are present in the dataset. sel sel(args, kwargs) Returns a boolean mask that selects jets according to the specified requirements. Arguments *args : arbitrary positional arguments Used to specify cuts to be made to the dataset while loading; see the detailed description of the positional arguments accepted by MODDataset . Returns numpy.ndarray of type bool A boolean mask that will select jets that pass all of the specified requirements. apply_mask apply_mask(mask, preserve_total_weight=False) Subselects jets held by the MODDataset according to a boolean mask. Arguments mask : numpy.ndarray or type bool A boolean mask used to select which jets are to be kept. Should be the same length as the MODDataset object. preserve_total_weight : bool Whether or not to keep the cross section of the MODDataset fixed after the selection. save save(filepath, npf=-1, compression=None, verbose=1, n_jobs=1) Saves a MODDataset in the MOD HDF5 format. Arguments filepath : str The filepath (with or without the '.h5' suffix) where the saved file will be located. npf : int The number of jets per file. If not -1 , multiple files will be created with npf jets as the maximum number stored in each file, in which case '_INDEX' , where INDEX is the index of that file, will be appended to the filename. compression : int or None If not None , the gzip compression level to use when saving the arrays in the HDF5 file. If not None , '_compressed' will be appended to the end of the filename. verbose : int Verbosity level to use when saving the files. n_jobs : int The number of processes to use when saving the files; only relevant when npf!=-1 . close close() Closes the underlying HDF5 file, if one is associated with the MODDataset object. Note that associated HDF5 files are closed by default when the MODDataset object is deleted. jets_i jets_i The jets_i array, described under MODDataset . jets_f jets_f The jets_f array, described under MODDataset . pfcs pfcs The pfcs array, described under MODDataset . gens gens The gens array, described under MODDataset . particles particles If this is a CMS or SIM dataset, particles is the same as pfcs ; for GEN it is the same as gens . filenames filenames The filenames array, described under MODDataset . hf hf The underlying HDF5 file, if one is associated to the MODDataset . load energyflow.datasets.mod.load(*args, amount=1, cache_dir='~/.energyflow', collection='CMS2011AJets', dataset='cms', subdatasets=None, validate_files=False, store_pfcs=True, store_gens=True, verbose=0) Loads samples from the specified MOD dataset. Any file that is needed that has not been cached will be automatically downloaded from Zenodo. Downloaded files are cached for later use. File checksums are optionally validated to ensure dataset fidelity. Arguments *args : arbitrary positional arguments Used to specify cuts to be made to the dataset while loading; see the detailed description of the positional arguments accepted by MODDataset . amount : int or float Approximate amount of the dataset to load. If an integer, this is the number of files to load (a warning is issued if more files are requested than are available). If a float, this is the fraction of the number of available files to load, rounded up to the nearest whole number. Note that since ints and floats are treated different, a value of 1 loads one file whereas 1.0 loads the entire dataset. A value of -1 also loads the entire dataset. cache_dir : str The directory where to store/look for the files. Note that 'datasets' is automatically appended to the end of this path, as well as the collection name. For example, the default is to download/look for files in the directory '~/.energyflow/datasets/CMS2011AJets' . collection : str Name of the collection of datasets to consider. Currently the only collection is 'CMS2011AJets' , though more may be added in the future. dataset : str Which dataset in the collection to load. Currently the 'CMS2011AJets' collection has 'cms' , 'sim' , and 'gen' datasets. subdatasets : { tuple , list } of str or None The names of subdatasets to use. A value of None uses all available subdatasets. Currently, for the 'CMS2011AJets' collection, the 'cms' dataset has one subdataset, 'CMS_Jet300_pT375-infGeV' , the 'sim' dataset has eight subdatasets arrange according to generator $\\hat p_T$, e.g. 'SIM470_Jet300_pT375-infGeV' , and the 'gen' dataset also has eight subdatasets arranged similaraly, e.g. 'GEN470_pT375-infGeV' . validate_files : bool Whether or not to validate files according to their MD5 hashes. It is a good idea to set this to True when first downloading the files from Zenodo in order to ensure they downloaded properly. store_pfcs : bool Whether or not to store PFCs if they are present in the dataset. store_gens : bool Whether or not to store gen-level particles (referred to as \"gens\") if they are present in the dataset. verbose : int Verbosity level to use when loading. 0 is the least verbose, 1 is more verbose, and 2 is the most verbose. Returns MODDataset A MODDataset object containing the selected events or jets from the specified collection, dataset, and subdatasets. filter_particles energyflow.datasets.mod.filter_particles(particles, which='all', pt_cut=None, chs=False, pt_i=0, pid_i=4, vertex_i=5) Constructs a mask that will select particles according to specified properties. Currently supported are selecting particles according to their charge, removing particles associated to a pileup vertex, and implementing a minimum particle-level pT cut Arguments particles : numpy.ndarray Two-dimensional array of particles. which : { 'all' , 'charged' , 'neutral' } Selects particles according to their charge. pt_cut : float or None If not None , the minimum transverse momentum a particle can have to be selected. chs : bool Whether or not to implement charged hadron subtraction (CHS), which removes particles associated to a non-leading vertex (i.e. with vertex ids greater than or equal to 1). pt_i : int Column index of the transverse momentum values of the particles. pid_i : int Column index of the particle IDs (used to select by charge). vertex_i : int Column index of the vertex IDs (used to implement CHS). Returns numpy.ndarray A boolean mask which selects the specified particles, i.e. particles[filter_particles(particles, ...)] will be an array of only those particles passing the specified cuts. kfactors energyflow.datasets.mod.kfactors(dataset, pts, npvs=None, collection='CMS2011AJets', apply_residual_correction=True) Evaluates k-factors used by a particular collection. Currently, since CMS2011AJets is the only supported collection, some of the arguments are specific to the details of this collection (such as the use of jet pTs) and may change in future versions of this function. Arguments dataset : { 'sim' , 'gen' } Specifies which type of k-factor to use. 'sim' includes a reweighting to match the distribution of the number of primary vertices between the simulation dataset and the CMS data whereas 'gen' does not. pts : numpy.ndarray The transverse momenta of the jets, used to determine the pT-dependent k-factor due to using only leading order matrix elements in the event generation. For the CMS2011AJets collection, these are derived from Figure 5 of this reference . npvs : numpy.ndarray of integer type or None The number of primary vertices of a simulated event, used to reweight a simulated event to match the pileup distribution of data. Should be the same length as pts and correspond to the same events. Not used if dataset is 'gen' . collection : str Name of the collection of datasets to consider. Currently the only collection is 'CMS2011AJets' , though more may be added in the future. apply_residual_correction : bool Whether or not to apply a residual correction derived from the first bin of the pT spectrum that corrects for the remaining difference between data and simulation. Returns numpy.ndarray An array of k-factors corresponding to the events specified by the pts and (optionally) the npvs arrays. These should be multiplied into any existing weight for the simulated or generated event. Quark and Gluon Jets Four datasets of quark and gluon jets, each having two million total jets, have been generated with Pythia and Herwig and are accessible through this submodule of EnergyFlow. The four datasets are: Pythia 8.226 quark (uds) and gluon jets. Pythia 8.235 quark (udscb) and gluon jets. Herwig 7.1.4 quark (uds) and gluon jets. Herwig 7.1.4 quark (udscb) and gluon jets To avoid downloading unnecessary samples, the datasets are contained in twenty files with 100k jets each, and only the required files are downloaded. These are based on the samples used in 1810.05165 . Splitting the data into 1.6M/200k/200k train/validation/test sets is recommended for standardized comparisons. Each dataset consists of two components: X : a three-dimensional numpy array of the jets with shape (num_data,max_num_particles,4) . y : a numpy array of quark/gluon jet labels (quark= 1 and gluon= 0 ). The jets are padded with zero-particles in order to make a contiguous array. The particles are given as (pt,y,phi,pid) values, where pid is the particle's PDG id . Quark jets either include or exclude $c$ and $b$ quarks depending on the with_bc argument. The samples are generated from $q\\bar q\\to Z(\\to\\nu\\bar\\nu)+g$ and $qg\\to Z(\\to\\nu\\bar\\nu)+(uds[cb])$ processes in $pp$ collisions at $\\sqrt{s}=14$ TeV. Hadronization and multiple parton interactions (i.e. underlying event) are turned on and the default tunings and shower parameters are used. Final state non-neutrino particles are clustered into $R=0.4$ anti-$k_T$ jets using FastJet 3.3.0. Jets with transverse momentum $p_T\\in[500,550]$ GeV and rapidity $|y|<1.7$ are kept. Particles are ensured have to $\\phi$ values within $\\pi$ of the jet (i.e. no $\\phi$-periodicity issues). No detector simulation is performed. The samples are also hosted on Zenodo and we ask that you cite them appropriately if they are useful to your research. For BibTex entries, see the FAQs . - Pythia samples - Herwig samples load energyflow.datasets.qg_jets.load(num_data=100000, generator='pythia', pad=True, with_bc=False, cache_dir='~/.energyflow') Loads samples from the dataset (which in total is contained in twenty files). Any file that is needed that has not been cached will be automatically downloaded. Downloading a file causes it to be cached for later use. Basic checksums are performed. Arguments num_data : int The number of events to return. A value of -1 means read in all events. generator : str Specifies which Monte Carlo generator the events should come from. Currently, the options are 'pythia' and 'herwig' . pad : bool Whether to pad the events with zeros to make them the same length. Note that if set to False , the returned X array will be an object array and not a 3-d array of floats. with_bc : bool Whether to include jets coming from bottom or charm quarks. Changing this flag does not mask out these jets but rather accesses an entirely different dataset. The datasets with and without b and c quarks should not be combined. cache_dir : str The directory where to store/look for the files. Note that 'datasets' is automatically appended to the end of this path. Returns 3-d numpy.ndarray , 1-d numpy.ndarray The X and y components of the dataset as specified above. If pad is False then these will be object arrays holding the events, each of which is a 2-d ndarray. Quark and Gluon Nsubs A dataset consisting of 45 $N$-subjettiness observables for 100k quark and gluon jets generated with Pythia 8.230. Following 1704.08249 , the observables are in the following order: \\{\\tau_1^{(\\beta=0.5)},\\tau_1^{(\\beta=1.0)},\\tau_1^{(\\beta=2.0)}, \\tau_2^{(\\beta=0.5)},\\tau_2^{(\\beta=1.0)},\\tau_2^{(\\beta=2.0)}, \\ldots, \\tau_{15}^{(\\beta=0.5)},\\tau_{15}^{(\\beta=1.0)},\\tau_{15}^{(\\beta=2.0)}\\}. The dataset contains two members: 'X' which is a numpy array of the nsubs that has shape (100000,45) and 'y' which is a numpy array of quark/gluon labels (quark= 1 and gluon= 0 ). load energyflow.datasets.qg_nsubs.load(num_data=-1, cache_dir=None) Loads the dataset. The first time this is called, it will automatically download the dataset. Future calls will attempt to use the cached dataset prior to redownloading. Arguments num_data : int The number of events to return. A value of -1 means read in all events. cache_dir : str The directory where to store/look for the file. Returns 3-d numpy.ndarray , 1-d numpy.ndarray The X and y components of the dataset as specified above.","title":"Datasets"},{"location":"docs/datasets/#cms-open-data-and-the-mod-hdf5-format","text":"Starting in 2014, the CMS Collaboration began to release research-grade recorded and simulated datasets on the CERN Open Data Portal . These fantastic resources provide a unique opportunity for researchers with diverse connections to the hep-ex world to engage with cutting edge particle physics by developing tools and testing novel strategies on actual LHC data. Our goal in making portions of the CMS Open Data available in a reprocessed format is to ease as best as possible the technical complications that have thus far been present when attempting to use Open Data (see also recent efforts by the CMS Collaboration to make the data more accessible). To facilitate access to Open Data, we have developed a format utilizing the widespread HDF5 file format that stores essential information for some particle physics analyses, such as the ones in Exploring the Space of Jets with CMS Open Data . This \"MOD HDF5 Format\" is currently optimized for studies based on jets, but may be updated in the future to support other types of analyses. To further the goals of Open Data, we have made our reprocessed samples available on the Zenodo platform . Currently, the only \"collection\" (in the parlance used below by the functions and classes that load and read MOD HDF5 files) of datasets that is available is the CMS2011AJets collection, which was used in Exploring the Space of Jets with CMS Open Data for EMD -based studies. More collections may be added in the future as our research group completes more studies with the Open Data. For now, this page focuses on the CMS2011AJets collection. This collection of datasets includes jets recorded by the CMS detector, as well as CMS-generated Pythia samples that are further passed through full CMS detector simulation. Hence, the datasets are refered to as CMS, SIM, and GEN (or in code as 'cms' , 'sim' , 'gen' ) for these three different ways of obtaining jets. The datasets include all available jets above 375 GeV, which is where the Jet300 trigger was found to be fully efficient in both data and simulation. Note that the pT values references in the name of the SIM/GEN datasets are those of the generator-level hard parton. The DOIs of CMS2011AJets MOD HDF5 datasets are: - CMS 2011A Jets, pT > 375 GeV - SIM/GEN QCD Jets 170-300 GeV - SIM/GEN QCD Jets 300-470 GeV - SIM/GEN QCD Jets 470-600 GeV - SIM/GEN QCD Jets 600-800 GeV - SIM/GEN QCD Jets 800-1000 GeV - SIM/GEN QCD Jets 1000-1400 GeV - SIM/GEN QCD Jets 1400-1800 GeV - SIM/GEN QCD Jets 1800-$\\infty$ GeV For more details regarding the creation of these samples, as well as for the DOIs of the original CMS Open Datasets, see Exploring the Space of Jets with CMS Open Data .","title":"CMS Open Data and the MOD HDF5 Format"},{"location":"docs/datasets/#moddataset","text":"Loads and provides access to datasets in MOD HDF5 format. Jets can be selected while loading from file according to a number of kinematic attributes. MOD HDF5 datasets are created via the save method. Currently, the MOD HDF5 format consists of an HDF5 file with the following arrays, each of which are stored as properties of the MODDataset : /jets_i - int64 An array of integer jet attributes, which are currently: fn : The file number of the jet, used to index the filenames array. rn : The run number of the jet. lbn : The lumiblock number (or lumisection) of the jet. evn : The event number of the jet. npv (CMS/SIM only) : The number of primary vertices of the event containing the jet. quality (CMS/SIM only) : The quality of the jet, where 0 means no quality, 1 is \"loose\", 2 is \"medium\", and 3 is \"strict\". hard_pid (SIM/GEN only) : The particle ID of the hard parton associated to the jet ( 0 if not associated). /jets_f - float64 An array of floating point jet attributes, which are currently: jet_pt : Transverse momentum of the jet. jet_y : Rapidity of the jet. jet_phi : Azimuthal angle of the jet. jet_m : Mass of the jet. jet_eta : Pseudorapidity of the jet. jec (CMS/SIM only) : Jet energy correction. jet_area (CMS/SIM only) : Area of the jet. jet_max_nef (CMS/SIM only) : Maximum of the hadronic and electromagnetic energy fractions of the jet. gen_jet_pt (SIM only) : Transverse momentum of an associated GEN jet. -1 if not associated. gen_jet_y (SIM only) : Rapidity of an associated GEN jet. -1 if not associated. gen_jet_phi (SIM only) : Azimuthal angle of an associated GEN jet. -1 if not associated. gen_jet_m (SIM only) : Mass of an associated GEN jet. -1 if not associated. gen_jet_eta (SIM only) : Pseudorapidity of an associated GEN jet. -1 if not associated. hard_pt (SIM/GEN only) : Transverse momentum of an associated hard parton. -1 if not associated. hard_y (SIM/GEN only) : Rapidity of an associated hard parton. -1 if not associated. hard_phi (SIM/GEN only) : Azimuthal angle of an associated hard parton. -1 if not associated. weight : Contribution of this jet to the cross-section, in nanobarns. /pfcs - float64 (CMS/SIM only) An array of all particle flow candidates, with attributes listed below. There is a separate /pfcs_index array in the file which contains information for MODDataset to separate these particles into separate jets. The columns of the array are currently: pt : Transverse momentum of the PFC. y : Rapidity of the PFC. phi : Azimuthal angle of the PFC. m : Mass of the PFC. pid : PDG ID of the PFC. vertex : Vertex ID of the PFC. 0 is leading vertex, >0 is a pileup vertex, and -1 is unknown. Neutral particles are assigned to the leading vertex. /gens - float64 (SIM/GEN only) An array of all generator-level particles, currently with the same columns as the pfcs array (the vertex column contains all 0 s). For the SIM dataset, these are the particles of jets associated to the SIM jets which are described in the jets_i and jets_f arrays. As with pfcs , there is a separate gens_index array which tells MODDataset how to separate these gen particles into distinct jets. /filenames - str An array of strings indexed by the fn attribute of each jet. For CMS, this array is one dimensional and contains the CMS-provided filenames. For SIM/GEN, this array is two dimensional where the first column is the pT value that appears in the name of the dataset and the second column is the CMS-provided filename. In all cases, indexing this array with the fn attribute of a jet gives the file information in which the event containing that jet is to be found. Note that the column names of the jets_i , jets_f , pfcs , and gens arrays are stored as lists of strings in the attributes jets_i_cols , jets_f_cols , pfcs_cols , and gens_cols . For each of the above arrays, MODDataset stores the index of the column as an attribute with the same name as the column. For example, for an instance called modds , modds.fn has a value of 0 since it is the first column in the jets_i array, modds.jet_phi has a value of 2 , modds.m has a value of 3 , etc. Even more helpfully, a view of each column of the jets arrays is stored as an attribute as well, so that modds.jet_pts is the same as modds.jets_f[:,modds.jet_pt] , modds.evns is the same as modds.jets_i[:,modds.evn] , etc. Additionally, one special view is stored, corr_jet_pts , which is equal to the product of the jet pTs and the JECs, i.e. modds.jet_pts*modds.jecs . MODDataset supports the builtin len() method, which returns the number of jets currently stored in the dataset, as well as the print() method, which prints a summary of the dataset. energyflow.datasets.mod.MODDataset(*args, datasets=None, path=None, num=-1, shuffle=True, store_pfcs=True, store_gens=True) MODDataset can be initialized from a MOD HDF5 file or from a list of existing MODDataset s. In the first case, the filename should be given as the first positional argument. In the second case, the datasets keyword argument should be set to a list of MODDataset objects. Arguments *args : arbitrary positional arguments Each argument specifies a requirement for an event to be selected and kept in the MODDataset . All requirements are ANDed together. Each specification can be a string or a tuple/list of objects that will be converted to strings and concatenated together. Each string specifies the name of one of the columns of one of the jets arrays ( 'corr_jet_pts' is also accepted, see above, as well as 'abs_jet_eta' , 'abs_gen_jet_y' , etc, which use the absolute values of the [pseudo]rapidities of the jets) as well as one or more comparisons to be performed using the values of that column and the given values in the string. For example, ('corr_jet_pts >', 400) , which is the same as 'corr_jet_pts>400' , will select jets with a corrected pT above 400 GeV. Ampersands may be used within one string to indicated multiple requirements, e.g. 'corr_jet_pts > 400 & abs_jet_eta' , which has the same effect as using multiple arguements each with a single requirement. datasets : { tuple , list } of MODDataset instances or None MODDataset s from which to initialize this dataset. Effectively what this does is to concatenate the arrays held by the datasets. Should always be None when initializing from an existing file. path : str or None If not None , then path is prepended to the filename when initializing from file. Has no effect when initializing from existing datasets. num : int The number of events or jets to keep after subselections are applied. A value of -1 keeps the entire dataset. The weights are properly rescaled to preserve the total cross section of the selection. shuffle : bool When subselecting a fraction of the dataset (i.e. num!=-1 ), if False the first num events passing cuts will be kept, if True then a random subset of num events will be kept. Note that this has no effect when num is -1 , and also that this flag only affects which events are selected and does not randomize the order of the events that are ultimately stored by the MODDataset object. store_pfcs : bool Whether or not to store PFCs if they are present in the dataset. store_gens : bool Whether or not to store gen-level particles (referred to as \"gens\") if they are present in the dataset.","title":"MODDataset"},{"location":"docs/datasets/#sel","text":"sel(args, kwargs) Returns a boolean mask that selects jets according to the specified requirements. Arguments *args : arbitrary positional arguments Used to specify cuts to be made to the dataset while loading; see the detailed description of the positional arguments accepted by MODDataset . Returns numpy.ndarray of type bool A boolean mask that will select jets that pass all of the specified requirements.","title":"sel"},{"location":"docs/datasets/#apply_mask","text":"apply_mask(mask, preserve_total_weight=False) Subselects jets held by the MODDataset according to a boolean mask. Arguments mask : numpy.ndarray or type bool A boolean mask used to select which jets are to be kept. Should be the same length as the MODDataset object. preserve_total_weight : bool Whether or not to keep the cross section of the MODDataset fixed after the selection.","title":"apply_mask"},{"location":"docs/datasets/#save","text":"save(filepath, npf=-1, compression=None, verbose=1, n_jobs=1) Saves a MODDataset in the MOD HDF5 format. Arguments filepath : str The filepath (with or without the '.h5' suffix) where the saved file will be located. npf : int The number of jets per file. If not -1 , multiple files will be created with npf jets as the maximum number stored in each file, in which case '_INDEX' , where INDEX is the index of that file, will be appended to the filename. compression : int or None If not None , the gzip compression level to use when saving the arrays in the HDF5 file. If not None , '_compressed' will be appended to the end of the filename. verbose : int Verbosity level to use when saving the files. n_jobs : int The number of processes to use when saving the files; only relevant when npf!=-1 .","title":"save"},{"location":"docs/datasets/#close","text":"close() Closes the underlying HDF5 file, if one is associated with the MODDataset object. Note that associated HDF5 files are closed by default when the MODDataset object is deleted.","title":"close"},{"location":"docs/datasets/#jets_i","text":"jets_i The jets_i array, described under MODDataset .","title":"jets_i"},{"location":"docs/datasets/#jets_f","text":"jets_f The jets_f array, described under MODDataset .","title":"jets_f"},{"location":"docs/datasets/#pfcs","text":"pfcs The pfcs array, described under MODDataset .","title":"pfcs"},{"location":"docs/datasets/#gens","text":"gens The gens array, described under MODDataset .","title":"gens"},{"location":"docs/datasets/#particles","text":"particles If this is a CMS or SIM dataset, particles is the same as pfcs ; for GEN it is the same as gens .","title":"particles"},{"location":"docs/datasets/#filenames","text":"filenames The filenames array, described under MODDataset .","title":"filenames"},{"location":"docs/datasets/#hf","text":"hf The underlying HDF5 file, if one is associated to the MODDataset .","title":"hf"},{"location":"docs/datasets/#load","text":"energyflow.datasets.mod.load(*args, amount=1, cache_dir='~/.energyflow', collection='CMS2011AJets', dataset='cms', subdatasets=None, validate_files=False, store_pfcs=True, store_gens=True, verbose=0) Loads samples from the specified MOD dataset. Any file that is needed that has not been cached will be automatically downloaded from Zenodo. Downloaded files are cached for later use. File checksums are optionally validated to ensure dataset fidelity. Arguments *args : arbitrary positional arguments Used to specify cuts to be made to the dataset while loading; see the detailed description of the positional arguments accepted by MODDataset . amount : int or float Approximate amount of the dataset to load. If an integer, this is the number of files to load (a warning is issued if more files are requested than are available). If a float, this is the fraction of the number of available files to load, rounded up to the nearest whole number. Note that since ints and floats are treated different, a value of 1 loads one file whereas 1.0 loads the entire dataset. A value of -1 also loads the entire dataset. cache_dir : str The directory where to store/look for the files. Note that 'datasets' is automatically appended to the end of this path, as well as the collection name. For example, the default is to download/look for files in the directory '~/.energyflow/datasets/CMS2011AJets' . collection : str Name of the collection of datasets to consider. Currently the only collection is 'CMS2011AJets' , though more may be added in the future. dataset : str Which dataset in the collection to load. Currently the 'CMS2011AJets' collection has 'cms' , 'sim' , and 'gen' datasets. subdatasets : { tuple , list } of str or None The names of subdatasets to use. A value of None uses all available subdatasets. Currently, for the 'CMS2011AJets' collection, the 'cms' dataset has one subdataset, 'CMS_Jet300_pT375-infGeV' , the 'sim' dataset has eight subdatasets arrange according to generator $\\hat p_T$, e.g. 'SIM470_Jet300_pT375-infGeV' , and the 'gen' dataset also has eight subdatasets arranged similaraly, e.g. 'GEN470_pT375-infGeV' . validate_files : bool Whether or not to validate files according to their MD5 hashes. It is a good idea to set this to True when first downloading the files from Zenodo in order to ensure they downloaded properly. store_pfcs : bool Whether or not to store PFCs if they are present in the dataset. store_gens : bool Whether or not to store gen-level particles (referred to as \"gens\") if they are present in the dataset. verbose : int Verbosity level to use when loading. 0 is the least verbose, 1 is more verbose, and 2 is the most verbose. Returns MODDataset A MODDataset object containing the selected events or jets from the specified collection, dataset, and subdatasets.","title":"load"},{"location":"docs/datasets/#filter_particles","text":"energyflow.datasets.mod.filter_particles(particles, which='all', pt_cut=None, chs=False, pt_i=0, pid_i=4, vertex_i=5) Constructs a mask that will select particles according to specified properties. Currently supported are selecting particles according to their charge, removing particles associated to a pileup vertex, and implementing a minimum particle-level pT cut Arguments particles : numpy.ndarray Two-dimensional array of particles. which : { 'all' , 'charged' , 'neutral' } Selects particles according to their charge. pt_cut : float or None If not None , the minimum transverse momentum a particle can have to be selected. chs : bool Whether or not to implement charged hadron subtraction (CHS), which removes particles associated to a non-leading vertex (i.e. with vertex ids greater than or equal to 1). pt_i : int Column index of the transverse momentum values of the particles. pid_i : int Column index of the particle IDs (used to select by charge). vertex_i : int Column index of the vertex IDs (used to implement CHS). Returns numpy.ndarray A boolean mask which selects the specified particles, i.e. particles[filter_particles(particles, ...)] will be an array of only those particles passing the specified cuts.","title":"filter_particles"},{"location":"docs/datasets/#kfactors","text":"energyflow.datasets.mod.kfactors(dataset, pts, npvs=None, collection='CMS2011AJets', apply_residual_correction=True) Evaluates k-factors used by a particular collection. Currently, since CMS2011AJets is the only supported collection, some of the arguments are specific to the details of this collection (such as the use of jet pTs) and may change in future versions of this function. Arguments dataset : { 'sim' , 'gen' } Specifies which type of k-factor to use. 'sim' includes a reweighting to match the distribution of the number of primary vertices between the simulation dataset and the CMS data whereas 'gen' does not. pts : numpy.ndarray The transverse momenta of the jets, used to determine the pT-dependent k-factor due to using only leading order matrix elements in the event generation. For the CMS2011AJets collection, these are derived from Figure 5 of this reference . npvs : numpy.ndarray of integer type or None The number of primary vertices of a simulated event, used to reweight a simulated event to match the pileup distribution of data. Should be the same length as pts and correspond to the same events. Not used if dataset is 'gen' . collection : str Name of the collection of datasets to consider. Currently the only collection is 'CMS2011AJets' , though more may be added in the future. apply_residual_correction : bool Whether or not to apply a residual correction derived from the first bin of the pT spectrum that corrects for the remaining difference between data and simulation. Returns numpy.ndarray An array of k-factors corresponding to the events specified by the pts and (optionally) the npvs arrays. These should be multiplied into any existing weight for the simulated or generated event.","title":"kfactors"},{"location":"docs/datasets/#quark-and-gluon-jets","text":"Four datasets of quark and gluon jets, each having two million total jets, have been generated with Pythia and Herwig and are accessible through this submodule of EnergyFlow. The four datasets are: Pythia 8.226 quark (uds) and gluon jets. Pythia 8.235 quark (udscb) and gluon jets. Herwig 7.1.4 quark (uds) and gluon jets. Herwig 7.1.4 quark (udscb) and gluon jets To avoid downloading unnecessary samples, the datasets are contained in twenty files with 100k jets each, and only the required files are downloaded. These are based on the samples used in 1810.05165 . Splitting the data into 1.6M/200k/200k train/validation/test sets is recommended for standardized comparisons. Each dataset consists of two components: X : a three-dimensional numpy array of the jets with shape (num_data,max_num_particles,4) . y : a numpy array of quark/gluon jet labels (quark= 1 and gluon= 0 ). The jets are padded with zero-particles in order to make a contiguous array. The particles are given as (pt,y,phi,pid) values, where pid is the particle's PDG id . Quark jets either include or exclude $c$ and $b$ quarks depending on the with_bc argument. The samples are generated from $q\\bar q\\to Z(\\to\\nu\\bar\\nu)+g$ and $qg\\to Z(\\to\\nu\\bar\\nu)+(uds[cb])$ processes in $pp$ collisions at $\\sqrt{s}=14$ TeV. Hadronization and multiple parton interactions (i.e. underlying event) are turned on and the default tunings and shower parameters are used. Final state non-neutrino particles are clustered into $R=0.4$ anti-$k_T$ jets using FastJet 3.3.0. Jets with transverse momentum $p_T\\in[500,550]$ GeV and rapidity $|y|<1.7$ are kept. Particles are ensured have to $\\phi$ values within $\\pi$ of the jet (i.e. no $\\phi$-periodicity issues). No detector simulation is performed. The samples are also hosted on Zenodo and we ask that you cite them appropriately if they are useful to your research. For BibTex entries, see the FAQs . - Pythia samples - Herwig samples","title":"Quark and Gluon Jets"},{"location":"docs/datasets/#load_1","text":"energyflow.datasets.qg_jets.load(num_data=100000, generator='pythia', pad=True, with_bc=False, cache_dir='~/.energyflow') Loads samples from the dataset (which in total is contained in twenty files). Any file that is needed that has not been cached will be automatically downloaded. Downloading a file causes it to be cached for later use. Basic checksums are performed. Arguments num_data : int The number of events to return. A value of -1 means read in all events. generator : str Specifies which Monte Carlo generator the events should come from. Currently, the options are 'pythia' and 'herwig' . pad : bool Whether to pad the events with zeros to make them the same length. Note that if set to False , the returned X array will be an object array and not a 3-d array of floats. with_bc : bool Whether to include jets coming from bottom or charm quarks. Changing this flag does not mask out these jets but rather accesses an entirely different dataset. The datasets with and without b and c quarks should not be combined. cache_dir : str The directory where to store/look for the files. Note that 'datasets' is automatically appended to the end of this path. Returns 3-d numpy.ndarray , 1-d numpy.ndarray The X and y components of the dataset as specified above. If pad is False then these will be object arrays holding the events, each of which is a 2-d ndarray.","title":"load"},{"location":"docs/datasets/#quark-and-gluon-nsubs","text":"A dataset consisting of 45 $N$-subjettiness observables for 100k quark and gluon jets generated with Pythia 8.230. Following 1704.08249 , the observables are in the following order: \\{\\tau_1^{(\\beta=0.5)},\\tau_1^{(\\beta=1.0)},\\tau_1^{(\\beta=2.0)}, \\tau_2^{(\\beta=0.5)},\\tau_2^{(\\beta=1.0)},\\tau_2^{(\\beta=2.0)}, \\ldots, \\tau_{15}^{(\\beta=0.5)},\\tau_{15}^{(\\beta=1.0)},\\tau_{15}^{(\\beta=2.0)}\\}. The dataset contains two members: 'X' which is a numpy array of the nsubs that has shape (100000,45) and 'y' which is a numpy array of quark/gluon labels (quark= 1 and gluon= 0 ).","title":"Quark and Gluon Nsubs"},{"location":"docs/datasets/#load_2","text":"energyflow.datasets.qg_nsubs.load(num_data=-1, cache_dir=None) Loads the dataset. The first time this is called, it will automatically download the dataset. Future calls will attempt to use the cached dataset prior to redownloading. Arguments num_data : int The number of events to return. A value of -1 means read in all events. cache_dir : str The directory where to store/look for the file. Returns 3-d numpy.ndarray , 1-d numpy.ndarray The X and y components of the dataset as specified above.","title":"load"},{"location":"docs/efps/","text":"The Energy Flow Polynomials (EFPs) are a set of observables, indexed by non-isomorphic multigraphs, which linearly span the space of infrared and collinear safe (IRC-safe) observables. An EFP, indexed by a multigraph $G$, takes the following form: \\text{EFP}_G=\\sum_{i_1=1}^M\\cdots\\sum_{i_N=1}^Mz_{i_1}\\cdots z_{i_N} \\prod_{(k,\\ell)\\in G}\\theta_{i_ki_\\ell} where $z_i$ is a measure of the energy of particle $i$ and $\\theta_{ij}$ is a measure of the angular separation between particles $i$ and $j$. The specific choices for \"energy\" and \"angular\" measure depend on the collider context and are discussed in the Measures section. EFP A class for representing and computing a single EFP. Note that all keyword arguments are stored as properties of the EFP instance. energyflow.EFP(edges, measure='hadr', beta=1, kappa=1, normed=True, coords=None, check_input=True, np_optimize='greedy') Arguments edges : list Edges of the EFP graph specified by pairs of vertices. measure : { 'hadr' , 'hadrdot' , 'ee' } The choice of measure. See Measures for additional info. beta : float The parameter $\\beta$ appearing in the measure. Must be greater than zero. kappa : { float , 'pf' } If a number, the energy weighting parameter $\\kappa$. If 'pf' , use $\\kappa=v-1$ where $v$ is the valency of the vertex. normed : bool Controls normalization of the energies in the measure. coords : { 'ptyphim' , 'epxpypz' , None } Controls which coordinates are assumed for the input. See Measures for additional info. check_input : bool Whether to check the type of the input each time or assume the first input type. np_optimize : { True , False , 'greedy' , 'optimal' } The optimize keyword of numpy.einsum_path . compute compute(event=None, zs=None, thetas=None) Computes the value of the EFP on a single event. Arguments event : 2-d array_like or fastjet.PseudoJet The event as an array of particles in the coordinates specified by coords . zs : 1-d array_like If present, thetas must also be present, and zs is used in place of the energies of an event. thetas : 2-d array_like If present, zs must also be present, and thetas is used in place of the pairwise angles of an event. Returns float The EFP value. batch_compute batch_compute(events, n_jobs=None) Computes the value of the EFP on several events. Arguments events : array_like or fastjet.PseudoJet The events as an array of arrays of particles in coordinates matching those anticipated by coords . n_jobs : int or None The number of worker processes to use. A value of None will use as many processes as there are CPUs on the machine. Returns 1-d numpy.ndarray A vector of the EFP value for each event. graph graph Graph of this EFP represented by a list of edges. simple_graph simple_graph Simple graph of this EFP (forgetting all multiedges) represented by a list of edges. n n Number of vertices in the graph of this EFP. d d Degree, or number of edges, in the graph of this EFP. c c VE complexity $\\chi$ of this EFP. EFPSet A class that holds a collection of EFPs and computes their values on events. Note that all keyword arguments are stored as properties of the EFPSet instance. energyflow.EFPSet(*args, filename=None, measure='hadr', beta=1, kappa=1, normed=True, coords='ptyphim', check_input=True, verbose=False) EFPSet can be initialized in one of three ways (in order of precedence): Default - Use the ($d\\le10$) EFPs that come installed with the EnergFlow package. Generator - Pass in a custom Generator object as the first positional argument. Custom File - Pass in the name of a .npz file saved with a custom Generator . To control which EFPs are included, EFPSet accepts an arbitrary number of specifications (see sel ) and only EFPs meeting each specification are included in the set. Arguments *args : arbitrary positional arguments If the first positional argument is a Generator instance, it is used for initialization. The remaining positional arguments must be valid arguments to sel . filename : string Path to a .npz file which has been saved by a valid energyflow.Generator . measure : { 'hadr' , 'hadr-dot' , 'ee' } See Measures for additional info. beta : float The parameter $\\beta$ appearing in the measure. Must be greater than zero. kappa : { float , 'pf' } If a number, the energy weighting parameter $\\kappa$. If 'pf' , use $\\kappa=v-1$ where $v$ is the valency of the vertex. normed : bool Controls normalization of the energies in the measure. coords : { 'ptyphim' , 'epxpypz' , None } Controls which coordinates are assumed for the input. See Measures for additional info. check_input : bool Whether to check the type of the input each time or assume the first input type. verbose : bool Controls printed output when initializing EFPSet. compute compute(event=None, zs=None, thetas=None) Computes the values of the stored EFPs on a single event. Arguments event : 2-d array_like or fastjet.PseudoJet The event as an array of particles in the coordinates specified by coords . zs : 1-d array_like If present, thetas must also be present, and zs is used in place of the energies of an event. thetas : 2-d array_like If present, zs must also be present, and thetas is used in place of the pairwise angles of an event. Returns 1-d numpy.ndarray A vector of the EFP values. batch_compute batch_compute(events, n_jobs=None) Computes the value of the stored EFPs on several events. Arguments events : array_like or fastjet.PseudoJet The events as an array of arrays of particles in coordinates matching those anticipated by coords . n_jobs : int or None The number of worker processes to use. A value of None will attempt to use as many processes as there are CPUs on the machine. Returns 2-d numpy.ndarray An array of the EFP values for each event. calc_disc calc_disc(X) Computes disconnected EFPs according to the internal specifications using the connected EFPs provided as input. Arguments X : numpy.ndarray Array of connected EFPs. Rows are different events, columns are the different EFPs. Can handle a single event (a 1-dim array) as input. EFPs are assumed to be in the order expected by the instance of EFPSet ; the safest way to ensure this is to use the same EFPSet to calculate both connected and disconnected EFPs. This function is used internally in compute and batch_compute . Returns numpy.ndarray A concatenated array of the connected and disconnected EFPs. sel sel(*args) Computes a boolean mask of EFPs matching each of the specifications provided by the args . Arguments *args : arbitrary positional arguments Each argument can be either a string or a length-two iterable. If the argument is a string, it should consist of three parts: a character which is a valid element of cols , a comparison operator (one of < , > , <= , >= , == , != ), and a number. Whitespace between the parts does not matter. If the argument is a tuple, the first element should be a string containing a column header character and a comparison operator; the second element is the value to be compared. The tuple version is useful when the value is a variable that changes (such as in a list comprehension). Returns 1-d numpy.ndarray A boolean array of length the number of EFPs stored by this object. count count(*args) Counts the number of EFPs meeting the specifications of the arguments using sel . Arguments *args : arbitrary positional arguments Valid arguments to be passed to sel . Returns int The number of EFPs meeting the specifications provided. graphs graphs(*args) Graphs meeting provided specifications. Arguments *args : arbitrary positional arguments Valid arguments to be passed to sel , or, if a single integer, the index of a particular graph. Returns list , if single integer argument is given The list of edges corresponding to the specified graph 1-d numpy.ndarray , otherwise An array of graphs (as lists of edges) matching the specifications. simple_graphs simple_graphs(*args) Simple graphs meeting provided specifications. Arguments *args : arbitrary positional arguments Valid arguments to be passed to sel , or, if a single integer, the index of particular simple graph. Returns list , if single integer argument is given The list of edges corresponding to the specified simple graph 1-d numpy.ndarray , otherwise An array of simple graphs (as lists of edges) matching the specifications. specs specs An array of EFP specifications. Each row represents an EFP and the columns represent the quantities indicated by cols . cspecs cspecs Specification array for connected EFPs. cols cols Column labels for specs . Those of primary interest are listed below. n : Number of vertices. e : Number of simple edges. d : Degree, or number of multiedges. v : Maximum valency (number of edges touching a vertex). k : Unique identifier within EFPs of this (n,d). c : VE complexity $\\chi$. p : Number of prime factors (or connected components). h : Number of valency 1 vertices (a.k.a. 'hanging chads').","title":"Energy Flow Polynomials"},{"location":"docs/efps/#efp","text":"A class for representing and computing a single EFP. Note that all keyword arguments are stored as properties of the EFP instance. energyflow.EFP(edges, measure='hadr', beta=1, kappa=1, normed=True, coords=None, check_input=True, np_optimize='greedy') Arguments edges : list Edges of the EFP graph specified by pairs of vertices. measure : { 'hadr' , 'hadrdot' , 'ee' } The choice of measure. See Measures for additional info. beta : float The parameter $\\beta$ appearing in the measure. Must be greater than zero. kappa : { float , 'pf' } If a number, the energy weighting parameter $\\kappa$. If 'pf' , use $\\kappa=v-1$ where $v$ is the valency of the vertex. normed : bool Controls normalization of the energies in the measure. coords : { 'ptyphim' , 'epxpypz' , None } Controls which coordinates are assumed for the input. See Measures for additional info. check_input : bool Whether to check the type of the input each time or assume the first input type. np_optimize : { True , False , 'greedy' , 'optimal' } The optimize keyword of numpy.einsum_path .","title":"EFP"},{"location":"docs/efps/#compute","text":"compute(event=None, zs=None, thetas=None) Computes the value of the EFP on a single event. Arguments event : 2-d array_like or fastjet.PseudoJet The event as an array of particles in the coordinates specified by coords . zs : 1-d array_like If present, thetas must also be present, and zs is used in place of the energies of an event. thetas : 2-d array_like If present, zs must also be present, and thetas is used in place of the pairwise angles of an event. Returns float The EFP value.","title":"compute"},{"location":"docs/efps/#batch_compute","text":"batch_compute(events, n_jobs=None) Computes the value of the EFP on several events. Arguments events : array_like or fastjet.PseudoJet The events as an array of arrays of particles in coordinates matching those anticipated by coords . n_jobs : int or None The number of worker processes to use. A value of None will use as many processes as there are CPUs on the machine. Returns 1-d numpy.ndarray A vector of the EFP value for each event.","title":"batch_compute"},{"location":"docs/efps/#graph","text":"graph Graph of this EFP represented by a list of edges.","title":"graph"},{"location":"docs/efps/#simple_graph","text":"simple_graph Simple graph of this EFP (forgetting all multiedges) represented by a list of edges.","title":"simple_graph"},{"location":"docs/efps/#n","text":"n Number of vertices in the graph of this EFP.","title":"n"},{"location":"docs/efps/#d","text":"d Degree, or number of edges, in the graph of this EFP.","title":"d"},{"location":"docs/efps/#c","text":"c VE complexity $\\chi$ of this EFP.","title":"c"},{"location":"docs/efps/#efpset","text":"A class that holds a collection of EFPs and computes their values on events. Note that all keyword arguments are stored as properties of the EFPSet instance. energyflow.EFPSet(*args, filename=None, measure='hadr', beta=1, kappa=1, normed=True, coords='ptyphim', check_input=True, verbose=False) EFPSet can be initialized in one of three ways (in order of precedence): Default - Use the ($d\\le10$) EFPs that come installed with the EnergFlow package. Generator - Pass in a custom Generator object as the first positional argument. Custom File - Pass in the name of a .npz file saved with a custom Generator . To control which EFPs are included, EFPSet accepts an arbitrary number of specifications (see sel ) and only EFPs meeting each specification are included in the set. Arguments *args : arbitrary positional arguments If the first positional argument is a Generator instance, it is used for initialization. The remaining positional arguments must be valid arguments to sel . filename : string Path to a .npz file which has been saved by a valid energyflow.Generator . measure : { 'hadr' , 'hadr-dot' , 'ee' } See Measures for additional info. beta : float The parameter $\\beta$ appearing in the measure. Must be greater than zero. kappa : { float , 'pf' } If a number, the energy weighting parameter $\\kappa$. If 'pf' , use $\\kappa=v-1$ where $v$ is the valency of the vertex. normed : bool Controls normalization of the energies in the measure. coords : { 'ptyphim' , 'epxpypz' , None } Controls which coordinates are assumed for the input. See Measures for additional info. check_input : bool Whether to check the type of the input each time or assume the first input type. verbose : bool Controls printed output when initializing EFPSet.","title":"EFPSet"},{"location":"docs/efps/#compute_1","text":"compute(event=None, zs=None, thetas=None) Computes the values of the stored EFPs on a single event. Arguments event : 2-d array_like or fastjet.PseudoJet The event as an array of particles in the coordinates specified by coords . zs : 1-d array_like If present, thetas must also be present, and zs is used in place of the energies of an event. thetas : 2-d array_like If present, zs must also be present, and thetas is used in place of the pairwise angles of an event. Returns 1-d numpy.ndarray A vector of the EFP values.","title":"compute"},{"location":"docs/efps/#batch_compute_1","text":"batch_compute(events, n_jobs=None) Computes the value of the stored EFPs on several events. Arguments events : array_like or fastjet.PseudoJet The events as an array of arrays of particles in coordinates matching those anticipated by coords . n_jobs : int or None The number of worker processes to use. A value of None will attempt to use as many processes as there are CPUs on the machine. Returns 2-d numpy.ndarray An array of the EFP values for each event.","title":"batch_compute"},{"location":"docs/efps/#calc_disc","text":"calc_disc(X) Computes disconnected EFPs according to the internal specifications using the connected EFPs provided as input. Arguments X : numpy.ndarray Array of connected EFPs. Rows are different events, columns are the different EFPs. Can handle a single event (a 1-dim array) as input. EFPs are assumed to be in the order expected by the instance of EFPSet ; the safest way to ensure this is to use the same EFPSet to calculate both connected and disconnected EFPs. This function is used internally in compute and batch_compute . Returns numpy.ndarray A concatenated array of the connected and disconnected EFPs.","title":"calc_disc"},{"location":"docs/efps/#sel","text":"sel(*args) Computes a boolean mask of EFPs matching each of the specifications provided by the args . Arguments *args : arbitrary positional arguments Each argument can be either a string or a length-two iterable. If the argument is a string, it should consist of three parts: a character which is a valid element of cols , a comparison operator (one of < , > , <= , >= , == , != ), and a number. Whitespace between the parts does not matter. If the argument is a tuple, the first element should be a string containing a column header character and a comparison operator; the second element is the value to be compared. The tuple version is useful when the value is a variable that changes (such as in a list comprehension). Returns 1-d numpy.ndarray A boolean array of length the number of EFPs stored by this object.","title":"sel"},{"location":"docs/efps/#count","text":"count(*args) Counts the number of EFPs meeting the specifications of the arguments using sel . Arguments *args : arbitrary positional arguments Valid arguments to be passed to sel . Returns int The number of EFPs meeting the specifications provided.","title":"count"},{"location":"docs/efps/#graphs","text":"graphs(*args) Graphs meeting provided specifications. Arguments *args : arbitrary positional arguments Valid arguments to be passed to sel , or, if a single integer, the index of a particular graph. Returns list , if single integer argument is given The list of edges corresponding to the specified graph 1-d numpy.ndarray , otherwise An array of graphs (as lists of edges) matching the specifications.","title":"graphs"},{"location":"docs/efps/#simple_graphs","text":"simple_graphs(*args) Simple graphs meeting provided specifications. Arguments *args : arbitrary positional arguments Valid arguments to be passed to sel , or, if a single integer, the index of particular simple graph. Returns list , if single integer argument is given The list of edges corresponding to the specified simple graph 1-d numpy.ndarray , otherwise An array of simple graphs (as lists of edges) matching the specifications.","title":"simple_graphs"},{"location":"docs/efps/#specs","text":"specs An array of EFP specifications. Each row represents an EFP and the columns represent the quantities indicated by cols .","title":"specs"},{"location":"docs/efps/#cspecs","text":"cspecs Specification array for connected EFPs.","title":"cspecs"},{"location":"docs/efps/#cols","text":"cols Column labels for specs . Those of primary interest are listed below. n : Number of vertices. e : Number of simple edges. d : Degree, or number of multiedges. v : Maximum valency (number of edges touching a vertex). k : Unique identifier within EFPs of this (n,d). c : VE complexity $\\chi$. p : Number of prime factors (or connected components). h : Number of valency 1 vertices (a.k.a. 'hanging chads').","title":"cols"},{"location":"docs/emd/","text":"The Energy Mover's Distance (EMD), also known as the Earth Mover's Distance, is a metric between particle collider events introduced in 1902.02346 . This submodule contains convenient functions for computing EMDs between individual events and collections of events. The core of the computation is done using the Python Optimal Transport (POT) library, which must be installed in order to use this submodule. From Eq. 1 in 1902.02346 , the EMD between two events is the minimum ''work'' required to rearrange one event $\\mathcal E$ into the other $\\mathcal E'$ by movements of energy $f_{ij}$ from particle $i$ in one event to particle $j$ in the other: \\text{EMD}(\\mathcal E,\\mathcal E^\\prime)=\\min_{\\{f_{ij}\\}}\\sum_{ij}f_{ij}\\frac{ \\theta_{ij}}{R} + \\left|\\sum_iE_i-\\sum_jE^\\prime_j\\right|,\\\\ f_{ij}\\ge 0, \\quad \\sum_jf_{ij}\\le E_i, \\quad \\sum_if_{ij}\\le E^\\prime_j, \\quad \\sum_{ij}f_{ij}=E_\\text{min}, where $E_i,E^\\prime_j$ are the energies of the particles in the two events, $\\theta_{ij}$ is an angular distance between particles, and $E_\\text{min}=\\min\\left(\\sum_iE_i,\\,\\sum_jE^\\prime_j\\right)$ is the smaller of the two total energies. In a hadronic context, transverse momenta are used instead of energies. emd energyflow.emd.emd(ev0, ev1, R=1.0, norm=False, measure='euclidean', coords='hadronic', return_flow=False, gdim=None, mask=False, n_iter_max=100000, periodic_phi=False, phi_col=2, empty_policy='error') Compute the EMD between two events. Arguments ev0 : numpy.ndarray The first event, given as a two-dimensional array. The event is assumed to be an (M,1+gdim) array of particles, where M is the multiplicity and gdim is the dimension of the ground space in which to compute euclidean distances between particles (as specified by the gdim keyword argument. The zeroth column is assumed to be the energies (or equivalently, the transverse momenta) of the particles. For typical hadron collider jet applications, each particle will be of the form (pT,y,phi) where y is the rapidity and phi is the azimuthal angle. ev1 : numpy.ndarray The other event, same format as ev0 . R : float The R parameter in the EMD definition that controls the relative importance of the two terms. Must be greater than or equal to half of the maximum ground distance in the space in order for the EMD to be a valid metric. norm : bool Whether or not to normalize the pT values of the events prior to computing the EMD. measure : str Controls which metric is used to calculate the ground distances between particles. 'euclidean' uses the euclidean metric in however many dimensions are provided and specified by gdim . 'spherical' uses the opening angle between particles on the sphere (note that this is not fully tested and should be used cautiously). coords : str Only has an effect if measure='spherical' , in which case it controls if 'hadronic' coordinates (pT,y,phi,[m]) are expected versus 'cartesian' coordinates (E,px,py,pz) . return_flow : bool Whether or not to return the flow matrix describing the optimal transport found during the computation of the EMD. Note that since the second term in Eq. 1 is implemented by including an additional particle in the event with lesser total pT, this will be reflected in the flow matrix. gdim : int The dimension of the ground metric space. Useful for restricting which dimensions are considered part of the ground space. Can be larger than the number of dimensions present in the events (in which case all dimensions will be included). If None , has no effect. mask : bool If True , ignores particles farther than R away from the origin. n_iter_max : int Maximum number of iterations for solving the optimal transport problem. periodic_phi : bool Whether to expect (and therefore properly handle) periodicity in the coordinate corresponding to the azimuthal angle $\\phi$. Should typically be True for event-level applications but can be set to False (which is slightly faster) for jet applications where all $\\phi$ differences are less than or equal to $\\pi$. phi_col : int The index of the column of $\\phi$ values in the event array. empty_policy : float or 'error' Controls behavior if an empty event is passed in. When set to 'error' , a ValueError is raised if an empty event is encountered. If set to a float, that value is returned is returned instead on an empty event. Returns float The EMD value. [ numpy.ndarray ], optional The flow matrix found while solving for the EMD. The (i,j) th entry is the amount of pT that flows between particle i in ev0 and particle j in ev1 . emds energyflow.emd.emds(X0, X1=None, R=1.0, norm=False, measure='euclidean', coords='hadronic', gdim=None, mask=False, n_iter_max=100000, periodic_phi=False, phi_col=2, empty_policy='error', n_jobs=None, verbose=0, print_every=1000000) Compute the EMD between collections of events. This can be used to compute EMDs between all pairs of events in a set or between events in two difference sets. Arguments X0 : list Iterable collection of events. Each event is assumed to be an (M,1+gdim) array of particles, where M is the multiplicity and gdim is the dimension of the ground space in which to compute euclidean distances between particles (specified by the gdim keyword argument). The zeroth column is assumed to be the energies (or equivalently, the transverse momenta) of the particles. For typical hadron collider jet applications, each particle will be of the form (pT,y,phi) where y is the rapidity and phi is the azimuthal angle. X1 : list or None Iterable collection of events in the same format as X0 , or None . If the latter, the pairwise distances between events in X0 will be computed and the returned matrix will be symmetric. R : float The R parameter in the EMD definition that controls the relative importance of the two terms. Must be greater than or equal to half of the maximum ground distance in the space in order for the EMD to be a valid metric. norm : bool Whether or not to normalize the pT values of the events prior to computing the EMD. measure : str Controls which metric is used to calculate the ground distances between particles. 'euclidean' uses the euclidean metric in however many dimensions are provided and specified by gdim . 'spherical' uses the opening angle between particles on the sphere (note that this is not fully tested and should be used cautiously). coords : str Only has an effect if measure='spherical' , in which case it controls if 'hadronic' coordinates (pT,y,phi,[m]) are expected versus 'cartesian' coordinates (E,px,py,pz) . gdim : int The dimension of the ground metric space. Useful for restricting which dimensions are considered part of the ground space. Can be larger than the number of dimensions present in the events (in which case all dimensions will be included). If None , has no effect. mask : bool If True , ignores particles farther than R away from the origin. n_iter_max : int Maximum number of iterations for solving the optimal transport problem. periodic_phi : bool Whether to expect (and therefore properly handle) periodicity in the coordinate corresponding to the azimuthal angle $\\phi$. Should typically be True for event-level applications but can be set to False (which is slightly faster) for jet applications where all $\\phi$ differences are less than or equal to $\\pi$. phi_col : int The index of the column of $\\phi$ values in the event array. empty_policy : float or 'error' Controls behavior if an empty event is passed in. When set to 'error' , a ValueError is raised if an empty event is encountered. If set to a float, that value is returned is returned instead on an empty event. n_jobs : int or None The number of worker processes to use. A value of None will use as many processes as there are CPUs on the machine. Note that for smaller numbers of events, a smaller value of n_jobs can be faster. verbose : int Controls the verbosity level. A value greater than 0 will print the progress of the computation at intervals specified by print_every . print_every : int The number of computations to do in between printing the progress. Even if the verbosity level is zero, this still plays a role in determining when the worker processes report the results back to the main process. Returns numpy.ndarray The EMD values as a two-dimensional array. If X1 was None , then the shape will be (len(X0), len(X0)) and the array will be symmetric, otherwise it will have shape (len(X0), len(X1)) .","title":"EMD"},{"location":"docs/emd/#emd","text":"energyflow.emd.emd(ev0, ev1, R=1.0, norm=False, measure='euclidean', coords='hadronic', return_flow=False, gdim=None, mask=False, n_iter_max=100000, periodic_phi=False, phi_col=2, empty_policy='error') Compute the EMD between two events. Arguments ev0 : numpy.ndarray The first event, given as a two-dimensional array. The event is assumed to be an (M,1+gdim) array of particles, where M is the multiplicity and gdim is the dimension of the ground space in which to compute euclidean distances between particles (as specified by the gdim keyword argument. The zeroth column is assumed to be the energies (or equivalently, the transverse momenta) of the particles. For typical hadron collider jet applications, each particle will be of the form (pT,y,phi) where y is the rapidity and phi is the azimuthal angle. ev1 : numpy.ndarray The other event, same format as ev0 . R : float The R parameter in the EMD definition that controls the relative importance of the two terms. Must be greater than or equal to half of the maximum ground distance in the space in order for the EMD to be a valid metric. norm : bool Whether or not to normalize the pT values of the events prior to computing the EMD. measure : str Controls which metric is used to calculate the ground distances between particles. 'euclidean' uses the euclidean metric in however many dimensions are provided and specified by gdim . 'spherical' uses the opening angle between particles on the sphere (note that this is not fully tested and should be used cautiously). coords : str Only has an effect if measure='spherical' , in which case it controls if 'hadronic' coordinates (pT,y,phi,[m]) are expected versus 'cartesian' coordinates (E,px,py,pz) . return_flow : bool Whether or not to return the flow matrix describing the optimal transport found during the computation of the EMD. Note that since the second term in Eq. 1 is implemented by including an additional particle in the event with lesser total pT, this will be reflected in the flow matrix. gdim : int The dimension of the ground metric space. Useful for restricting which dimensions are considered part of the ground space. Can be larger than the number of dimensions present in the events (in which case all dimensions will be included). If None , has no effect. mask : bool If True , ignores particles farther than R away from the origin. n_iter_max : int Maximum number of iterations for solving the optimal transport problem. periodic_phi : bool Whether to expect (and therefore properly handle) periodicity in the coordinate corresponding to the azimuthal angle $\\phi$. Should typically be True for event-level applications but can be set to False (which is slightly faster) for jet applications where all $\\phi$ differences are less than or equal to $\\pi$. phi_col : int The index of the column of $\\phi$ values in the event array. empty_policy : float or 'error' Controls behavior if an empty event is passed in. When set to 'error' , a ValueError is raised if an empty event is encountered. If set to a float, that value is returned is returned instead on an empty event. Returns float The EMD value. [ numpy.ndarray ], optional The flow matrix found while solving for the EMD. The (i,j) th entry is the amount of pT that flows between particle i in ev0 and particle j in ev1 .","title":"emd"},{"location":"docs/emd/#emds","text":"energyflow.emd.emds(X0, X1=None, R=1.0, norm=False, measure='euclidean', coords='hadronic', gdim=None, mask=False, n_iter_max=100000, periodic_phi=False, phi_col=2, empty_policy='error', n_jobs=None, verbose=0, print_every=1000000) Compute the EMD between collections of events. This can be used to compute EMDs between all pairs of events in a set or between events in two difference sets. Arguments X0 : list Iterable collection of events. Each event is assumed to be an (M,1+gdim) array of particles, where M is the multiplicity and gdim is the dimension of the ground space in which to compute euclidean distances between particles (specified by the gdim keyword argument). The zeroth column is assumed to be the energies (or equivalently, the transverse momenta) of the particles. For typical hadron collider jet applications, each particle will be of the form (pT,y,phi) where y is the rapidity and phi is the azimuthal angle. X1 : list or None Iterable collection of events in the same format as X0 , or None . If the latter, the pairwise distances between events in X0 will be computed and the returned matrix will be symmetric. R : float The R parameter in the EMD definition that controls the relative importance of the two terms. Must be greater than or equal to half of the maximum ground distance in the space in order for the EMD to be a valid metric. norm : bool Whether or not to normalize the pT values of the events prior to computing the EMD. measure : str Controls which metric is used to calculate the ground distances between particles. 'euclidean' uses the euclidean metric in however many dimensions are provided and specified by gdim . 'spherical' uses the opening angle between particles on the sphere (note that this is not fully tested and should be used cautiously). coords : str Only has an effect if measure='spherical' , in which case it controls if 'hadronic' coordinates (pT,y,phi,[m]) are expected versus 'cartesian' coordinates (E,px,py,pz) . gdim : int The dimension of the ground metric space. Useful for restricting which dimensions are considered part of the ground space. Can be larger than the number of dimensions present in the events (in which case all dimensions will be included). If None , has no effect. mask : bool If True , ignores particles farther than R away from the origin. n_iter_max : int Maximum number of iterations for solving the optimal transport problem. periodic_phi : bool Whether to expect (and therefore properly handle) periodicity in the coordinate corresponding to the azimuthal angle $\\phi$. Should typically be True for event-level applications but can be set to False (which is slightly faster) for jet applications where all $\\phi$ differences are less than or equal to $\\pi$. phi_col : int The index of the column of $\\phi$ values in the event array. empty_policy : float or 'error' Controls behavior if an empty event is passed in. When set to 'error' , a ValueError is raised if an empty event is encountered. If set to a float, that value is returned is returned instead on an empty event. n_jobs : int or None The number of worker processes to use. A value of None will use as many processes as there are CPUs on the machine. Note that for smaller numbers of events, a smaller value of n_jobs can be faster. verbose : int Controls the verbosity level. A value greater than 0 will print the progress of the computation at intervals specified by print_every . print_every : int The number of computations to do in between printing the progress. Even if the verbosity level is zero, this still plays a role in determining when the worker processes report the results back to the main process. Returns numpy.ndarray The EMD values as a two-dimensional array. If X1 was None , then the shape will be (len(X0), len(X0)) and the array will be symmetric, otherwise it will have shape (len(X0), len(X1)) .","title":"emds"},{"location":"docs/gen/","text":"Implementation of EFP Generator class. Generator Generates non-isomorphic multigraphs according to provided specifications. energyflow.Generator(dmax=None, nmax=None, emax=None, cmax=None, vmax=None, comp_dmaxs=None, filename=None, np_optimize='greedy', verbose=False) Doing a fresh generation of connected multigraphs ( filename=None ) requires that igraph be installed. Arguments dmax : int The maximum number of edges of the generated connected graphs. nmax : int The maximum number of vertices of the generated connected graphs. emax : int The maximum number of edges of the generated connected simple graphs. cmax : int The maximum VE complexity $\\chi$ of the generated connected graphs. vmax : int The maximum valency of the generated connected graphs. comp_dmaxs : { dict , int } If an integer, the maximum number of edges of the generated disconnected graphs. If a dictionary, the keys are numbers of vertices and the values are the maximum number of edges of the generated disconnected graphs with that number of vertices. filename : str If None , do a complete generation from scratch. If set to a string, read in connected graphs from the file given, restrict them according to the various 'max' parameters, and do a fresh disconnected generation. The special value filename='default' means to read in graphs from the default file. This is useful when various disconnected graph parameters are to be varied since the generation of large simple graphs is the most computationlly intensive part. np_optimize : { True , False , 'greedy' , 'optimal' } The optimize keyword of numpy.einsum_path . verbose : bool A flag to control printing. save save(filename) Save the current generator to file. Arguments filename : str The path to save the file. specs specs An array of EFP specifications. Each row represents an EFP and the columns represent the quantities indicated by cols .","title":"Multigraph Generation"},{"location":"docs/gen/#generator","text":"Generates non-isomorphic multigraphs according to provided specifications. energyflow.Generator(dmax=None, nmax=None, emax=None, cmax=None, vmax=None, comp_dmaxs=None, filename=None, np_optimize='greedy', verbose=False) Doing a fresh generation of connected multigraphs ( filename=None ) requires that igraph be installed. Arguments dmax : int The maximum number of edges of the generated connected graphs. nmax : int The maximum number of vertices of the generated connected graphs. emax : int The maximum number of edges of the generated connected simple graphs. cmax : int The maximum VE complexity $\\chi$ of the generated connected graphs. vmax : int The maximum valency of the generated connected graphs. comp_dmaxs : { dict , int } If an integer, the maximum number of edges of the generated disconnected graphs. If a dictionary, the keys are numbers of vertices and the values are the maximum number of edges of the generated disconnected graphs with that number of vertices. filename : str If None , do a complete generation from scratch. If set to a string, read in connected graphs from the file given, restrict them according to the various 'max' parameters, and do a fresh disconnected generation. The special value filename='default' means to read in graphs from the default file. This is useful when various disconnected graph parameters are to be varied since the generation of large simple graphs is the most computationlly intensive part. np_optimize : { True , False , 'greedy' , 'optimal' } The optimize keyword of numpy.einsum_path . verbose : bool A flag to control printing.","title":"Generator"},{"location":"docs/gen/#save","text":"save(filename) Save the current generator to file. Arguments filename : str The path to save the file.","title":"save"},{"location":"docs/gen/#specs","text":"specs An array of EFP specifications. Each row represents an EFP and the columns represent the quantities indicated by cols .","title":"specs"},{"location":"docs/measures/","text":"Energy and Angular Measures The appropriate notions of energy and angle depend on the collider context. Typically, one wants to work with observables that respect the appropriate Lorentz subgroup for the collision type of interest. EnergyFlow is capable of handling two broad classes of measures: $e^+e^-$ and hadronic, which are selected using the required measure argument. For substructure applications, it is often convenient to normalize the energies so that $\\sum_iz_i=1$. The normed keyword argument is provided to control normalization of the energies (default is True ). Each measure comes with a parameter $\\beta>0$ which controls the relative weighting between smaller and larger anglular structures. This can be set using the beta keyword argument (default is 1 ). There is also a $\\kappa$ parameter to control the relative weighting between soft and hard energies. This can be set using the kappa keyword argument (default is 1 ). Only kappa=1 yields collinear-safe observables. Beyond the measures implemented here, the user can implement their own custom measure by passing in ${z_i}$ and ${\\theta_{ij}}$ directly to the EFP classes. Hadronic Measures For hadronic collisions, observables are typically desired to be invariant under boosts along the beam direction and rotations about the beam direction. Thus, particle transverse momentum $p_T$ and rapidity-azimuth coordinates $(y,\\phi)$ are used. There are two hadronic measures implemented in EnergyFlow: 'hadr' and 'hadrdot' . These are listed explicitly below. 'hadr' : z_i=p_{T,i}^{\\kappa},\\quad\\quad \\theta_{ij}=(\\Delta y_{ij}^2 + \\Delta\\phi_{ij}^2)^{\\beta/2}. 'hadrdot' : z_i=p_{T,i}^{\\kappa},\\quad\\quad \\theta_{ij}=\\left(\\frac{2p^\\mu_ip_{j\\mu}} {p_{T,i}p_{T,j}}\\right)^{\\beta/2}. e+e- Measures For $e^+e^-$ collisions, observables are typically desired to be invariant under the full group of rotations about the interaction point. Since the center of momentum energy is known, the particle energy $E$ is typically used. For the angular measure, pairwise Lorentz contractions of the normalized particle four-momenta are used. There is one $e^+e^-$ measure implemented. 'ee' : z_i = E_{i}^{\\kappa},\\quad\\quad \\theta_{ij} = \\left(\\frac{2p_i^\\mu p_{j \\mu}} {E_i E_j}\\right)^{\\beta/2}. Measure Class for dealing with any kind of measure. energyflow.Measure(measure, beta=1, kappa=1, normed=True, coords=None, check_input=True) Processes inputs according to the measure choice. Arguments measure : string The string specifying the energy and angular measures to use. beta : float The angular weighting exponent $\\beta$. Must be positive. kappa : { float , 'pf' } If a number, the energy weighting exponent $\\kappa$. If 'pf' , use $\\kappa=v$ where $v$ is the valency of the vertex. 'pf' cannot be used with measure 'hadr' . Only IRC-safe for kappa=1 . normed : bool Whether or not to use normalized energies. coords : { 'ptyphim' , 'epxpypz' , None } Controls which coordinates are assumed for the input. If 'ptyphim' , the fourth column (the masses) is optional and massless particles are assumed if it is not present. If None , coords with be 'ptyphim' if using a hadronic measure and 'epxpypz' if using the e+e- measure. check_input : bool Whether to check the type of input each time or assume the first input type. evaluate evaluate(arg) Evaluate the measure on a set of particles. Arguments arg : 2-d numpy.ndarray A two-dimensional array of the particles with each row being a particle and the columns specified by the coords attribute. Returns ( 1-d numpy.ndarray , 2-d numpy.ndarray ) ( zs , thetas ) where zs is a vector of the energy fractions for each particle and thetas is the distance matrix between the particles.","title":"Measures"},{"location":"docs/measures/#energy-and-angular-measures","text":"The appropriate notions of energy and angle depend on the collider context. Typically, one wants to work with observables that respect the appropriate Lorentz subgroup for the collision type of interest. EnergyFlow is capable of handling two broad classes of measures: $e^+e^-$ and hadronic, which are selected using the required measure argument. For substructure applications, it is often convenient to normalize the energies so that $\\sum_iz_i=1$. The normed keyword argument is provided to control normalization of the energies (default is True ). Each measure comes with a parameter $\\beta>0$ which controls the relative weighting between smaller and larger anglular structures. This can be set using the beta keyword argument (default is 1 ). There is also a $\\kappa$ parameter to control the relative weighting between soft and hard energies. This can be set using the kappa keyword argument (default is 1 ). Only kappa=1 yields collinear-safe observables. Beyond the measures implemented here, the user can implement their own custom measure by passing in ${z_i}$ and ${\\theta_{ij}}$ directly to the EFP classes.","title":"Energy and Angular Measures"},{"location":"docs/measures/#hadronic-measures","text":"For hadronic collisions, observables are typically desired to be invariant under boosts along the beam direction and rotations about the beam direction. Thus, particle transverse momentum $p_T$ and rapidity-azimuth coordinates $(y,\\phi)$ are used. There are two hadronic measures implemented in EnergyFlow: 'hadr' and 'hadrdot' . These are listed explicitly below. 'hadr' : z_i=p_{T,i}^{\\kappa},\\quad\\quad \\theta_{ij}=(\\Delta y_{ij}^2 + \\Delta\\phi_{ij}^2)^{\\beta/2}. 'hadrdot' : z_i=p_{T,i}^{\\kappa},\\quad\\quad \\theta_{ij}=\\left(\\frac{2p^\\mu_ip_{j\\mu}} {p_{T,i}p_{T,j}}\\right)^{\\beta/2}.","title":"Hadronic Measures"},{"location":"docs/measures/#ee-measures","text":"For $e^+e^-$ collisions, observables are typically desired to be invariant under the full group of rotations about the interaction point. Since the center of momentum energy is known, the particle energy $E$ is typically used. For the angular measure, pairwise Lorentz contractions of the normalized particle four-momenta are used. There is one $e^+e^-$ measure implemented. 'ee' : z_i = E_{i}^{\\kappa},\\quad\\quad \\theta_{ij} = \\left(\\frac{2p_i^\\mu p_{j \\mu}} {E_i E_j}\\right)^{\\beta/2}.","title":"e+e- Measures"},{"location":"docs/measures/#measure","text":"Class for dealing with any kind of measure. energyflow.Measure(measure, beta=1, kappa=1, normed=True, coords=None, check_input=True) Processes inputs according to the measure choice. Arguments measure : string The string specifying the energy and angular measures to use. beta : float The angular weighting exponent $\\beta$. Must be positive. kappa : { float , 'pf' } If a number, the energy weighting exponent $\\kappa$. If 'pf' , use $\\kappa=v$ where $v$ is the valency of the vertex. 'pf' cannot be used with measure 'hadr' . Only IRC-safe for kappa=1 . normed : bool Whether or not to use normalized energies. coords : { 'ptyphim' , 'epxpypz' , None } Controls which coordinates are assumed for the input. If 'ptyphim' , the fourth column (the masses) is optional and massless particles are assumed if it is not present. If None , coords with be 'ptyphim' if using a hadronic measure and 'epxpypz' if using the e+e- measure. check_input : bool Whether to check the type of input each time or assume the first input type.","title":"Measure"},{"location":"docs/measures/#evaluate","text":"evaluate(arg) Evaluate the measure on a set of particles. Arguments arg : 2-d numpy.ndarray A two-dimensional array of the particles with each row being a particle and the columns specified by the coords attribute. Returns ( 1-d numpy.ndarray , 2-d numpy.ndarray ) ( zs , thetas ) where zs is a vector of the energy fractions for each particle and thetas is the distance matrix between the particles.","title":"evaluate"},{"location":"docs/obs/","text":"Implementations of some observables that are not covered by other portions of EnergyFlow. Some observables require the FastJet Python interface to be importable; if it's not no warnings or errors will be issued, the observables will simply not be included in this module. image_activity energyflow.image_activity(ptyphis, f=0.95, R=1.0, npix=33, center=None, axis=None) zg energyflow.zg(ptyphims, zcut=0.1, beta=0, R=1.0, algorithm='ca') zg_from_pj energyflow.zg_from_pj(pseudojet, zcut=0.1, beta=0, R=1.0)","title":"Observables"},{"location":"docs/obs/#image_activity","text":"energyflow.image_activity(ptyphis, f=0.95, R=1.0, npix=33, center=None, axis=None)","title":"image_activity"},{"location":"docs/obs/#zg","text":"energyflow.zg(ptyphims, zcut=0.1, beta=0, R=1.0, algorithm='ca')","title":"zg"},{"location":"docs/obs/#zg_from_pj","text":"energyflow.zg_from_pj(pseudojet, zcut=0.1, beta=0, R=1.0)","title":"zg_from_pj"},{"location":"docs/utils/","text":"Particle Tools Tools for dealing with particle momenta four-vectors. A four-vector can either be in Cartesian coordinates, [e,px,py,pz] (energy, momentum in x direction, momentum in y direction, momentum in z direction), or hadronic coordinates, [pt,y,phi,m] (transverse momentum, rapidity, azimuthal angle, mass), which are related via: p_T=\\sqrt{p_x^2+p_y^2},\\quad y=\\text{arctanh}\\,\\frac{p_z}{E},\\quad \\phi=\\arctan_2\\frac{p_y}{p_x},\\quad m=\\sqrt{E^2-p_x^2-p_y^2-p_z^2} and inversely: E=\\cosh y\\sqrt{p_T^2+m^2},\\quad p_x=p_T\\cos\\phi,\\quad p_y=p_T\\sin\\phi,\\quad p_z=\\sinh y\\sqrt{p_T^2+m^2}. The pseudorapidity eta can be obtained from a Cartesian four-momentum as: \\eta=\\text{arctanh}\\,\\frac{p_z}{|\\vec p|},\\quad |\\vec p|\\equiv\\sqrt{p_x^2+p_y^2+p_z^2}, and is related to the rapidity via \\eta=\\text{arcsinh}\\left(\\sinh y\\,\\left(1+m^2/p_T^2\\right)^{1/2}\\right),\\quad y=\\text{arcsinh}\\left(\\sinh \\eta\\,\\left(1+m^2/p_T^2\\right)^{-1/2}\\right). Note that the above formulas are numerically stable up to values of rapidity or pseudorapidity of a few hundred, above which the formulas have numerical issues. In this case, a different but equivalent formulae are used that are numerically stable in this region. In all cases, the $p_T\\to0$ limit produces infinite values. In the context of this package, an \"event\" is a two-dimensional numpy array with shape (M,4) where M is the multiplicity. An array of events is a three-dimensional array with shape (N,M,4) where N is the number of events. The valid inputs and outputs of the functions here will be described using this terminology. ptyphims_from_p4s energyflow.ptyphims_from_p4s(p4s, phi_ref=None) Convert to hadronic coordinates [pt,y,phi,m] from Cartesian coordinates. All-zero four-vectors are left alone. Arguments p4s : numpy.ndarray or list A single particle, event, or array of events in Cartesian coordinates. phi_ref : { None , 'hardest' , float , numpy.ndarray } Used to help deal with the fact that $\\phi$ is a periodic coordinate. If a float (which should be in $[0,2\\pi)$), all phi values will be within $\\pm\\pi$ of this reference value. If '\\hardest' , the phi of the hardest particle is used as the reference value. If None , all phis will be in the range $[0,2\\pi)$. An array is accepted in the case that p4s is an array of events, in which case the phi_ref array should have shape (N,) where N is the number of events. Returns numpy.ndarray An array of hadronic four-momenta with the same shape as the input. pts_from_p4s energyflow.pts_from_p4s(p4s) Calculate the transverse momenta of a collection of four-vectors. Arguments p4s : numpy.ndarray or list A single particle, event, or array of events in Cartesian coordinates. Returns numpy.ndarray An array of transverse momenta with shape p4s.shape[:-1] . pt2s_from_p4s energyflow.pt2s_from_p4s(p4s) Calculate the squared transverse momenta of a collection of four-vectors. Arguments p4s : numpy.ndarray or list A single particle, event, or array of events in Cartesian coordinates. Returns numpy.ndarray An array of squared transverse momenta with shape p4s.shape[:-1] . ys_from_p4s energyflow.ys_from_p4s(p4s) Calculate the rapidities of a collection of four-vectors. Returns zero for all-zero particles Arguments p4s : numpy.ndarray or list A single particle, event, or array of events in Cartesian coordinates. Returns numpy.ndarray An array of rapidities with shape p4s.shape[:-1] . etas_from_p4s energyflow.etas_from_p4s(p4s) Calculate the pseudorapidities of a collection of four-vectors. Returns zero for all-zero particles Arguments p4s : numpy.ndarray or list A single particle, event, or array of events in Cartesian coordinates. Returns numpy.ndarray An array of pseudorapidities with shape p4s.shape[:-1] . phis_from_p4s energyflow.phis_from_p4s(p4s, phi_ref=None) Calculate the azimuthal angles of a collection of four-vectors. Arguments p4s : numpy.ndarray or list A single particle, event, or array of events in Cartesian coordinates. phi_ref : { float , numpy.ndarray , None , 'hardest' } Used to help deal with the fact that $\\phi$ is a periodic coordinate. If a float (which should be in $[0,2\\pi)$), all phi values will be within $\\pm\\pi$ of this reference value. If '\\hardest' , the phi of the hardest particle is used as the reference value. If None , all phis will be in the range $[0,2\\pi)$. An array is accepted in the case that p4s is an array of events, in which case the phi_ref array should have shape (N,) where N is the number of events. Returns numpy.ndarray An array of azimuthal angles with shape p4s.shape[:-1] . m2s_from_p4s energyflow.m2s_from_p4s(p4s) Calculate the squared masses of a collection of four-vectors. Arguments p4s : numpy.ndarray or list A single particle, event, or array of events in Cartesian coordinates. Returns numpy.ndarray An array of squared masses with shape p4s.shape[:-1] . ms_from_p4s energyflow.ms_from_p4s(p4s) Calculate the masses of a collection of four-vectors. Arguments p4s : numpy.ndarray or list A single particle, event, or array of events in Cartesian coordinates. Returns numpy.ndarray An array of masses with shape p4s.shape[:-1] . ms_from_ps energyflow.ms_from_ps(ps) Calculate the masses of a collection of Lorentz vectors in two or more spacetime dimensions. Arguments ps : numpy.ndarray or list A single particle, event, or array of events in Cartesian coordinates in $d\\ge2$ spacetime dimensions. Returns numpy.ndarray An array of masses with shape ps.shape[:-1] . etas_from_pts_ys_ms energyflow.etas_from_pts_ys_ms(pts, ys, ms) Calculate pseudorapidities from transverse momenta, rapidities, and masses. All input arrays should have the same shape. Arguments pts : numpy.ndarray Array of transverse momenta. ys : numpy.ndarray Array of rapidities. ms : numpy.ndarray Array of masses. Returns numpy.ndarray Array of pseudorapidities with the same shape as ys . ys_from_pts_etas_ms energyflow.ys_from_pts_etas_ms(pts, etas, ms) Calculate rapidities from transverse momenta, pseudorapidities, and masses. All input arrays should have the same shape. Arguments pts : numpy.ndarray Array of transverse momenta. etas : numpy.ndarray Array of pseudorapidities. ms : numpy.ndarray Array of masses. Returns numpy.ndarray Array of rapidities with the same shape as etas . p4s_from_ptyphims energyflow.p4s_from_ptyphims(ptyphims) Calculate Cartesian four-vectors from transverse momenta, rapidities, azimuthal angles, and (optionally) masses for each input. Arguments ptyphims : numpy.ndarray or list A single particle, event, or array of events in hadronic coordinates. The mass is optional and if left out will be taken to be zero. Returns numpy.ndarray An array of Cartesian four-vectors. p4s_from_ptyphipids energyflow.p4s_from_ptyphipids(ptyphipids, error_on_unknown=False) Calculate Cartesian four-vectors from transverse momenta, rapidities, azimuthal angles, and particle IDs for each input. The particle IDs are used to lookup the mass of the particle. Transverse momenta should have units of GeV when using this function. Arguments ptyphipids : numpy.ndarray or list A single particle, event, or array of events in hadronic coordinates where the mass is replaced by the PDG ID of the particle. error_on_unknown : bool See the corresponding argument of pids2ms . Returns numpy.ndarray An array of Cartesian four-vectors with the same shape as the input. sum_ptyphims energyflow.sum_ptyphims(ptyphims, scheme='escheme') Add a collection of four-vectors that are expressed in hadronic coordinates by first converting to Cartesian coordinates and then summing. Arguments ptyphims : numpy.ndarray or list An event in hadronic coordinates. The mass is optional and if left out will be taken to be zero. scheme : str A string specifying a recombination scheme for adding four-vectors together. Currently supported options are 'escheme' , which adds the vectors in Cartesian coordinates, and 'ptscheme' , which sums the pTs of each particle and places the jet axis at the pT-weighted centroid in the rapidity-azimuth plane. Note that 'ptscheme' will return a three-vector consisting of the jet [pT,y,phi] with no mass value. Returns numpy.ndarray Array of summed four-vectors, in hadronic coordinates. sum_ptyphipids energyflow.sum_ptyphipids(ptyphipids, scheme='escheme', error_on_unknown=False) Add a collection of four-vectors that are expressed as [pT,y,phi,pdgid] . Arguments ptyphipids : numpy.ndarray or list A single particle or event in hadronic coordinates where the mass is replaced by the PDG ID of the particle. scheme : str See the argument of the same name here . error_on_unknown : bool See the corresponding argument of pids2ms . Returns numpy.ndarray Array of summed four-vectors, in hadronic coordinates. center_ptyphims energyflow.center_ptyphims(ptyphims, axis=None, center='escheme', copy=True) Center a collection of four-vectors according to a calculated or provided axis. Arguments ptyphims : numpy.ndarray or list An event in hadronic coordinates. The mass is optional and if left out will be taken to be zero. axis : numpy.ndarray If not None , the [y,phi] values to use for centering. center : str The centering scheme to be used. Valid options are the same as the scheme argument here . copy : bool Whether or not to copy the input array. Returns numpy.ndarray An array of hadronic four-momenta with the positions centered around the origin. rotate_ptyphims energyflow.rotate_ptyphims(ptyphims, rotate='ptscheme', center=None, copy=True) Rotate a collection of four-vectors to vertically align the principal component of the energy-flow tensor. The principal component is obtained as the eigenvector of the energy-flow tensor with the largest eigenvalue. It is only defined up to a sign, however it is ensured that Arguments ptyphims : numpy.ndarray or list An event in hadronic coordinates. The mass is optional and if left out will be taken to be zero. rotate : str The rotation scheme to be used. Currently, only 'ptscheme' is supported, which causes the rotation to take place in the rapidity-azimuth plane. center : str or None If not None , the event will be centered prior to rotation and this argument will be passed on to center_ptyphims as the centering scheme. copy : bool Whether or not to copy the input array. Returns numpy.ndarray An array of hadronic four-momenta with the positions rotated around the origin. reflect_ptyphims energyflow.reflect_ptyphims(ptyphims, which='both', center=None, copy=True) pids2ms energyflow.pids2ms(pids, error_on_unknown=False) Map an array of Particle Data Group IDs to an array of the corresponding particle masses (in GeV). Arguments pids : numpy.ndarray or list An array of numeric (float or integer) PDG ID values. error_on_unknown : bool Controls whether a KeyError is raised if an unknown PDG ID is encountered. If False , unknown PDG IDs will map to zero. Returns numpy.ndarray An array of masses in GeV. pids2chrgs energyflow.pids2chrgs(pids, error_on_unknown=False) Map an array of Particle Data Group IDs to an array of the corresponding particle charges (in fundamental units where the charge of the electron is -1). Arguments pids : numpy.ndarray or list An array of numeric (float or integer) PDG ID values. error_on_unknown : bool Controls whether a KeyError is raised if an unknown PDG ID is encountered. If False , unknown PDG IDs will map to zero. Returns numpy.ndarray An array of charges as floats. ischrgd energyflow.ischrgd(pids, ignored_pids=None) phi_fix energyflow.phi_fix(phis, phi_ref, copy=True) A function to ensure that all phis are within $\\pi$ of phi_ref . It is assumed that all starting phi values are $\\pm 2\\pi$ of phi_ref . Arguments phis : numpy.ndarray or list Array of phi values. phi_ref : { float or numpy.ndarray } A reference value used so that all phis will be within $\\pm\\pi$ of this value. Should have a shape of phis.shape[:-1] . copy : bool Determines if phis are copied or not. If False then phis is modified in place. Returns numpy.ndarray An array of the fixed phi values. flat_metric energyflow.flat_metric(dim) The Minkowski metric in dim spacetime dimensions in the mostly-minus convention. Arguments dim : int The number of spacetime dimensions (thought to be four in our universe). Returns 1-d numpy.ndarray A dim -length, one-dimensional (not matrix) array equal to [+1,-1,...,-1] . Random Events Functions to generate random sets of four-vectors. Includes an implementation of the RAMBO algorithm for sampling uniform M-body massless phase space. Also includes other functions for various random, non-center of momentum, and non-uniform sampling. gen_random_events energyflow.gen_random_events(nevents, nparticles, dim=4, mass=0.0) Generate random events with a given number of particles in a given spacetime dimension. The spatial components of the momenta are distributed uniformly in $[-1,+1]$. These events are not guaranteed to uniformly sample phase space. Arguments nevents : int Number of events to generate. nparticles : int Number of particles in each event. dim : int Number of spacetime dimensions. mass : float or 'random' Mass of the particles to generate. Can be set to 'random' to obtain a different random mass for each particle. Returns numpy.ndarray An (nevents,nparticles,dim) array of events. The particles are specified as [E,p1,p2,...] . If nevents is 1 then that axis is dropped. gen_random_events_mcom energyflow.gen_random_events_mcom(nevents, nparticles, dim=4) Generate random events with a given number of massless particles in a given spacetime dimension. The total momentum are made to sum to zero. These events are not guaranteed to uniformly sample phase space. Arguments nevents : int Number of events to generate. nparticles : int Number of particles in each event. dim : int Number of spacetime dimensions. Returns numpy.ndarray An (nevents,nparticles,dim) array of events. The particles are specified as [E,p1,p2,...] . gen_massless_phase_space energyflow.gen_massless_phase_space(nevents, nparticles, energy=1.0) Implementation of the RAMBO algorithm for uniformly sampling massless M-body phase space for any center of mass energy. Arguments nevents : int Number of events to generate. nparticles : int Number of particles in each event. energy : float Total center of mass energy of each event. Returns numpy.ndarray An (nevents,nparticles,4) array of events. The particles are specified as [E,p_x,p_y,p_z] . If nevents is 1 then that axis is dropped. Data Tools Functions for dealing with datasets. These are not importable from the top level energyflow module, but must instead be imported from energyflow.utils . get_examples energyflow.utils.get_examples(path='~/.energyflow', which='all', overwrite=False) Pulls examples from GitHub. To ensure availability of all examples update EnergyFlow to the latest version. Arguments path : str The destination for the downloaded files. Note that examples is automatically appended to the end of this path. which : { list , 'all' } List of examples to download, or the string 'all' in which case all the available examples are downloaded. overwrite : bool Whether to overwrite existing files or not. data_split energyflow.utils.data_split(*args, train=-1, val=0.0, test=0.1, shuffle=True) A function to split a dataset into train, test, and optionally validation datasets. Arguments *args : arbitrary numpy.ndarray datasets An arbitrary number of datasets, each required to have the same number of elements, as numpy arrays. train : { int , float } If a float, the fraction of elements to include in the training set. If an integer, the number of elements to include in the training set. The value -1 is special and means include the remaining part of the dataset in the training dataset after the test and (optionally) val parts have been removed val : { int , float } If a float, the fraction of elements to include in the validation set. If an integer, the number of elements to include in the validation set. The value 0 is special and means do not form a validation set. test : { int , float } If a float, the fraction of elements to include in the test set. If an integer, the number of elements to include in the test set. shuffle : bool A flag to control whether the dataset is shuffle prior to being split into parts. Returns list A list of the split datasets in train, [val], test order. If datasets X , Y , and Z were given as args (and assuming a non-zero val ), then [ X_train , X_val , X_test , Y_train , Y_val , Y_test , Z_train , Z_val , Z_test ] will be returned. to_categorical energyflow.utils.to_categorical(labels, num_classes=None) One-hot encodes class labels. Arguments labels : 1-d numpy.ndarray Labels in the range [0,num_classes) . num_classes : { int , None } The total number of classes. If None , taken to be the maximum label plus one. Returns 2-d numpy.ndarray The one-hot encoded labels. remap_pids energyflow.utils.remap_pids(events, pid_i=3) Remaps PDG id numbers to small floats for use in a neural network. events are modified in place and nothing is returned. Arguments events : 3-d numpy.ndarray The events as an array of arrays of particles. pid_i : int The index corresponding to pid information along the last axis of events . Image Tools Functions for dealing with image representations of events. These are not importable from the top level energyflow module, but must instead be imported from energyflow.utils . pixelate energyflow.utils.pixelate(jet, npix=33, img_width=0.8, nb_chan=1, norm=True, charged_counts_only=False) A function for creating a jet image from an array of particles. Arguments jet : numpy.ndarray An array of particles where each particle is of the form [pt,y,phi,pid] where the particle id column is only used if nb_chan=2 and charged_counts_only=True . npix : int The number of pixels on one edge of the jet image, which is taken to be a square. img_width : float The size of one edge of the jet image in the rapidity-azimuth plane. nb_chan : { 1 , 2 } The number of channels in the jet image. If 1 , then only a $p_T$ channel is constructed (grayscale). If 2 , then both a $p_T$ channel and a count channel are formed (color). norm : bool Whether to normalize the $p_T$ pixels to sum to 1 . charged_counts_only : bool If making a count channel, whether to only include charged particles. Requires that pid information be given. Returns 3-d numpy.ndarray The jet image as a (nb_chan, npix, npix) array. standardize energyflow.utils.standardize(*args, channels=None, copy=False, reg=10**-10) Normalizes each argument by the standard deviation of the pixels in arg[0]. The expected use case would be standardize(X_train, X_val, X_test) . Arguments *args : arbitrary numpy.ndarray datasets An arbitrary number of datasets, each required to have the same shape in all but the first axis. channels : int A list of which channels (assumed to be the second axis) to standardize. None is interpretted to mean every channel. copy : bool Whether or not to copy the input arrays before modifying them. reg : float Small parameter used to avoid dividing by zero. It's important that this be kept consistent for images used with a given model. Returns list A list of the now-standardized arguments. zero_center energyflow.utils.zero_center(args, kwargs) Subtracts the mean of arg[0] from the arguments. The expected use case would be standardize(X_train, X_val, X_test) . Arguments *args : arbitrary numpy.ndarray datasets An arbitrary number of datasets, each required to have the same shape in all but the first axis. channels : int A list of which channels (assumed to be the second axis) to zero center. None is interpretted to mean every channel. copy : bool Whether or not to copy the input arrays before modifying them. Returns list A list of the zero-centered arguments. FastJet Tools pjs_from_ptyphims energyflow.utils.pjs_from_ptyphims(ptyphims) cluster energyflow.utils.cluster(pjs, algorithm='ca', R=1000.0) softdrop energyflow.utils.softdrop(jet, zcut=0.1, beta=0, R=1.0)","title":"Utils"},{"location":"docs/utils/#particle-tools","text":"Tools for dealing with particle momenta four-vectors. A four-vector can either be in Cartesian coordinates, [e,px,py,pz] (energy, momentum in x direction, momentum in y direction, momentum in z direction), or hadronic coordinates, [pt,y,phi,m] (transverse momentum, rapidity, azimuthal angle, mass), which are related via: p_T=\\sqrt{p_x^2+p_y^2},\\quad y=\\text{arctanh}\\,\\frac{p_z}{E},\\quad \\phi=\\arctan_2\\frac{p_y}{p_x},\\quad m=\\sqrt{E^2-p_x^2-p_y^2-p_z^2} and inversely: E=\\cosh y\\sqrt{p_T^2+m^2},\\quad p_x=p_T\\cos\\phi,\\quad p_y=p_T\\sin\\phi,\\quad p_z=\\sinh y\\sqrt{p_T^2+m^2}. The pseudorapidity eta can be obtained from a Cartesian four-momentum as: \\eta=\\text{arctanh}\\,\\frac{p_z}{|\\vec p|},\\quad |\\vec p|\\equiv\\sqrt{p_x^2+p_y^2+p_z^2}, and is related to the rapidity via \\eta=\\text{arcsinh}\\left(\\sinh y\\,\\left(1+m^2/p_T^2\\right)^{1/2}\\right),\\quad y=\\text{arcsinh}\\left(\\sinh \\eta\\,\\left(1+m^2/p_T^2\\right)^{-1/2}\\right). Note that the above formulas are numerically stable up to values of rapidity or pseudorapidity of a few hundred, above which the formulas have numerical issues. In this case, a different but equivalent formulae are used that are numerically stable in this region. In all cases, the $p_T\\to0$ limit produces infinite values. In the context of this package, an \"event\" is a two-dimensional numpy array with shape (M,4) where M is the multiplicity. An array of events is a three-dimensional array with shape (N,M,4) where N is the number of events. The valid inputs and outputs of the functions here will be described using this terminology.","title":"Particle Tools"},{"location":"docs/utils/#ptyphims_from_p4s","text":"energyflow.ptyphims_from_p4s(p4s, phi_ref=None) Convert to hadronic coordinates [pt,y,phi,m] from Cartesian coordinates. All-zero four-vectors are left alone. Arguments p4s : numpy.ndarray or list A single particle, event, or array of events in Cartesian coordinates. phi_ref : { None , 'hardest' , float , numpy.ndarray } Used to help deal with the fact that $\\phi$ is a periodic coordinate. If a float (which should be in $[0,2\\pi)$), all phi values will be within $\\pm\\pi$ of this reference value. If '\\hardest' , the phi of the hardest particle is used as the reference value. If None , all phis will be in the range $[0,2\\pi)$. An array is accepted in the case that p4s is an array of events, in which case the phi_ref array should have shape (N,) where N is the number of events. Returns numpy.ndarray An array of hadronic four-momenta with the same shape as the input.","title":"ptyphims_from_p4s"},{"location":"docs/utils/#pts_from_p4s","text":"energyflow.pts_from_p4s(p4s) Calculate the transverse momenta of a collection of four-vectors. Arguments p4s : numpy.ndarray or list A single particle, event, or array of events in Cartesian coordinates. Returns numpy.ndarray An array of transverse momenta with shape p4s.shape[:-1] .","title":"pts_from_p4s"},{"location":"docs/utils/#pt2s_from_p4s","text":"energyflow.pt2s_from_p4s(p4s) Calculate the squared transverse momenta of a collection of four-vectors. Arguments p4s : numpy.ndarray or list A single particle, event, or array of events in Cartesian coordinates. Returns numpy.ndarray An array of squared transverse momenta with shape p4s.shape[:-1] .","title":"pt2s_from_p4s"},{"location":"docs/utils/#ys_from_p4s","text":"energyflow.ys_from_p4s(p4s) Calculate the rapidities of a collection of four-vectors. Returns zero for all-zero particles Arguments p4s : numpy.ndarray or list A single particle, event, or array of events in Cartesian coordinates. Returns numpy.ndarray An array of rapidities with shape p4s.shape[:-1] .","title":"ys_from_p4s"},{"location":"docs/utils/#etas_from_p4s","text":"energyflow.etas_from_p4s(p4s) Calculate the pseudorapidities of a collection of four-vectors. Returns zero for all-zero particles Arguments p4s : numpy.ndarray or list A single particle, event, or array of events in Cartesian coordinates. Returns numpy.ndarray An array of pseudorapidities with shape p4s.shape[:-1] .","title":"etas_from_p4s"},{"location":"docs/utils/#phis_from_p4s","text":"energyflow.phis_from_p4s(p4s, phi_ref=None) Calculate the azimuthal angles of a collection of four-vectors. Arguments p4s : numpy.ndarray or list A single particle, event, or array of events in Cartesian coordinates. phi_ref : { float , numpy.ndarray , None , 'hardest' } Used to help deal with the fact that $\\phi$ is a periodic coordinate. If a float (which should be in $[0,2\\pi)$), all phi values will be within $\\pm\\pi$ of this reference value. If '\\hardest' , the phi of the hardest particle is used as the reference value. If None , all phis will be in the range $[0,2\\pi)$. An array is accepted in the case that p4s is an array of events, in which case the phi_ref array should have shape (N,) where N is the number of events. Returns numpy.ndarray An array of azimuthal angles with shape p4s.shape[:-1] .","title":"phis_from_p4s"},{"location":"docs/utils/#m2s_from_p4s","text":"energyflow.m2s_from_p4s(p4s) Calculate the squared masses of a collection of four-vectors. Arguments p4s : numpy.ndarray or list A single particle, event, or array of events in Cartesian coordinates. Returns numpy.ndarray An array of squared masses with shape p4s.shape[:-1] .","title":"m2s_from_p4s"},{"location":"docs/utils/#ms_from_p4s","text":"energyflow.ms_from_p4s(p4s) Calculate the masses of a collection of four-vectors. Arguments p4s : numpy.ndarray or list A single particle, event, or array of events in Cartesian coordinates. Returns numpy.ndarray An array of masses with shape p4s.shape[:-1] .","title":"ms_from_p4s"},{"location":"docs/utils/#ms_from_ps","text":"energyflow.ms_from_ps(ps) Calculate the masses of a collection of Lorentz vectors in two or more spacetime dimensions. Arguments ps : numpy.ndarray or list A single particle, event, or array of events in Cartesian coordinates in $d\\ge2$ spacetime dimensions. Returns numpy.ndarray An array of masses with shape ps.shape[:-1] .","title":"ms_from_ps"},{"location":"docs/utils/#etas_from_pts_ys_ms","text":"energyflow.etas_from_pts_ys_ms(pts, ys, ms) Calculate pseudorapidities from transverse momenta, rapidities, and masses. All input arrays should have the same shape. Arguments pts : numpy.ndarray Array of transverse momenta. ys : numpy.ndarray Array of rapidities. ms : numpy.ndarray Array of masses. Returns numpy.ndarray Array of pseudorapidities with the same shape as ys .","title":"etas_from_pts_ys_ms"},{"location":"docs/utils/#ys_from_pts_etas_ms","text":"energyflow.ys_from_pts_etas_ms(pts, etas, ms) Calculate rapidities from transverse momenta, pseudorapidities, and masses. All input arrays should have the same shape. Arguments pts : numpy.ndarray Array of transverse momenta. etas : numpy.ndarray Array of pseudorapidities. ms : numpy.ndarray Array of masses. Returns numpy.ndarray Array of rapidities with the same shape as etas .","title":"ys_from_pts_etas_ms"},{"location":"docs/utils/#p4s_from_ptyphims","text":"energyflow.p4s_from_ptyphims(ptyphims) Calculate Cartesian four-vectors from transverse momenta, rapidities, azimuthal angles, and (optionally) masses for each input. Arguments ptyphims : numpy.ndarray or list A single particle, event, or array of events in hadronic coordinates. The mass is optional and if left out will be taken to be zero. Returns numpy.ndarray An array of Cartesian four-vectors.","title":"p4s_from_ptyphims"},{"location":"docs/utils/#p4s_from_ptyphipids","text":"energyflow.p4s_from_ptyphipids(ptyphipids, error_on_unknown=False) Calculate Cartesian four-vectors from transverse momenta, rapidities, azimuthal angles, and particle IDs for each input. The particle IDs are used to lookup the mass of the particle. Transverse momenta should have units of GeV when using this function. Arguments ptyphipids : numpy.ndarray or list A single particle, event, or array of events in hadronic coordinates where the mass is replaced by the PDG ID of the particle. error_on_unknown : bool See the corresponding argument of pids2ms . Returns numpy.ndarray An array of Cartesian four-vectors with the same shape as the input.","title":"p4s_from_ptyphipids"},{"location":"docs/utils/#sum_ptyphims","text":"energyflow.sum_ptyphims(ptyphims, scheme='escheme') Add a collection of four-vectors that are expressed in hadronic coordinates by first converting to Cartesian coordinates and then summing. Arguments ptyphims : numpy.ndarray or list An event in hadronic coordinates. The mass is optional and if left out will be taken to be zero. scheme : str A string specifying a recombination scheme for adding four-vectors together. Currently supported options are 'escheme' , which adds the vectors in Cartesian coordinates, and 'ptscheme' , which sums the pTs of each particle and places the jet axis at the pT-weighted centroid in the rapidity-azimuth plane. Note that 'ptscheme' will return a three-vector consisting of the jet [pT,y,phi] with no mass value. Returns numpy.ndarray Array of summed four-vectors, in hadronic coordinates.","title":"sum_ptyphims"},{"location":"docs/utils/#sum_ptyphipids","text":"energyflow.sum_ptyphipids(ptyphipids, scheme='escheme', error_on_unknown=False) Add a collection of four-vectors that are expressed as [pT,y,phi,pdgid] . Arguments ptyphipids : numpy.ndarray or list A single particle or event in hadronic coordinates where the mass is replaced by the PDG ID of the particle. scheme : str See the argument of the same name here . error_on_unknown : bool See the corresponding argument of pids2ms . Returns numpy.ndarray Array of summed four-vectors, in hadronic coordinates.","title":"sum_ptyphipids"},{"location":"docs/utils/#center_ptyphims","text":"energyflow.center_ptyphims(ptyphims, axis=None, center='escheme', copy=True) Center a collection of four-vectors according to a calculated or provided axis. Arguments ptyphims : numpy.ndarray or list An event in hadronic coordinates. The mass is optional and if left out will be taken to be zero. axis : numpy.ndarray If not None , the [y,phi] values to use for centering. center : str The centering scheme to be used. Valid options are the same as the scheme argument here . copy : bool Whether or not to copy the input array. Returns numpy.ndarray An array of hadronic four-momenta with the positions centered around the origin.","title":"center_ptyphims"},{"location":"docs/utils/#rotate_ptyphims","text":"energyflow.rotate_ptyphims(ptyphims, rotate='ptscheme', center=None, copy=True) Rotate a collection of four-vectors to vertically align the principal component of the energy-flow tensor. The principal component is obtained as the eigenvector of the energy-flow tensor with the largest eigenvalue. It is only defined up to a sign, however it is ensured that Arguments ptyphims : numpy.ndarray or list An event in hadronic coordinates. The mass is optional and if left out will be taken to be zero. rotate : str The rotation scheme to be used. Currently, only 'ptscheme' is supported, which causes the rotation to take place in the rapidity-azimuth plane. center : str or None If not None , the event will be centered prior to rotation and this argument will be passed on to center_ptyphims as the centering scheme. copy : bool Whether or not to copy the input array. Returns numpy.ndarray An array of hadronic four-momenta with the positions rotated around the origin.","title":"rotate_ptyphims"},{"location":"docs/utils/#reflect_ptyphims","text":"energyflow.reflect_ptyphims(ptyphims, which='both', center=None, copy=True)","title":"reflect_ptyphims"},{"location":"docs/utils/#pids2ms","text":"energyflow.pids2ms(pids, error_on_unknown=False) Map an array of Particle Data Group IDs to an array of the corresponding particle masses (in GeV). Arguments pids : numpy.ndarray or list An array of numeric (float or integer) PDG ID values. error_on_unknown : bool Controls whether a KeyError is raised if an unknown PDG ID is encountered. If False , unknown PDG IDs will map to zero. Returns numpy.ndarray An array of masses in GeV.","title":"pids2ms"},{"location":"docs/utils/#pids2chrgs","text":"energyflow.pids2chrgs(pids, error_on_unknown=False) Map an array of Particle Data Group IDs to an array of the corresponding particle charges (in fundamental units where the charge of the electron is -1). Arguments pids : numpy.ndarray or list An array of numeric (float or integer) PDG ID values. error_on_unknown : bool Controls whether a KeyError is raised if an unknown PDG ID is encountered. If False , unknown PDG IDs will map to zero. Returns numpy.ndarray An array of charges as floats.","title":"pids2chrgs"},{"location":"docs/utils/#ischrgd","text":"energyflow.ischrgd(pids, ignored_pids=None)","title":"ischrgd"},{"location":"docs/utils/#phi_fix","text":"energyflow.phi_fix(phis, phi_ref, copy=True) A function to ensure that all phis are within $\\pi$ of phi_ref . It is assumed that all starting phi values are $\\pm 2\\pi$ of phi_ref . Arguments phis : numpy.ndarray or list Array of phi values. phi_ref : { float or numpy.ndarray } A reference value used so that all phis will be within $\\pm\\pi$ of this value. Should have a shape of phis.shape[:-1] . copy : bool Determines if phis are copied or not. If False then phis is modified in place. Returns numpy.ndarray An array of the fixed phi values.","title":"phi_fix"},{"location":"docs/utils/#flat_metric","text":"energyflow.flat_metric(dim) The Minkowski metric in dim spacetime dimensions in the mostly-minus convention. Arguments dim : int The number of spacetime dimensions (thought to be four in our universe). Returns 1-d numpy.ndarray A dim -length, one-dimensional (not matrix) array equal to [+1,-1,...,-1] .","title":"flat_metric"},{"location":"docs/utils/#random-events","text":"Functions to generate random sets of four-vectors. Includes an implementation of the RAMBO algorithm for sampling uniform M-body massless phase space. Also includes other functions for various random, non-center of momentum, and non-uniform sampling.","title":"Random Events"},{"location":"docs/utils/#gen_random_events","text":"energyflow.gen_random_events(nevents, nparticles, dim=4, mass=0.0) Generate random events with a given number of particles in a given spacetime dimension. The spatial components of the momenta are distributed uniformly in $[-1,+1]$. These events are not guaranteed to uniformly sample phase space. Arguments nevents : int Number of events to generate. nparticles : int Number of particles in each event. dim : int Number of spacetime dimensions. mass : float or 'random' Mass of the particles to generate. Can be set to 'random' to obtain a different random mass for each particle. Returns numpy.ndarray An (nevents,nparticles,dim) array of events. The particles are specified as [E,p1,p2,...] . If nevents is 1 then that axis is dropped.","title":"gen_random_events"},{"location":"docs/utils/#gen_random_events_mcom","text":"energyflow.gen_random_events_mcom(nevents, nparticles, dim=4) Generate random events with a given number of massless particles in a given spacetime dimension. The total momentum are made to sum to zero. These events are not guaranteed to uniformly sample phase space. Arguments nevents : int Number of events to generate. nparticles : int Number of particles in each event. dim : int Number of spacetime dimensions. Returns numpy.ndarray An (nevents,nparticles,dim) array of events. The particles are specified as [E,p1,p2,...] .","title":"gen_random_events_mcom"},{"location":"docs/utils/#gen_massless_phase_space","text":"energyflow.gen_massless_phase_space(nevents, nparticles, energy=1.0) Implementation of the RAMBO algorithm for uniformly sampling massless M-body phase space for any center of mass energy. Arguments nevents : int Number of events to generate. nparticles : int Number of particles in each event. energy : float Total center of mass energy of each event. Returns numpy.ndarray An (nevents,nparticles,4) array of events. The particles are specified as [E,p_x,p_y,p_z] . If nevents is 1 then that axis is dropped.","title":"gen_massless_phase_space"},{"location":"docs/utils/#data-tools","text":"Functions for dealing with datasets. These are not importable from the top level energyflow module, but must instead be imported from energyflow.utils .","title":"Data Tools"},{"location":"docs/utils/#get_examples","text":"energyflow.utils.get_examples(path='~/.energyflow', which='all', overwrite=False) Pulls examples from GitHub. To ensure availability of all examples update EnergyFlow to the latest version. Arguments path : str The destination for the downloaded files. Note that examples is automatically appended to the end of this path. which : { list , 'all' } List of examples to download, or the string 'all' in which case all the available examples are downloaded. overwrite : bool Whether to overwrite existing files or not.","title":"get_examples"},{"location":"docs/utils/#data_split","text":"energyflow.utils.data_split(*args, train=-1, val=0.0, test=0.1, shuffle=True) A function to split a dataset into train, test, and optionally validation datasets. Arguments *args : arbitrary numpy.ndarray datasets An arbitrary number of datasets, each required to have the same number of elements, as numpy arrays. train : { int , float } If a float, the fraction of elements to include in the training set. If an integer, the number of elements to include in the training set. The value -1 is special and means include the remaining part of the dataset in the training dataset after the test and (optionally) val parts have been removed val : { int , float } If a float, the fraction of elements to include in the validation set. If an integer, the number of elements to include in the validation set. The value 0 is special and means do not form a validation set. test : { int , float } If a float, the fraction of elements to include in the test set. If an integer, the number of elements to include in the test set. shuffle : bool A flag to control whether the dataset is shuffle prior to being split into parts. Returns list A list of the split datasets in train, [val], test order. If datasets X , Y , and Z were given as args (and assuming a non-zero val ), then [ X_train , X_val , X_test , Y_train , Y_val , Y_test , Z_train , Z_val , Z_test ] will be returned.","title":"data_split"},{"location":"docs/utils/#to_categorical","text":"energyflow.utils.to_categorical(labels, num_classes=None) One-hot encodes class labels. Arguments labels : 1-d numpy.ndarray Labels in the range [0,num_classes) . num_classes : { int , None } The total number of classes. If None , taken to be the maximum label plus one. Returns 2-d numpy.ndarray The one-hot encoded labels.","title":"to_categorical"},{"location":"docs/utils/#remap_pids","text":"energyflow.utils.remap_pids(events, pid_i=3) Remaps PDG id numbers to small floats for use in a neural network. events are modified in place and nothing is returned. Arguments events : 3-d numpy.ndarray The events as an array of arrays of particles. pid_i : int The index corresponding to pid information along the last axis of events .","title":"remap_pids"},{"location":"docs/utils/#image-tools","text":"Functions for dealing with image representations of events. These are not importable from the top level energyflow module, but must instead be imported from energyflow.utils .","title":"Image Tools"},{"location":"docs/utils/#pixelate","text":"energyflow.utils.pixelate(jet, npix=33, img_width=0.8, nb_chan=1, norm=True, charged_counts_only=False) A function for creating a jet image from an array of particles. Arguments jet : numpy.ndarray An array of particles where each particle is of the form [pt,y,phi,pid] where the particle id column is only used if nb_chan=2 and charged_counts_only=True . npix : int The number of pixels on one edge of the jet image, which is taken to be a square. img_width : float The size of one edge of the jet image in the rapidity-azimuth plane. nb_chan : { 1 , 2 } The number of channels in the jet image. If 1 , then only a $p_T$ channel is constructed (grayscale). If 2 , then both a $p_T$ channel and a count channel are formed (color). norm : bool Whether to normalize the $p_T$ pixels to sum to 1 . charged_counts_only : bool If making a count channel, whether to only include charged particles. Requires that pid information be given. Returns 3-d numpy.ndarray The jet image as a (nb_chan, npix, npix) array.","title":"pixelate"},{"location":"docs/utils/#standardize","text":"energyflow.utils.standardize(*args, channels=None, copy=False, reg=10**-10) Normalizes each argument by the standard deviation of the pixels in arg[0]. The expected use case would be standardize(X_train, X_val, X_test) . Arguments *args : arbitrary numpy.ndarray datasets An arbitrary number of datasets, each required to have the same shape in all but the first axis. channels : int A list of which channels (assumed to be the second axis) to standardize. None is interpretted to mean every channel. copy : bool Whether or not to copy the input arrays before modifying them. reg : float Small parameter used to avoid dividing by zero. It's important that this be kept consistent for images used with a given model. Returns list A list of the now-standardized arguments.","title":"standardize"},{"location":"docs/utils/#zero_center","text":"energyflow.utils.zero_center(args, kwargs) Subtracts the mean of arg[0] from the arguments. The expected use case would be standardize(X_train, X_val, X_test) . Arguments *args : arbitrary numpy.ndarray datasets An arbitrary number of datasets, each required to have the same shape in all but the first axis. channels : int A list of which channels (assumed to be the second axis) to zero center. None is interpretted to mean every channel. copy : bool Whether or not to copy the input arrays before modifying them. Returns list A list of the zero-centered arguments.","title":"zero_center"},{"location":"docs/utils/#fastjet-tools","text":"","title":"FastJet Tools"},{"location":"docs/utils/#pjs_from_ptyphims","text":"energyflow.utils.pjs_from_ptyphims(ptyphims)","title":"pjs_from_ptyphims"},{"location":"docs/utils/#cluster","text":"energyflow.utils.cluster(pjs, algorithm='ca', R=1000.0)","title":"cluster"},{"location":"docs/utils/#softdrop","text":"energyflow.utils.softdrop(jet, zcut=0.1, beta=0, R=1.0)","title":"softdrop"}]}